{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A collection of functions used for performing clustering tasks**  \n",
    "Part 1 of this notebook consists of a class definition for fitting a distribution to a multidimensional point cloud.  \n",
    "Part 2 is a little deprecated not but kept for reference; it contains functionality for pre-filtering the histograms in the training set based on their moments (e.g. mean, rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: fitting a distribution to a point cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports \n",
    "\n",
    "# external modules\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import gaussian_kde\n",
    "import numpy as np\n",
    "\n",
    "# local modules\n",
    "from notebook_utils.notebook_to_script import save_notebook_as_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### functionality for fitting a function to point multidimensional point clouds\n",
    "\n",
    "class fitfunction:\n",
    "    ### abstract base class for all fit functions\n",
    "    # all other fit functions inherit from fitfunction and overload its functions\n",
    "    # no concrete fitting procedure is implemented,\n",
    "    # but some basic checks on dimensionality are performed\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self,points):\n",
    "        self.npoints = points.shape[0]\n",
    "        self.ndims = points.shape[1]\n",
    "        \n",
    "    # get pdf at points\n",
    "    def pdf(self,points):\n",
    "        ### get the pdf (probability density function) value at given points\n",
    "        # points is a 2D numpy array of shape (npoints,ndims)\n",
    "        # the output is a 1D array of shape (npoints)\n",
    "        pshape = points.shape\n",
    "        if not len(pshape)==2:\n",
    "            print('wrong input shape')\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def pdfgrid(self,grid):\n",
    "        ### get the pdf (probability density function) value at a given grid\n",
    "        # (only applicable to 2D case!)\n",
    "        # grid is a np array of shape (nx,ny,2)\n",
    "        # containing the x- and y-values in its first and second depth-wise dimension respectively.\n",
    "        # the grid is typically (but not necessarily) created via:\n",
    "        # x,y = np.mgrid[<xrange>,<yrange>]\n",
    "        # grid = np.dstack(x,y)\n",
    "        gshape = grid.shape\n",
    "        if not (self.ndims==2 and len(gshape)==3 and gshape[2]==2):\n",
    "            print('wrong input shape')\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "class lognormal(fitfunction):\n",
    "    \n",
    "    # parameters\n",
    "    # mean: multidim mean of underlying normal\n",
    "    # cov: multidim covariance matrix of underlying normal\n",
    "    # mvn: scipy.stats multivariate_normal object\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self,points):\n",
    "        # points is a np array of shape (npoints,ndims)\n",
    "        super().__init__(points)\n",
    "        # transform the data from assumed log-normal to normal\n",
    "        points_log = np.log(points)\n",
    "        # fit a total multivariate normal distribution\n",
    "        self.mean = np.mean(points_log,axis=0)\n",
    "        self.cov = np.cov(points_log,rowvar=False)\n",
    "        self.mvn = multivariate_normal(self.mean,self.cov)\n",
    "        \n",
    "    # get pdf at points\n",
    "    def pdf(self,points):\n",
    "        if not super().pdf(points): return None\n",
    "        return self.mvn.pdf(np.log(points))\n",
    "    \n",
    "    def pdfgrid(self,grid):\n",
    "        if not super().pdfgrid(points): return None\n",
    "        return self.mvn.pdf(np.log(grid))\n",
    "    \n",
    "class exponential(fitfunction):\n",
    "    \n",
    "    # parameters\n",
    "    # l: multidim lambda parameter of exponential\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self,points):\n",
    "        # points is a np array of shape (npoints,ndims)\n",
    "        super().__init__(points)\n",
    "        # for now use mean for beta, maybe change later!\n",
    "        self.l = np.reciprocal(np.mean(points,axis=0))\n",
    "        \n",
    "    # get pdf at points\n",
    "    def pdf(self,points):\n",
    "        if not super().pdf(points): return None\n",
    "        return np.prod(self.l)*np.exp(-np.multiply(np.repeat(self.l,len(points),axis=0),points))\n",
    "        \n",
    "    def pdfgrid(self,grid):\n",
    "        if not super().pdfgrid(grid): return None\n",
    "        return np.prod(self.l)*np.exp(-np.sum(np.multiply(self.l,grid),axis=2))\n",
    "\n",
    "class seminormal(fitfunction):\n",
    "    # this is not strictly speaking a probability distribution,\n",
    "    # only the first quadrant of the result of fitting a normal distribution\n",
    "    # to the data + its mirror image wrt the origin\n",
    "    \n",
    "    # parameters\n",
    "    # cov: multidim covariance matrix of normal distribution\n",
    "    # mvn: scipy.stats multivariate_normal object\n",
    "        \n",
    "    # constructor from point cloud\n",
    "    def __init__(self,points=[]):\n",
    "        if len(points)==0: return\n",
    "        super().__init__(points)\n",
    "        points = np.vstack((points,-points))\n",
    "        self.cov = np.cov(points,rowvar=False)\n",
    "        self.mvn = multivariate_normal(np.zeros(self.ndims),self.cov)\n",
    "        \n",
    "    # get pdf at points\n",
    "    def pdf(self,points):\n",
    "        if not super().pdf(points): return None\n",
    "        return self.mvn.pdf(points)\n",
    "    \n",
    "    def pdfgrid(self,grid):\n",
    "        if not super().pdfgrid(grid): return None\n",
    "        return self.mvn.pdf(grid)\n",
    "    \n",
    "    def save(self,path):\n",
    "        np.save(path,self.cov)\n",
    "        \n",
    "    def load(self,path):\n",
    "        self.cov = np.load(path)\n",
    "        self.ndims = len(self.cov)\n",
    "        self.mvn = multivariate_normal(np.zeros(self.ndims),self.cov)\n",
    "        \n",
    "class gaussiankde(fitfunction):\n",
    "    # wrapper for scipy.stats.gaussian_kde (gaussian kernel density estimation)\n",
    "    \n",
    "    # parameters\n",
    "    # kernel: scipy.stats.gaussian_kde object\n",
    "    # cov: covariance matrix \n",
    "    # (use same definition as for seminormal, maybe later replace by internal kernel.covariance)\n",
    "    \n",
    "    # constructor from point cloud\n",
    "    def __init__(self,points=[],bw='default'):\n",
    "        if len(points)==0: return\n",
    "        super().__init__(points)\n",
    "        self.cov = np.cov(points,rowvar=False)\n",
    "        if bw=='default': bw = 20*np.power(self.npoints,-1/(self.ndims+4))\n",
    "        elif bw=='scott': bw = np.power(self.npoints,-1/(self.ndims+4))\n",
    "        self.kernel = gaussian_kde(np.transpose(points),bw_method=bw)\n",
    "        \n",
    "    # get pdf at points\n",
    "    def pdf(self,points):\n",
    "        if not super().pdf(points): return None\n",
    "        return self.kernel.pdf(np.transpose(points))\n",
    "    \n",
    "    # get pdf at grid\n",
    "    def pdfgrid(self,grid):\n",
    "        if not super().pdfgrid(grid): return None\n",
    "        # implementation seems to be different from scipy.mvn, explicit conversion to point array and back is needed\n",
    "        X = grid[:,:,0]\n",
    "        Y = grid[:,:,1]\n",
    "        pos = np.vstack((np.ravel(X),np.ravel(Y)))\n",
    "        Z = self.kernel.pdf(pos)\n",
    "        return np.reshape(Z,X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: moment-based pre-filtering functionality**  \n",
    "(Not used in the results from the latest weeks/months but it might be re-introduced at some point.  \n",
    "The functions below have however not been used for a long time and are not guaranteed to work out of the box...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecdist(moments,index):\n",
    "    # does not work well if there are outliers which dominate the distance\n",
    "    relmoments = moments-np.tile([moments[index,:]],(len(moments),1))\n",
    "    sumsm = np.sum(relmoments[:index,:],axis=0)\n",
    "    sumgr = np.sum(relmoments[index+1:,:],axis=0)\n",
    "    distofsum = np.sqrt(np.sum(np.power(sumsm-sumgr,2)))\n",
    "    sumofdist = np.sum(np.sqrt(np.sum(np.power(relmoments,2),axis=1)))\n",
    "    return distofsum/sumofdist\n",
    "\n",
    "def costhetadist(moments,index):\n",
    "    # works more or less but not all bad points have small values, \n",
    "    # allows to identify problematic regions but not individual LS\n",
    "    if(index==0 or index==len(moments)-1): return 0\n",
    "    inprod = np.sum(np.multiply(moments[index-1,:],moments[index+1,:]))\n",
    "    norms = np.sqrt(np.sum(np.power(moments[index-1,:],2)))*np.sqrt(np.sum(np.power(moments[index+1,:],2)))\n",
    "    return inprod/norms\n",
    "\n",
    "def avgnndist(moments,index,nn):\n",
    "    # seems to work well for the runs tested!\n",
    "    rng = np.array(range(index-nn,index+nn+1))\n",
    "    rng = rng[(rng>=0) & (rng<len(moments))]\n",
    "    moments_sec = moments[rng,:]-np.tile(moments[index,:],(len(rng),1))\n",
    "    return np.sum(np.sqrt(np.sum(np.power(moments_sec,2),axis=1)))/len(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getavgnndist(hists,nmoments,xmin,xmax,nbins,nneighbours):\n",
    "    dists = np.zeros(len(hists))\n",
    "    moments = np.zeros((len(hists),nmoments))\n",
    "    binwidth = (xmax-xmin)/nbins\n",
    "    bins = np.tile(np.linspace(xmin+binwidth/2,xmax-binwidth/2,num=nbins,endpoint=True),(len(hists),1))\n",
    "    for i in range(1,nmoments+1):\n",
    "        moments[:,i-1] = moment(bins,hists,i)\n",
    "    for i in range(len(hists)):\n",
    "        dists[i] = avgnndist(moments,i,nneighbours)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filteranomalous(df,nmoments=3,rmouterflow=True,rmlargest=0.,doplot=True,):\n",
    "    \n",
    "    # do preliminary filtering (no DCS-bit OFF)\n",
    "    # (implicitly re-index)\n",
    "    print('total number of LS: '+str(len(df)))\n",
    "    df = select_golden_and_bad(df)\n",
    "    print('filtered number of LS (DCS-bit ON): '+str(len(df)))\n",
    "    runs = get_runs(df)\n",
    "    #print('found following runs: '+str(runs))\n",
    "    \n",
    "    # initializations\n",
    "    nlumi = 0\n",
    "    dists = []\n",
    "    xmin = 0.\n",
    "    xmax = 1.\n",
    "    \n",
    "    # loop over runs and calculate distances\n",
    "    for run in runs:\n",
    "        dfr = select_runs(df,[run])\n",
    "        (hists,_,_) = get_hist_values(dfr)\n",
    "        nlumi += len(hists)\n",
    "        if rmouterflow: hists = hists[:,1:-1]\n",
    "        rdists = getavgnndist(hists,nmoments,xmin,xmax,len(hists[0]),2)\n",
    "        for d in rdists: dists.append(d)\n",
    "            \n",
    "    # concatenate all runs\n",
    "    dists = np.array(dists)\n",
    "    ind = np.linspace(0,nlumi,num=nlumi,endpoint=False)\n",
    "    if doplot: plotdistance(dists,ind,rmlargest=rmlargest)\n",
    "    (gmean,gstd) = getmeanstd(dists,ind,rmlargest=rmlargest)\n",
    "        \n",
    "    # add a columns to the original df\n",
    "    df['dist'] = dists\n",
    "    df['passmomentmethod'] = np.where(dists<gmean+3*gstd,1,0)\n",
    "    \n",
    "    # select separate df's\n",
    "    dfpass = df[df['dist']<gmean+3*gstd]\n",
    "    dfpass.reset_index(drop=True,inplace=True)\n",
    "    dffail = df[df['dist']>gmean+3*gstd]\n",
    "    dffail.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    return (df,dfpass,dffail,gmean,gstd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_notebook_as_script( 'clustering_utils.ipynb' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
