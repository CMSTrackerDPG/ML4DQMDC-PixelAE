{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Some utilities and example notebooks for ML4DQM/DC This repository contains example code for the ML4DQM/DC project. It was developed with the idea of training autoencoders on the per-lumisection histograms stored in dedicated DQMIO files in order to (partially) automate the DQM and/or DC process. However, it is intended to be generally useful for any ML4DQM/DC study (i.e. any subsystem, any type of histogram, any classification method). In more detail: The framework has been developed based on a number of 1D histograms related to the status of the pixel tracker. Support for 2D histograms was added later and tested preliminarily, but some parts of the code may not yet support them. Please feel free to contact me if you notice a place in the code where this is the case. Likewise, most of the development effort went into using an autoencoder as classification algorithm, i.e. looking at the mean-squared-difference between a histogram and its reconstruction as a measure for anomality. Support for other algorithms is also present, one just needs to define a class deriving from src/classifiers/HistogramClassifier. Working examples for NMF, PCA and a couple of other classifiers are present in the tutorials. Structure of this repository: There are five important directories: tutorials, utils, src, run and omsapi. The other directories in the repository contain either data or meta-information (e.g. for documentation). In more detail: utils: contains a number of python notebooks and equivalent scripts with static utility functions for general use. They are being called and used in various places throughout the code. src: contains the classes for this repository: DataLoader: class for convenient loading of histograms from the input csv files. HistStruct: a histogram container for easy handling of multiple types of histograms simultaneously and consistently. classifiers: folder containing an abstract base HistogramClassifier class and derived classes representing concrete examples of histogram classification algorithms. cloudfitters: folder containing an abstract base CloudFitter class and derived classes representing concrete examples of point cloud fitting algorithms (used when working with multiple types of histograms simultaneously). tutorials: contains a number of notebooks that can be used to get familiar with the code and its capabilities. run: contains code for alternative ways of running the workflow, e.g. with configuration files or with a GUI. Note: under development, recommended to start with script-based workflows first (as exemplified in the tutorials). omsapi: standalone API for retrieving information from OMS. Tutorials: Some tutorials are located in the tutorials folder in this repository, that should help you get started with the code. They can be grouped into different steps: Step 1: put the data in a more manageable format. The raw csv files that are our common input are not very easy to work with. Therefore you would probably first want to do something similar to what's done in the notebook read_and_write_data.ipynb. See the code and inline comments in that script and the functions it refers to for more detailed explanation. Its output is one single csv file per histogram type and per year, which is often much more convenient than the original csv files (which contain all histogram types together and are split per number of lines, not per run). All other functions and notebooks presuppose this first step. Step 2: plot the data. Next, you can run plot_histograms.ipynb and plot_histograms_loop.ipynb. These notebooks should help you get a feeling of what your histogram looks like in general, and perhaps help you find some anomalies that you can use for testing. For 2D histograms, look at plot_histograms_2d.ipynb instead. Step 3: train an autoencoder. The scripts autoencoder.ipynb and autoencoder_iterative.ipynb are used to train an autoencoder on the whole dataset or a particular subset respectively. Finally, autoencoder_combine.ipynb trains autoencoders on multiple types of histograms and combines the mse's for each. An example on how to implement another classification method is shown in template_combine.ipynb. Other remarks: The repository contains no data files. I was planning to put some example data files in a data folder, but the files are too big for github. However, the tutorial read_and_write_data.ipynb should help you get the data from where it is stored and put it in a useful format for further processing. Another way to get started is to get them from my CERNBox Disclaimer: the whole repository is still in development stage. Feel free to contact me in case you found bugs or if you have other suggestions. To get the tutorial notebooks running in SWAN (preferred method): Log in to SWAN. Go to Projects. Click the cloud icon that says 'Download Project from git' Paste the following url: https://github.com/LukaLambrecht/ML4DQM-DC.git . (alternative method): Log in to SWAN. Click on the leftmost icon on the top right ('new terminal'). Navigate to where you want this repository (the starting place is your CERNBox home directory). Paste this command: git clone https://github.com/LukaLambrecht/ML4DQM-DC.git (or however you usually clone a repository). Exit the terminal. The folder should now be where you cloned it, and you can open and run the notebooks in it in SWAN. Further documentation: Documentation for all the class definitions and functions in the relevant code directories: https://LukaLambrecht.github.io/ML4DQM-DC/ (note: this documentation is generated automatically from comments in the code and currently not yet in perfect shape, both regarding content and layout). Note that the website above does not include documentation for the tutorials (yet?). However, some comments in the tutorial notebooks should provide (enough?) explanation to follow along.","title":"Home"},{"location":"#some-utilities-and-example-notebooks-for-ml4dqmdc","text":"This repository contains example code for the ML4DQM/DC project. It was developed with the idea of training autoencoders on the per-lumisection histograms stored in dedicated DQMIO files in order to (partially) automate the DQM and/or DC process. However, it is intended to be generally useful for any ML4DQM/DC study (i.e. any subsystem, any type of histogram, any classification method). In more detail: The framework has been developed based on a number of 1D histograms related to the status of the pixel tracker. Support for 2D histograms was added later and tested preliminarily, but some parts of the code may not yet support them. Please feel free to contact me if you notice a place in the code where this is the case. Likewise, most of the development effort went into using an autoencoder as classification algorithm, i.e. looking at the mean-squared-difference between a histogram and its reconstruction as a measure for anomality. Support for other algorithms is also present, one just needs to define a class deriving from src/classifiers/HistogramClassifier. Working examples for NMF, PCA and a couple of other classifiers are present in the tutorials.","title":"Some utilities and example notebooks for ML4DQM/DC"},{"location":"#structure-of-this-repository","text":"There are five important directories: tutorials, utils, src, run and omsapi. The other directories in the repository contain either data or meta-information (e.g. for documentation). In more detail: utils: contains a number of python notebooks and equivalent scripts with static utility functions for general use. They are being called and used in various places throughout the code. src: contains the classes for this repository: DataLoader: class for convenient loading of histograms from the input csv files. HistStruct: a histogram container for easy handling of multiple types of histograms simultaneously and consistently. classifiers: folder containing an abstract base HistogramClassifier class and derived classes representing concrete examples of histogram classification algorithms. cloudfitters: folder containing an abstract base CloudFitter class and derived classes representing concrete examples of point cloud fitting algorithms (used when working with multiple types of histograms simultaneously). tutorials: contains a number of notebooks that can be used to get familiar with the code and its capabilities. run: contains code for alternative ways of running the workflow, e.g. with configuration files or with a GUI. Note: under development, recommended to start with script-based workflows first (as exemplified in the tutorials). omsapi: standalone API for retrieving information from OMS.","title":"Structure of this repository:"},{"location":"#tutorials","text":"Some tutorials are located in the tutorials folder in this repository, that should help you get started with the code. They can be grouped into different steps: Step 1: put the data in a more manageable format. The raw csv files that are our common input are not very easy to work with. Therefore you would probably first want to do something similar to what's done in the notebook read_and_write_data.ipynb. See the code and inline comments in that script and the functions it refers to for more detailed explanation. Its output is one single csv file per histogram type and per year, which is often much more convenient than the original csv files (which contain all histogram types together and are split per number of lines, not per run). All other functions and notebooks presuppose this first step. Step 2: plot the data. Next, you can run plot_histograms.ipynb and plot_histograms_loop.ipynb. These notebooks should help you get a feeling of what your histogram looks like in general, and perhaps help you find some anomalies that you can use for testing. For 2D histograms, look at plot_histograms_2d.ipynb instead. Step 3: train an autoencoder. The scripts autoencoder.ipynb and autoencoder_iterative.ipynb are used to train an autoencoder on the whole dataset or a particular subset respectively. Finally, autoencoder_combine.ipynb trains autoencoders on multiple types of histograms and combines the mse's for each. An example on how to implement another classification method is shown in template_combine.ipynb.","title":"Tutorials:"},{"location":"#other-remarks","text":"The repository contains no data files. I was planning to put some example data files in a data folder, but the files are too big for github. However, the tutorial read_and_write_data.ipynb should help you get the data from where it is stored and put it in a useful format for further processing. Another way to get started is to get them from my CERNBox Disclaimer: the whole repository is still in development stage. Feel free to contact me in case you found bugs or if you have other suggestions.","title":"Other remarks:"},{"location":"#to-get-the-tutorial-notebooks-running-in-swan","text":"","title":"To get the tutorial notebooks running in SWAN"},{"location":"#preferred-method","text":"Log in to SWAN. Go to Projects. Click the cloud icon that says 'Download Project from git' Paste the following url: https://github.com/LukaLambrecht/ML4DQM-DC.git .","title":"(preferred method):"},{"location":"#alternative-method","text":"Log in to SWAN. Click on the leftmost icon on the top right ('new terminal'). Navigate to where you want this repository (the starting place is your CERNBox home directory). Paste this command: git clone https://github.com/LukaLambrecht/ML4DQM-DC.git (or however you usually clone a repository). Exit the terminal. The folder should now be where you cloned it, and you can open and run the notebooks in it in SWAN.","title":"(alternative method):"},{"location":"#further-documentation","text":"Documentation for all the class definitions and functions in the relevant code directories: https://LukaLambrecht.github.io/ML4DQM-DC/ (note: this documentation is generated automatically from comments in the code and currently not yet in perfect shape, both regarding content and layout). Note that the website above does not include documentation for the tutorials (yet?). However, some comments in the tutorial notebooks should provide (enough?) explanation to follow along.","title":"Further documentation:"},{"location":"omsapi/","text":"OMS API: retrieve information from the OMS database Collection of tools for obtaining OMS information in json-like format. Note: this functionality supersedes the older version in the omsinterface folder! References: The code is based on the oms api repository here: https://gitlab.cern.ch/cmsoms/oms-api-client . The file omsapi.py in this folder is a direct copy of the omsapi/__init__.py file in that repository, as recommended by the developers to get it running on SWAN. See also these slides for further info on the setup of the app and this site for the available endpoints. How to use: You will need to authenticate through an application registered with the OMS developer team. Either contact me on llambrec@cern.ch so I can send you my application ID and client secret, or create your own as explained below. Open example.ipynb for some examples. You need to import get_oms_api.py, then create an OMSAPI instance via get_oms_api() (only once, can be re-used for multiple queries) and then query the information via get_oms_data( \\<arguments> ). See example.ipynb or get_oms_data.py for details. How to create a personal application for authentication: You will need to register a personal application ID and client secret with the OMS developer team. See the slides linked above on how to do that (only slide 4-6 are relevant, the rest has been taken care of). You will receive an application ID and client secret (both are just string-like variables). Create a new python file in this folder called clientid.py and define two variables in there: API_CLIENT_ID = '\\<your application ID>' API_CLIENT_SECRET = '\\<your client secret>' That should be all!","title":"README"},{"location":"omsapi/get_oms_data/","text":"get oms data Functionality to call the OMS API with the correct query based on input parameters How to use? Check the readme file in this directory for the required setup! In particular, you will need an application ID and client secret to authenticate. Once this is ready, you can do the following: - Import this module, for example via \"from get_oms_data import get_oms_api, get_oms_data, get_oms_response_attribute\" - Create an instance of the OMS API class using \"omsapi = get_oms_api()\" This instance can be re-used for all consecutive calls to OMS, no need to recreate it for every call. - Make a call to \"get_oms_data\", where the first argument is the instance you just created. Other arguments: see the function documentation below. - The returned object is a complicated dictionary containing all information. Simply print it to find out its exact structure and how to access exactly the values you need. The function \"get_oms_response_attribute\" is a small helper function to retrieve a specific attribute from this dictionary. See the notebook example.ipynb in this directory for some examples! get_oms_api() get an OMSAPI instance takes no input arguments, as the configuration parameters are unlikely to change very often if needed, these parameters can be changed in the file urls.py get_oms_data( omsapi, api_endpoint, runnb, extrafilters=[], extraargs={}, sort=None, attributes=[], limit_entries=1000) query some data from OMS input arguments: - omsapi: an OMSAPI instance, e.g. created by get_oms_api() - api_endpoint: string, target information, e.g. 'runs' or 'lumisections' (see the readme for a link where the available endpoints are listed) - runnb: run number(s) to retrieve the info for, either integer (for single run) or tuple or list of two elements (first run and last run) (can also be None to not filter on run number but this is not recommended) - extrafilters: list of extra filters (apart from run number), each filter is supposed to be a dict of the form {'attribute_name':<name>,'value':<value>,'operator':<operator>} where <name> must be a valid field name in the OMS data, <value> its value, and <operator> chosen from \"EQ\", \"NEQ\", \"LT\", \"GT\", \"LE\", \"GE\" or \"LIKE\" - extraargs: dict of custom key/value pairs to add to the query (still experimental, potentially usable for changing the granularity from 'run' to 'lumisection' for e.g. L1 trigger rates, see example.ipynb) - sort: valid field name in the OMS data by which to sort - attributes: list of valid field names in the OMS data to return (if not specified, all information is returned) - limit_entries: entry limit for output json object get_oms_response_attribute( omsresponse, attribute ) small helper function to retrieve a list of values for a single attribute input arguments: - omsresponse: the json-like object returned by get_oms_data - attribute: name of one of the attributes present in omsresponse","title":"get_oms_data"},{"location":"omsapi/get_oms_data/#get-oms-data","text":"Functionality to call the OMS API with the correct query based on input parameters How to use? Check the readme file in this directory for the required setup! In particular, you will need an application ID and client secret to authenticate. Once this is ready, you can do the following: - Import this module, for example via \"from get_oms_data import get_oms_api, get_oms_data, get_oms_response_attribute\" - Create an instance of the OMS API class using \"omsapi = get_oms_api()\" This instance can be re-used for all consecutive calls to OMS, no need to recreate it for every call. - Make a call to \"get_oms_data\", where the first argument is the instance you just created. Other arguments: see the function documentation below. - The returned object is a complicated dictionary containing all information. Simply print it to find out its exact structure and how to access exactly the values you need. The function \"get_oms_response_attribute\" is a small helper function to retrieve a specific attribute from this dictionary. See the notebook example.ipynb in this directory for some examples!","title":"get oms data"},{"location":"omsapi/get_oms_data/#get95oms95api","text":"get an OMSAPI instance takes no input arguments, as the configuration parameters are unlikely to change very often if needed, these parameters can be changed in the file urls.py","title":"get_oms_api()"},{"location":"omsapi/get_oms_data/#get95oms95data-omsapi-api95endpoint-runnb-extrafilters-extraargs-sortnone-attributes-limit95entries1000","text":"query some data from OMS input arguments: - omsapi: an OMSAPI instance, e.g. created by get_oms_api() - api_endpoint: string, target information, e.g. 'runs' or 'lumisections' (see the readme for a link where the available endpoints are listed) - runnb: run number(s) to retrieve the info for, either integer (for single run) or tuple or list of two elements (first run and last run) (can also be None to not filter on run number but this is not recommended) - extrafilters: list of extra filters (apart from run number), each filter is supposed to be a dict of the form {'attribute_name':<name>,'value':<value>,'operator':<operator>} where <name> must be a valid field name in the OMS data, <value> its value, and <operator> chosen from \"EQ\", \"NEQ\", \"LT\", \"GT\", \"LE\", \"GE\" or \"LIKE\" - extraargs: dict of custom key/value pairs to add to the query (still experimental, potentially usable for changing the granularity from 'run' to 'lumisection' for e.g. L1 trigger rates, see example.ipynb) - sort: valid field name in the OMS data by which to sort - attributes: list of valid field names in the OMS data to return (if not specified, all information is returned) - limit_entries: entry limit for output json object","title":"get_oms_data( omsapi, api_endpoint, runnb, extrafilters=[], extraargs={}, sort=None, attributes=[], limit_entries=1000)"},{"location":"omsapi/get_oms_data/#get95oms95response95attribute-omsresponse-attribute","text":"small helper function to retrieve a list of values for a single attribute input arguments: - omsresponse: the json-like object returned by get_oms_data - attribute: name of one of the attributes present in omsresponse","title":"get_oms_response_attribute( omsresponse, attribute )"},{"location":"omsapi/omsapi/","text":"omsapi [class] OMSApiException(Exception) (no valid documentation found) [class] OMSQuery(object) (no valid documentation found) \u2937 __init__(self, base_url, resource, verbose, cookies, oms_auth, cert_verify, retry_on_err_sec, proxies) (no valid documentation found) \u2937 _attr_exists(self, attr) (no valid documentation found) \u2937 _load_meta(self) (no valid documentation found) \u2937 _warn(self, message, raise_exc=False) (no valid documentation found) \u2937 set_verbose(self, verbose) (no valid documentation found) \u2937 set_validation(self, attribute_validation) (no valid documentation found) \u2937 attrs(self, attributes=None) (no valid documentation found) \u2937 filters(self, filters) (no valid documentation found) \u2937 filter(self, attribute, value, operator=\"EQ\") (no valid documentation found) \u2937 clear_filter(self) (no valid documentation found) \u2937 sort(self, attribute, asc=True) (no valid documentation found) \u2937 paginate(self, page=1, per_page=10) (no valid documentation found) \u2937 include(self, key) (no valid documentation found) \u2937 custom(self, key, value=None) (no valid documentation found) \u2937 data_query(self) (no valid documentation found) \u2937 data(self) (no valid documentation found) \u2937 meta(self) (no valid documentation found) \u2937 get_request(self, url, verify=False) (no valid documentation found) [class] OMSAPIOAuth(object) (no valid documentation found) \u2937 __init__(self, client_id, client_secret, audience=\"cmsoms-prod\", cert_verify=True, proxies={}, retry_on_err_sec=0) (no valid documentation found) \u2937 auth_oidc(self) (no valid documentation found) \u2937 auth_oidc_req(self) (no valid documentation found) [class] OMSAPI(object) (no valid documentation found) \u2937 __init__(self, api_url=\"https://cmsoms.cern.ch/agg/api\", api_version=\"v1\", verbose=True, cert_verify=True, retry_on_err_sec=0, proxies={}) (no valid documentation found) \u2937 query(self, resource, query_validation=True) (no valid documentation found) \u2937 auth_oidc(self, client_id, client_secret, audience=\"cmsoms-prod\", proxies={}) (no valid documentation found) \u2937 auth_krb(self, cookie_path=\"ssocookies.txt\") (no valid documentation found) rm_file(filename) (no valid documentation found)","title":"omsapi"},{"location":"omsapi/omsapi/#omsapi","text":"","title":"omsapi"},{"location":"omsapi/omsapi/#class-omsapiexceptionexception","text":"(no valid documentation found)","title":"[class] OMSApiException(Exception)"},{"location":"omsapi/omsapi/#class-omsqueryobject","text":"(no valid documentation found)","title":"[class] OMSQuery(object)"},{"location":"omsapi/omsapi/#9595init9595self-base95url-resource-verbose-cookies-oms95auth-cert95verify-retry95on95err95sec-proxies","text":"(no valid documentation found)","title":"&#10551; __init__(self, base_url, resource, verbose, cookies, oms_auth, cert_verify, retry_on_err_sec, proxies)"},{"location":"omsapi/omsapi/#95attr95existsself-attr","text":"(no valid documentation found)","title":"&#10551; _attr_exists(self, attr)"},{"location":"omsapi/omsapi/#95load95metaself","text":"(no valid documentation found)","title":"&#10551; _load_meta(self)"},{"location":"omsapi/omsapi/#95warnself-message-raise95excfalse","text":"(no valid documentation found)","title":"&#10551; _warn(self, message, raise_exc=False)"},{"location":"omsapi/omsapi/#set95verboseself-verbose","text":"(no valid documentation found)","title":"&#10551; set_verbose(self, verbose)"},{"location":"omsapi/omsapi/#set95validationself-attribute95validation","text":"(no valid documentation found)","title":"&#10551; set_validation(self, attribute_validation)"},{"location":"omsapi/omsapi/#attrsself-attributesnone","text":"(no valid documentation found)","title":"&#10551; attrs(self, attributes=None)"},{"location":"omsapi/omsapi/#filtersself-filters","text":"(no valid documentation found)","title":"&#10551; filters(self, filters)"},{"location":"omsapi/omsapi/#filterself-attribute-value-operatoreq","text":"(no valid documentation found)","title":"&#10551; filter(self, attribute, value, operator=\"EQ\")"},{"location":"omsapi/omsapi/#clear95filterself","text":"(no valid documentation found)","title":"&#10551; clear_filter(self)"},{"location":"omsapi/omsapi/#sortself-attribute-asctrue","text":"(no valid documentation found)","title":"&#10551; sort(self, attribute, asc=True)"},{"location":"omsapi/omsapi/#paginateself-page1-per95page10","text":"(no valid documentation found)","title":"&#10551; paginate(self, page=1, per_page=10)"},{"location":"omsapi/omsapi/#includeself-key","text":"(no valid documentation found)","title":"&#10551; include(self, key)"},{"location":"omsapi/omsapi/#customself-key-valuenone","text":"(no valid documentation found)","title":"&#10551; custom(self, key, value=None)"},{"location":"omsapi/omsapi/#data95queryself","text":"(no valid documentation found)","title":"&#10551; data_query(self)"},{"location":"omsapi/omsapi/#dataself","text":"(no valid documentation found)","title":"&#10551; data(self)"},{"location":"omsapi/omsapi/#metaself","text":"(no valid documentation found)","title":"&#10551; meta(self)"},{"location":"omsapi/omsapi/#get95requestself-url-verifyfalse","text":"(no valid documentation found)","title":"&#10551; get_request(self, url, verify=False)"},{"location":"omsapi/omsapi/#class-omsapioauthobject","text":"(no valid documentation found)","title":"[class] OMSAPIOAuth(object)"},{"location":"omsapi/omsapi/#9595init9595self-client95id-client95secret-audiencecmsoms-prod-cert95verifytrue-proxies-retry95on95err95sec0","text":"(no valid documentation found)","title":"&#10551; __init__(self, client_id, client_secret, audience=\"cmsoms-prod\", cert_verify=True, proxies={}, retry_on_err_sec=0)"},{"location":"omsapi/omsapi/#auth95oidcself","text":"(no valid documentation found)","title":"&#10551; auth_oidc(self)"},{"location":"omsapi/omsapi/#auth95oidc95reqself","text":"(no valid documentation found)","title":"&#10551; auth_oidc_req(self)"},{"location":"omsapi/omsapi/#class-omsapiobject","text":"(no valid documentation found)","title":"[class] OMSAPI(object)"},{"location":"omsapi/omsapi/#9595init9595self-api95urlhttpscmsomscernchaggapi-api95versionv1-verbosetrue-cert95verifytrue-retry95on95err95sec0-proxies","text":"(no valid documentation found)","title":"&#10551; __init__(self, api_url=\"https://cmsoms.cern.ch/agg/api\", api_version=\"v1\", verbose=True, cert_verify=True, retry_on_err_sec=0, proxies={})"},{"location":"omsapi/omsapi/#queryself-resource-query95validationtrue","text":"(no valid documentation found)","title":"&#10551; query(self, resource, query_validation=True)"},{"location":"omsapi/omsapi/#auth95oidcself-client95id-client95secret-audiencecmsoms-prod-proxies","text":"(no valid documentation found)","title":"&#10551; auth_oidc(self, client_id, client_secret, audience=\"cmsoms-prod\", proxies={})"},{"location":"omsapi/omsapi/#auth95krbself-cookie95pathssocookiestxt","text":"(no valid documentation found)","title":"&#10551; auth_krb(self, cookie_path=\"ssocookies.txt\")"},{"location":"omsapi/omsapi/#rm95filefilename","text":"(no valid documentation found)","title":"rm_file(filename)"},{"location":"omsapi/urls/","text":"urls","title":"urls"},{"location":"omsapi/urls/#urls","text":"","title":"urls"},{"location":"omsinterface/","text":"Tools for obtaining OMS information in json format !WARNING: superseded by newer OMS API version, see omsapi folder (as opposed to omsinterface here)! The code in this folder still works (on May 21 2021 at least) but it is nowhere used in the other notebooks in this project and will not be maintained. It can however be used as an alternative / backup way of accessing OMS that does not require to register your own app (see omsapi for details), a CERN username and password suffice. References: The code is largely based on / copied from the wbmcrawlr tool ( https://github.com/CMSTrackerDPG/wbmcrawlr ) and the cernrequests package ( https://github.com/CMSTrackerDPG/cernrequests ) by the Tracker DPG. All credits to the Tracker DPG group, all mistakes in copying or modifying are of course my own. How to use: Open example.ipynb The configuration should be quite self-explanatory: choose the mode ('run' for run information, 'lumsisections' for per-lumisection information, other modes also available), enter the run number for which to retrieve the information, choose the mode for authentication (via CERN username and password or via CERN grid certificate). In case you choose to use a certificate, first edit the file 'cert.py' to set the correct paths to where you stored your certificate and key. Else you will be prompted to enter your username and password. Run the cells below, the requested information should now be stored in the specified output json file. Instead, you could also directly use the resulting object in your script without writing it and loading it to a json file. Notes: Preliminary implementation, will be extended to e.g. filter on specific data fields only (for example only keep pileup or luminosity per lumisection), etc. Update: no extensions are planned, as this method of accessing OMS is superseded by a new API version, see above. The authentication with a certificate still has an issue. For now, you need both a certificate and a username/password to make the data retrieval work. The method using username and password seems to work without additional certificate. For more information on how to obtain a certificate, follow this link: https://github.com/CMSTrackerDPG/cernrequests#prerequisites One could also install the wbmcrawlr tool (from https://github.com/CMSTrackerDPG/wbmcrawlr ) and use it directly. Both methods are equivalent up to now, except these notebooks don't require additional installation of the wbmcrawler tool and cernrequests module.","title":"README"},{"location":"omsinterface/cert/","text":"cert","title":"cert"},{"location":"omsinterface/cert/#cert","text":"","title":"cert"},{"location":"omsinterface/connectiontools/","text":"connectiontools Tools for checking connectivity to specific URLs and obtaining cookies The functions in this script are not my own, but largely based on the wbmcrawler and cernrequests packages. See the readme file in this directory for more information. For normal users these functions should not be called directly, everything is handled by a single call to get_oms_data.py / get_oms_data. See get_oms_data.py in this directory for more information. check_connectivity(url) (no valid documentation found) get_cookies(url, authmode, **kwargs) (no valid documentation found) get_cookies_from_certificate(url, certificate, **kwargs) (no valid documentation found) get_cookies_from_login(url, login, **kwargs) (no valid documentation found) _construct_certificate_authentication_url(login_redirect_url) (no valid documentation found) _extract_login_form( xml_response_content ) (no valid documentation found) _modify_xml_content( xml_response_content ) (no valid documentation found)","title":"connectiontools"},{"location":"omsinterface/connectiontools/#connectiontools","text":"Tools for checking connectivity to specific URLs and obtaining cookies The functions in this script are not my own, but largely based on the wbmcrawler and cernrequests packages. See the readme file in this directory for more information. For normal users these functions should not be called directly, everything is handled by a single call to get_oms_data.py / get_oms_data. See get_oms_data.py in this directory for more information.","title":"connectiontools"},{"location":"omsinterface/connectiontools/#check95connectivityurl","text":"(no valid documentation found)","title":"check_connectivity(url)"},{"location":"omsinterface/connectiontools/#get95cookiesurl-authmode-kwargs","text":"(no valid documentation found)","title":"get_cookies(url, authmode, **kwargs)"},{"location":"omsinterface/connectiontools/#get95cookies95from95certificateurl-certificate-kwargs","text":"(no valid documentation found)","title":"get_cookies_from_certificate(url, certificate, **kwargs)"},{"location":"omsinterface/connectiontools/#get95cookies95from95loginurl-login-kwargs","text":"(no valid documentation found)","title":"get_cookies_from_login(url, login, **kwargs)"},{"location":"omsinterface/connectiontools/#95construct95certificate95authentication95urllogin95redirect95url","text":"(no valid documentation found)","title":"_construct_certificate_authentication_url(login_redirect_url)"},{"location":"omsinterface/connectiontools/#95extract95login95form-xml95response95content","text":"(no valid documentation found)","title":"_extract_login_form( xml_response_content )"},{"location":"omsinterface/connectiontools/#95modify95xml95content-xml95response95content","text":"(no valid documentation found)","title":"_modify_xml_content( xml_response_content )"},{"location":"omsinterface/get_oms_data/","text":"get oms data Main function in omsinterface to retrieve information from OMS How to use? See the readme file in this directory and the notebook example.ipynb! get_oms_data( mode, run, hltpathname='', authmode='login' ) main function for retrieving information from the OMS database input arguments: - mode: a string representing the type of information to retrieve. the following options are currently supported: 'run' -> retrieve information per run 'lumisections' -> retrieve information per lumisection 'hltpathinfos' -> get information on the available HLT paths for a given run, in particular their names, 'hltrate' -> get the trigger rate of a specified HLT path 'hltrates' -> get the trigger rate for all available HLT paths - run: a single run number (integer format) note: in case mode is 'run', the run argument can also be a tuple representing a range of runs. - hltpathname: the name of a HLT path for which to retrieve the trigger rate. ignored if mode is not 'hltrate' - authmode: string representing mode of authentication. choose from 'login' (you will be prompted for your cern username and password) or 'certificate' (requires you to have set up the path to a valid certificate) returns: - a list or dict (depending on the specifications) containing all information. simply print it to see how to access the exact values you need.","title":"get_oms_data"},{"location":"omsinterface/get_oms_data/#get-oms-data","text":"Main function in omsinterface to retrieve information from OMS How to use? See the readme file in this directory and the notebook example.ipynb!","title":"get oms data"},{"location":"omsinterface/get_oms_data/#get95oms95data-mode-run-hltpathname-authmodelogin","text":"main function for retrieving information from the OMS database input arguments: - mode: a string representing the type of information to retrieve. the following options are currently supported: 'run' -> retrieve information per run 'lumisections' -> retrieve information per lumisection 'hltpathinfos' -> get information on the available HLT paths for a given run, in particular their names, 'hltrate' -> get the trigger rate of a specified HLT path 'hltrates' -> get the trigger rate for all available HLT paths - run: a single run number (integer format) note: in case mode is 'run', the run argument can also be a tuple representing a range of runs. - hltpathname: the name of a HLT path for which to retrieve the trigger rate. ignored if mode is not 'hltrate' - authmode: string representing mode of authentication. choose from 'login' (you will be prompted for your cern username and password) or 'certificate' (requires you to have set up the path to a valid certificate) returns: - a list or dict (depending on the specifications) containing all information. simply print it to see how to access the exact values you need.","title":"get_oms_data( mode, run, hltpathname='', authmode='login' )"},{"location":"omsinterface/omstools/","text":"omstools Tools for accessing the OMS database The functions in this script are not my own, but largely based on the wbmcrawler and cernrequests packages. See the readme file in this directory for more information. For normal users these functions should not be called directly, everything is handled by a single call to get_oms_data.py / get_oms_data. See get_oms_data.py in this directory for more information. check_oms_connectivity() (no valid documentation found) get_oms_cookies( authmode, **kwargs ) (no valid documentation found) _get_oms_resource_within_cern_gpn(relative_url) (no valid documentation found) _get_oms_resource_authenticated(relative_url, cookies) (no valid documentation found) get_oms_resource(table, parameters, **kwargs) (no valid documentation found) _get_single_resource(table, parameters, **kwargs) (no valid documentation found) get_run(run_number, **kwargs) (no valid documentation found) get_fill(fill_number, **kwargs) (no valid documentation found) _get_resources_page(table, parameters, page, page_size, **kwargs) (no valid documentation found) get_resources(table, parameters, page_size=PAGE_SIZE, silent=False, **kwargs) (no valid documentation found) get_runs(begin, end, **kwargs) (no valid documentation found) get_fills(begin, end, **kwargs) (no valid documentation found) get_lumisection_count(run_number, **kwargs) (no valid documentation found) get_lumisections( run_number=None, fill_number=None, start_time=None, end_time=None, **kwargs) (no valid documentation found) get_hltpathinfos(run_number, **kwargs) (no valid documentation found) get_hltpathrates(run_number, path_name, **kwargs) (no valid documentation found) get_all_hltpathrates(run_number, silent=False, **kwargs) (no valid documentation found) calc_page_count(resource_count, page_size) (no valid documentation found) flatten_resource(response) (no valid documentation found) progress_bar(current, total, text=\"\", filler=\"#\") (no valid documentation found) print_progress(current, total, text=\"\", args, *kwargs) (no valid documentation found)","title":"omstools"},{"location":"omsinterface/omstools/#omstools","text":"Tools for accessing the OMS database The functions in this script are not my own, but largely based on the wbmcrawler and cernrequests packages. See the readme file in this directory for more information. For normal users these functions should not be called directly, everything is handled by a single call to get_oms_data.py / get_oms_data. See get_oms_data.py in this directory for more information.","title":"omstools"},{"location":"omsinterface/omstools/#check95oms95connectivity","text":"(no valid documentation found)","title":"check_oms_connectivity()"},{"location":"omsinterface/omstools/#get95oms95cookies-authmode-kwargs","text":"(no valid documentation found)","title":"get_oms_cookies( authmode, **kwargs )"},{"location":"omsinterface/omstools/#95get95oms95resource95within95cern95gpnrelative95url","text":"(no valid documentation found)","title":"_get_oms_resource_within_cern_gpn(relative_url)"},{"location":"omsinterface/omstools/#95get95oms95resource95authenticatedrelative95url-cookies","text":"(no valid documentation found)","title":"_get_oms_resource_authenticated(relative_url, cookies)"},{"location":"omsinterface/omstools/#get95oms95resourcetable-parameters-kwargs","text":"(no valid documentation found)","title":"get_oms_resource(table, parameters, **kwargs)"},{"location":"omsinterface/omstools/#95get95single95resourcetable-parameters-kwargs","text":"(no valid documentation found)","title":"_get_single_resource(table, parameters, **kwargs)"},{"location":"omsinterface/omstools/#get95runrun95number-kwargs","text":"(no valid documentation found)","title":"get_run(run_number, **kwargs)"},{"location":"omsinterface/omstools/#get95fillfill95number-kwargs","text":"(no valid documentation found)","title":"get_fill(fill_number, **kwargs)"},{"location":"omsinterface/omstools/#95get95resources95pagetable-parameters-page-page95size-kwargs","text":"(no valid documentation found)","title":"_get_resources_page(table, parameters, page, page_size, **kwargs)"},{"location":"omsinterface/omstools/#get95resourcestable-parameters-page95sizepage95size-silentfalse-kwargs","text":"(no valid documentation found)","title":"get_resources(table, parameters, page_size=PAGE_SIZE, silent=False, **kwargs)"},{"location":"omsinterface/omstools/#get95runsbegin-end-kwargs","text":"(no valid documentation found)","title":"get_runs(begin, end, **kwargs)"},{"location":"omsinterface/omstools/#get95fillsbegin-end-kwargs","text":"(no valid documentation found)","title":"get_fills(begin, end, **kwargs)"},{"location":"omsinterface/omstools/#get95lumisection95countrun95number-kwargs","text":"(no valid documentation found)","title":"get_lumisection_count(run_number, **kwargs)"},{"location":"omsinterface/omstools/#get95lumisections-run95numbernone-fill95numbernone-start95timenone-end95timenone-kwargs","text":"(no valid documentation found)","title":"get_lumisections( run_number=None, fill_number=None, start_time=None, end_time=None, **kwargs)"},{"location":"omsinterface/omstools/#get95hltpathinfosrun95number-kwargs","text":"(no valid documentation found)","title":"get_hltpathinfos(run_number, **kwargs)"},{"location":"omsinterface/omstools/#get95hltpathratesrun95number-path95name-kwargs","text":"(no valid documentation found)","title":"get_hltpathrates(run_number, path_name, **kwargs)"},{"location":"omsinterface/omstools/#get95all95hltpathratesrun95number-silentfalse-kwargs","text":"(no valid documentation found)","title":"get_all_hltpathrates(run_number, silent=False, **kwargs)"},{"location":"omsinterface/omstools/#calc95page95countresource95count-page95size","text":"(no valid documentation found)","title":"calc_page_count(resource_count, page_size)"},{"location":"omsinterface/omstools/#flatten95resourceresponse","text":"(no valid documentation found)","title":"flatten_resource(response)"},{"location":"omsinterface/omstools/#progress95barcurrent-total-text-filler","text":"(no valid documentation found)","title":"progress_bar(current, total, text=\"\", filler=\"#\")"},{"location":"omsinterface/omstools/#print95progresscurrent-total-text-args-kwargs","text":"(no valid documentation found)","title":"print_progress(current, total, text=\"\", args, *kwargs)"},{"location":"omsinterface/urls/","text":"urls","title":"urls"},{"location":"omsinterface/urls/#urls","text":"","title":"urls"},{"location":"src/DataLoader/","text":"DataLoader [class] DataLoader(object) class for loading histograms from disk the input usually consists of: - a csv file or a folder containing csv files in the correct format - a set of histogram names to load - a specification in terms of eras or years the output typically consists of pandas dataframes containing the requested histograms. \u2937 __init__( self ) initializer \u2937 check_year( self, year ) (no valid documentation found) \u2937 check_eras( self, eras, year ) (no valid documentation found) \u2937 check_dim( self, dim ) (no valid documentation found) \u2937 check_eos( self ) (no valid documentation found) \u2937 get_default_data_dirs( self, year='2017', eras=[], dim=1 ) get the default data directories for the data for this project note: this returns the directories where the data is currently stored; might change in future reprocessings of the data, and should be extended for upcoming Run-III data. note: default directories are on the /eos file system. this function will throw an exception if it has not access to /eos. input arguments: - year: data-taking year, should be '2017' or '2018' so far (default: 2017) - eras: list of valid eras for the given data-taking year (default: all eras) - dim: dimension of requested histograms (1 or 2) \u2937 get_csv_files_in_dir( self, inputdir, sort=True ) get a (sorted) list of csv files in a given input directory input arguments: - inputdir: directory to scan for csv files - sort: boolean whether to sort the files \u2937 get_csv_files_in_dirs( self, inputdirs, sort=True ) find the csv files in a set of input directories and return them in one list input arguments: - list of input directories where to look for csv files - sort: see get_csv_files_in_dir \u2937 get_default_csv_files( self, year='2017', eras=[], dim=1, sort=True ) read the csv files from the default directories with input data for this project note: default directories are on the /eos file system. this function will throw an exception if it has not access to /eos. input arguments: - year, eras, dim: see get_default_data_dirs! - sort: see get_csv_files_in_dir! \u2937 get_dataframe_from_file( self, csvfile, histnames=[] ) load histograms from a given file \u2937 get_dataframe_from_files( self, csvfiles, histnames=[] ) load histograms from a given set of files \u2937 write_dataframe_to_file( self, df, csvfile ) write a dataframe to a csv file","title":"DataLoader"},{"location":"src/DataLoader/#dataloader","text":"","title":"DataLoader"},{"location":"src/DataLoader/#class-dataloaderobject","text":"class for loading histograms from disk the input usually consists of: - a csv file or a folder containing csv files in the correct format - a set of histogram names to load - a specification in terms of eras or years the output typically consists of pandas dataframes containing the requested histograms.","title":"[class] DataLoader(object)"},{"location":"src/DataLoader/#9595init9595-self","text":"initializer","title":"&#10551; __init__( self )"},{"location":"src/DataLoader/#check95year-self-year","text":"(no valid documentation found)","title":"&#10551; check_year( self, year )"},{"location":"src/DataLoader/#check95eras-self-eras-year","text":"(no valid documentation found)","title":"&#10551; check_eras( self, eras, year )"},{"location":"src/DataLoader/#check95dim-self-dim","text":"(no valid documentation found)","title":"&#10551; check_dim( self, dim )"},{"location":"src/DataLoader/#check95eos-self","text":"(no valid documentation found)","title":"&#10551; check_eos( self )"},{"location":"src/DataLoader/#get95default95data95dirs-self-year2017-eras-dim1","text":"get the default data directories for the data for this project note: this returns the directories where the data is currently stored; might change in future reprocessings of the data, and should be extended for upcoming Run-III data. note: default directories are on the /eos file system. this function will throw an exception if it has not access to /eos. input arguments: - year: data-taking year, should be '2017' or '2018' so far (default: 2017) - eras: list of valid eras for the given data-taking year (default: all eras) - dim: dimension of requested histograms (1 or 2)","title":"&#10551; get_default_data_dirs( self, year='2017', eras=[], dim=1 )"},{"location":"src/DataLoader/#get95csv95files95in95dir-self-inputdir-sorttrue","text":"get a (sorted) list of csv files in a given input directory input arguments: - inputdir: directory to scan for csv files - sort: boolean whether to sort the files","title":"&#10551; get_csv_files_in_dir( self, inputdir, sort=True )"},{"location":"src/DataLoader/#get95csv95files95in95dirs-self-inputdirs-sorttrue","text":"find the csv files in a set of input directories and return them in one list input arguments: - list of input directories where to look for csv files - sort: see get_csv_files_in_dir","title":"&#10551; get_csv_files_in_dirs( self, inputdirs, sort=True )"},{"location":"src/DataLoader/#get95default95csv95files-self-year2017-eras-dim1-sorttrue","text":"read the csv files from the default directories with input data for this project note: default directories are on the /eos file system. this function will throw an exception if it has not access to /eos. input arguments: - year, eras, dim: see get_default_data_dirs! - sort: see get_csv_files_in_dir!","title":"&#10551; get_default_csv_files( self, year='2017', eras=[], dim=1, sort=True )"},{"location":"src/DataLoader/#get95dataframe95from95file-self-csvfile-histnames","text":"load histograms from a given file","title":"&#10551; get_dataframe_from_file( self, csvfile, histnames=[] )"},{"location":"src/DataLoader/#get95dataframe95from95files-self-csvfiles-histnames","text":"load histograms from a given set of files","title":"&#10551; get_dataframe_from_files( self, csvfiles, histnames=[] )"},{"location":"src/DataLoader/#write95dataframe95to95file-self-df-csvfile","text":"write a dataframe to a csv file","title":"&#10551; write_dataframe_to_file( self, df, csvfile )"},{"location":"src/HistStruct/","text":"HistStruct HistStruct: consistent treatment of multiple histogram types The HistStruct class is intended to be the main data structure used within this framework. A HistStruct object basically consists of a mutually consistent collection of numpy arrays, where each numpy array corresponds to one histogram type, with dimensions (number of histograms, number of bins). The HistStruct has functions to easily perform the following common tasks (among others): - select a subset of runs and/or lumisections (e.g. using a custom or predefined json file formatted selector), - prepare the data for machine learning training, with all kinds of preprocessing, - evaluate classifiers (machine learning types or other). Up to now the HistStruct is not used in many places, the main reason being that most of the tutorials for example were written (or at leasted started) before this class. When only processing a single histogram type, the HistStruct might be a bit of an overkill and one could choose to operate on the dataframe directly. However, especially when using multiple histogram types, the HistStruct is very handy to keep everything consistent. See the tutorial autoencoder_combine.ipynb for an important example! [class] HistStruct(object) main data structure used within this framework a HistStruct object basically consists of a mutually consistent collection of numpy arrays, where each numpy array corresponds to one histogram type, with dimensions (number of histograms, number of bins). the HistStruct has functions to easily perform the following common tasks (among others): - select a subset of runs and/or lumisections (e.g. using a json file formatted selector), - prepare the data for machine learning training - evaluate classifiers (machine learning types or other) \u2937 __init__( self ) empty initializer, setting all containers to empty defaults a HistStruct object has the following properties: histnames: list of histogram names histograms: dict mapping histogram name to 2D numpy array of histograms (shape (nhists,nbins)) nentries: dict mapping histogram name to 1D numpy array of number of entries per histogram (same length as histograms) histranges: dict mapping histogram name to tuple with (xmin, xmax) runnbs: 1D numpy array of run numbers (same length as histograms) lsnbs: 1D numpy array of lumisection numbers (same length as histograms) globalscores: 1D numpy array of global score per lumisection (same length as histograms) classifiers: dict mapping histogram name to object of type HistogramClassifier scores: dict mapping histogram name to 1D numpy array of values associated to the histograms (same length as histograms) masks: dict mapping name to 1D numpy array of booleans (same length as histograms) that can be used for masking exthistograms: dict of dicts similar to histograms for additional (e.g. artificially generated) histograms extscores: dict of dicts similar to scores for additional (e.g. artificially generated) histograms extglobalscores: dict of lists similar to scores for additional (e.g. artificially generated) histograms \u2937 __str__( self ) get a printable representation of a HistStruct \u2937 save( self, path, save_classifiers=True ) save a HistStruct object to a pkl file input arguments: - path where to store the file (appendix .zip is automatically appended) - save_classifiers: a boolean whether to include the classifiers if present in the HistStruct [class] classifiers = dict(self.classifiers) (no valid documentation found) [class] classifier.save( os.path.join(cpath,histname) ) (no valid documentation found) \u2937 load( self, path, load_classifiers=True, verbose=False ) load a HistStruct object from a pkl file input arguments: - path to a zip file containing a HistStruct object - load_classifiers: a boolean whether to load the classifiers if present - verbose: boolean whether to print some information \u2937 add_dataframe( self, df, cropslices=None, donormalize=True, rebinningfactor=None ) add a dataframe to a HistStruct input arguments: - df: a pandas dataframe as read from the input csv files - cropslices: list of slices (one per dimension) by which to crop the histograms - donormalize: boolean whether to normalize the histograms - rebinningfactor: factor by which to group bins together for more details on cropslices, donormalize and rebinningfactor, see hist_utils.py / preparedatafromdf! notes: - the new dataframe can contain one or multiple histogram types - the new dataframe must contain the same run and lumisection numbers (for each histogram type in it) as already present in the HistStruct, except if it is the first one to be added - alternative to adding the dataframe with the options cropslices, donormalize and rebinningfactor (that will be passed down to preparedatafromdf), one can also call preparedatafromdf manually and add it with add_histograms, allowing for more control over complicated preprocessing. \u2937 add_histograms( self, histname, histograms, runnbs, lsnbs, nentries=None ) add a set of histograms to a HistStruct input arguments: - histname: name of the histogram type to be added - histograms: a numpy array of shape (nhistograms,nbins), assumed to be of a single type - runnbs: a 1D list or array of length nhistograms containing the run number per histogram - lsnbs: a 1D list or array of length nhistograms containing the lumisection number per histogram - nentries: a 1D list or array of length nhistograms containing the number of entries per histogram notes: - must be provided explicitly since histograms might be normalized, in which case the number of entries cannot be determined from the sum of bin contents. - used for (de-)selecting histograms with sufficient statistics; if you don't need that type of selection, nentries can be left at default. - default is None, meaning all entries will be set to zero. notes: - no preprocessing is performed, this is assumed to have been done manually (if needed) before adding the histograms - runnbs and lsnbs must correspond to what is already in the current HistStruct, except if this is the first set of histogram to be added - see also add_dataframe for an alternative way of adding histograms \u2937 add_globalscores( self, globalscores ) add an array of global scores (one per lumisection) input arguments: - globalscores: 1D numpy array of scores (must have same length as lumisection and run numbers) \u2937 add_extglobalscores( self, extname, globalscores ) add an array of global scores (one per lumisection) for a specified extra set of histograms in the HistStruct input arguments: - extname: name of extra histogram set - globalscores: 1D numpy array of scores note: this function checks if all histogram types in this set contain the same number of histograms, (and that this number corresponds to the length of globalscores) else adding globalscores is meaningless \u2937 get_globalscores_jsonformat( self, working_point=None ) make a json format listing all lumisections in this histstruct the output list has entries for global score, pass/fail given working point, and masks input arguments: - working_point: if present, an entry will be made for each lumisection whether it passes this working point \u2937 add_exthistograms( self, extname, histname, histograms, overwrite=False ) add a set of extra histograms to a HistStruct these histograms are not assumed to correspond to physical run/lumisections numbers (e.g. resampled ones), and no consistency checks are done input arguments: - extname: name of the extra histogram set (you can add multiple, e.g. resampled_good, resampled_bad and/or resampled_training) - histname: name of the histogram type - histograms: a numpy array of shape (nhistograms,nbins) - overwrite: boolean whether to overwrite a set of histograms of the same name if present (default: raise exception) \u2937 add_mask( self, name, mask ) add a mask to a HistStruct input arguments: - name: a name for the mask - mask: a 1D np array of booleans with same length as number of lumisections in HistStruct \u2937 remove_mask( self, name ) inverse operation of add_mask \u2937 add_json_mask( self, name, jsondict ) add a mask corresponding to a json dict input arguments: - name: a name for the mask - jsondict: a dictionary in typical json format (see the golden json file for inspiration) all lumisections present in the jsondict will be masked True, the others False. \u2937 add_goldenjson_mask( self, name ) add a mask corresponding to the golden json file input arguments: - name: a name for the mask \u2937 add_dcsonjson_mask( self, name ) add a mask corresponding to the DCS-bit on json file input arguments: - name: a name for the mask \u2937 add_stat_mask( self, name, histnames=None, min_entries_to_bins_ratio=-1, max_entries_to_bins_ratio=-1 ) add a mask corresponding to lumisections where all histograms have statistics within given bounds input arguments: - histnames: list of histogram names to take into account for making the mask (default: all in the HistStruct) - min_entries_to_bins_ratio: number of entries divided by number of bins, lower boundary for statistics (default: no lower boundary) - max_entries_to_bins_ratio: same but upper boundary instead of lower boundary (default: no upper boundary) \u2937 add_highstat_mask( self, name, histnames=None, entries_to_bins_ratio=100 ) shorthand call to add_stat_mask with only lower boundary and no upper boundary for statistics input arguments: - entries_to_bins_ratio: number of entries divided by number of bins, lower boundary for statistics others: see add_stat_mask \u2937 get_combined_mask( self, names ) get a combined mask given multiple mask names mostly for internal use; externally you can use get_histograms( histname, <list of mask names>) directly \u2937 get_masknames( self ) return a simple list of all mask names in the current HistStruct \u2937 get_runnbs( self, masknames=None ) get the array of run numbers, optionally after masking input arguments: - masknames: list of names of masks (default: no masking, return full array) \u2937 get_lsnbs( self, masknames=None ) get the array of lumisection numbers, optionally after masking input arguments: - masknames: list of names of masks (default: no masking, return full array) \u2937 get_index( self, runnb, lsnb ) get the index in the current HistStruct of a given run and lumisection number input arguments: - runnb and lsnb: run and lumisection number respectively \u2937 get_scores( self, histname=None, masknames=None ) get the array of scores for a given histogram type, optionally after masking input arguments: - histname: name of the histogram type for which to retrieve the score. if None, return a dict matching histnames to arrays of scores - masknames: list of names of masks (default: no masking, return full array) notes: - this method takes the scores from the HistStruct.scores attribute; make sure to have evaluated the classifiers before calling this method, else an exception will be thrown. \u2937 get_scores_array( self, masknames=None ) similar to get_scores, but with different return type: np array of shape (nhistograms, nhistogramtypes) \u2937 get_extscores( self, extname, histname=None ) get the array of scores for a given histogram type in a given extra set. input arguments: - extname: name of the extra set (see also add_exthistograms) - histname: name of the histogram type for which to retrieve the score. if None, return a dict matching histnames to arrays of scores notes: - this method takes the scores from the HistStruct.extscores attribute; make sure to have evaluated the classifiers before calling this method, else an exception will be thrown. \u2937 get_extscores_array( self, extname ) similar to get_extscores, but with different return type: np array of shape (nhistograms, nhistogramtypes) \u2937 get_scores_ls( self, runnb, lsnb, histnames=None, suppresswarnings=False ) get the scores for a given run/lumisection number and for given histogram names input arguments: - runnb: run number - lsnb: lumisection number - histnames: names of the histogram types for which to retrieve the score. returns: - a dict matching each name in histnames to a score (or None if no valid score) notes: - this method takes the scores from the HistStruct.scores attribute; make sure to have evaluated the classifiers before calling this method, else the returned scores will be None. \u2937 get_globalscores( self, masknames=None ) get the array of global scores, optionally after masking input arguments: - masknames: list of names of masks (default: no masking, return full array) notes: - this method takes the scores from the HistStruct.globalscores attribute; make sure to have set this attribute with add_globalscores, else an exception will be thrown. \u2937 get_globalscore_ls( self, runnb, lsnb ) get the global score for a given run/lumisection number input arguments: - runnb: run number - lsnb: lumisection number - histnames: names of the histogram types for which to retrieve the score. returns: - a dict matching each name in histnames to a score (or None if no valid score) notes: - this method takes the scores from the HistStruct.scores attribute; make sure to have evaluated the classifiers before calling this method, else the returned scores will be None. \u2937 get_extglobalscores( self, extname ) get the array of global scores for one of the extra histogram sets input arguments: - extname: name of the extra histogram set notes: - this method takes the scores from the HistStruct.extglobalscores attribute; make sure to have set this attribute with add_extglobalscores, else an exception will be thrown. \u2937 get_histograms( self, histname=None, masknames=None ) get the array of histograms for a given type, optionally after masking input arguments: - histname: name of the histogram type to retrieve if None, return a dict matching histnames to arrays of histograms - masknames: list of names of masks (default: no masking, return full array) \u2937 get_exthistograms( self, extname, histname=None ) get the array of extra histograms for a given set name and type name input arguments: - extname: name of the set of extra histograms (see also add_exthistograms) - histname: name of the histogram type to retrieve if None, return a dict matching histnames to arrays of histograms \u2937 add_classifier( self, histname, classifier, evaluate=False ) add a histogram classifier for a given histogram name to the HistStruct input arguments: - histname: a valid histogram name present in the HistStruct to which this classifier applies - classifier: an object of type HistogramClassifier (i.e. of any class that derives from it) - evaluate: a bool whether to evaluate the classifier (and store the result in the 'scores' attribute) if set to True, the result is both returned and stored in the 'scores' attribute. \u2937 evaluate_classifier( self, histname, extname=None ) evaluate a histogram classifier for a given histogram name in the HistStruct input arguments: - histname: a valid histogram name present in the HistStruct for which to evaluate the classifier - extname: name of a set of extra histograms (see add_exthistograms) if None, will evaluate the classifer for the main set of histograms notes: - the result is both returned and stored in the 'scores' attribute \u2937 plot_histograms( self, histnames=None, masknames=None, colorlist=[], labellist=[], transparencylist=[], titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs ) plot the histograms in a HistStruct, optionally after msking note: so far only for 1D hsitograms. case of 2D histograms requires different plotting method since they cannot be clearly overlaid. if a HistStruct contains both 1D and 2D histograms, the 1D histograms must be selected with the histnames argument. input arguments: - histnames: list of names of the histogram types to plot (default: all) - masknames: list of list of mask names note: each element in masknames represents a set of masks to apply; the histograms passing different sets of masks are plotted in different colors - colorlist: list of matplotlib colors, must have same length as masknames - labellist: list of labels for the legend, must have same legnth as masknames - transparencylist: list of transparency values, must have same length as masknames - titledict: dict mapping histogram names to titles for the subplots (default: title = histogram name) - xaxtitledict: dict mapping histogram names to x-axis titles for the subplots (default: no x-axis title) - yaxtitledict: dict mapping histogram names to y-axis titles for the subplots (default: no y-axis title) - physicalxax: bool whether to use physical x-axis range or simply use bin number (default) - kwargs: keyword arguments passed down to plot_utils.plot_sets \u2937 plot_ls( self, runnb, lsnb, histnames=None, histlabel=None, recohist=None, recohistlabel='Reconstruction', refhists=None, refhistslabel='Reference histograms', refhiststransparency=None, titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs) plot the histograms in a HistStruct for a given run/ls number versus their references and/or their reconstruction note: so far only for 1D histograms. case of 2D histograms requires different plotting method since they cannot be clearly overlaid. if a HistStruct contains both 1D and 2D histograms, the 1D histograms must be selected with the histnames argument. input arguments: - runnb: run number - lsnb: lumisection number - histnames: names of histogram types to plot (default: all) - histlabel: legend entry for the histogram (default: run and lumisection number) - recohist: dict matching histogram names to reconstructed histograms notes: - 'reconstructed histograms' refers to e.g. autoencoder or NMF reconstructions; some models (e.g. simply looking at histogram moments) might not have this kind of reconstruction - in principle one histogram per key is expected, but still the the shape must be 2D (i.e. (1,nbins)) - in case recohist is set to 'auto', the reconstruction is calculated on the fly for the input histograms - recohistlabel: legend entry for the reco histograms - refhists: dict matching histogram names to reference histograms notes: - multiple histograms (i.e. a 2D array) per key are expected; in case there is only one reference histogram, it must be reshaped into (1,nbins) - refhistslabel: legend entry for the reference histograms - titledict: dict mapping histogram names to titles for the subplots (default: title = histogram name) - xaxtitledict: dict mapping histogram names to x-axis titles for the subplots (default: no x-axis title) - yaxtitledict: dict mapping histogram names to y-axis titles for the subplots (default: no y-axis title) - physicalxax: bool whether to use physical x-axis range or simply use bin number (default) - kwargs: keyword arguments passed down to plot_utils.plot_sets \u2937 plot_run( self, runnb, masknames=None, recohist=None, recohistlabel='reco', refhists=None, refhistslabel='reference', doprint=False) call plot_ls for all lumisections in a given run","title":"HistStruct"},{"location":"src/HistStruct/#histstruct","text":"HistStruct: consistent treatment of multiple histogram types The HistStruct class is intended to be the main data structure used within this framework. A HistStruct object basically consists of a mutually consistent collection of numpy arrays, where each numpy array corresponds to one histogram type, with dimensions (number of histograms, number of bins). The HistStruct has functions to easily perform the following common tasks (among others): - select a subset of runs and/or lumisections (e.g. using a custom or predefined json file formatted selector), - prepare the data for machine learning training, with all kinds of preprocessing, - evaluate classifiers (machine learning types or other). Up to now the HistStruct is not used in many places, the main reason being that most of the tutorials for example were written (or at leasted started) before this class. When only processing a single histogram type, the HistStruct might be a bit of an overkill and one could choose to operate on the dataframe directly. However, especially when using multiple histogram types, the HistStruct is very handy to keep everything consistent. See the tutorial autoencoder_combine.ipynb for an important example!","title":"HistStruct"},{"location":"src/HistStruct/#class-histstructobject","text":"main data structure used within this framework a HistStruct object basically consists of a mutually consistent collection of numpy arrays, where each numpy array corresponds to one histogram type, with dimensions (number of histograms, number of bins). the HistStruct has functions to easily perform the following common tasks (among others): - select a subset of runs and/or lumisections (e.g. using a json file formatted selector), - prepare the data for machine learning training - evaluate classifiers (machine learning types or other)","title":"[class] HistStruct(object)"},{"location":"src/HistStruct/#9595init9595-self","text":"empty initializer, setting all containers to empty defaults a HistStruct object has the following properties: histnames: list of histogram names histograms: dict mapping histogram name to 2D numpy array of histograms (shape (nhists,nbins)) nentries: dict mapping histogram name to 1D numpy array of number of entries per histogram (same length as histograms) histranges: dict mapping histogram name to tuple with (xmin, xmax) runnbs: 1D numpy array of run numbers (same length as histograms) lsnbs: 1D numpy array of lumisection numbers (same length as histograms) globalscores: 1D numpy array of global score per lumisection (same length as histograms) classifiers: dict mapping histogram name to object of type HistogramClassifier scores: dict mapping histogram name to 1D numpy array of values associated to the histograms (same length as histograms) masks: dict mapping name to 1D numpy array of booleans (same length as histograms) that can be used for masking exthistograms: dict of dicts similar to histograms for additional (e.g. artificially generated) histograms extscores: dict of dicts similar to scores for additional (e.g. artificially generated) histograms extglobalscores: dict of lists similar to scores for additional (e.g. artificially generated) histograms","title":"&#10551; __init__( self )"},{"location":"src/HistStruct/#9595str9595-self","text":"get a printable representation of a HistStruct","title":"&#10551; __str__( self )"},{"location":"src/HistStruct/#save-self-path-save95classifierstrue","text":"save a HistStruct object to a pkl file input arguments: - path where to store the file (appendix .zip is automatically appended) - save_classifiers: a boolean whether to include the classifiers if present in the HistStruct","title":"&#10551; save( self, path, save_classifiers=True )"},{"location":"src/HistStruct/#class-classifiers-dictselfclassifiers","text":"(no valid documentation found)","title":"[class] classifiers = dict(self.classifiers)"},{"location":"src/HistStruct/#class-classifiersave-ospathjoincpathhistname","text":"(no valid documentation found)","title":"[class] classifier.save( os.path.join(cpath,histname) )"},{"location":"src/HistStruct/#load-self-path-load95classifierstrue-verbosefalse","text":"load a HistStruct object from a pkl file input arguments: - path to a zip file containing a HistStruct object - load_classifiers: a boolean whether to load the classifiers if present - verbose: boolean whether to print some information","title":"&#10551; load( self, path, load_classifiers=True, verbose=False )"},{"location":"src/HistStruct/#add95dataframe-self-df-cropslicesnone-donormalizetrue-rebinningfactornone","text":"add a dataframe to a HistStruct input arguments: - df: a pandas dataframe as read from the input csv files - cropslices: list of slices (one per dimension) by which to crop the histograms - donormalize: boolean whether to normalize the histograms - rebinningfactor: factor by which to group bins together for more details on cropslices, donormalize and rebinningfactor, see hist_utils.py / preparedatafromdf! notes: - the new dataframe can contain one or multiple histogram types - the new dataframe must contain the same run and lumisection numbers (for each histogram type in it) as already present in the HistStruct, except if it is the first one to be added - alternative to adding the dataframe with the options cropslices, donormalize and rebinningfactor (that will be passed down to preparedatafromdf), one can also call preparedatafromdf manually and add it with add_histograms, allowing for more control over complicated preprocessing.","title":"&#10551; add_dataframe( self, df, cropslices=None, donormalize=True, rebinningfactor=None )"},{"location":"src/HistStruct/#add95histograms-self-histname-histograms-runnbs-lsnbs-nentriesnone","text":"add a set of histograms to a HistStruct input arguments: - histname: name of the histogram type to be added - histograms: a numpy array of shape (nhistograms,nbins), assumed to be of a single type - runnbs: a 1D list or array of length nhistograms containing the run number per histogram - lsnbs: a 1D list or array of length nhistograms containing the lumisection number per histogram - nentries: a 1D list or array of length nhistograms containing the number of entries per histogram notes: - must be provided explicitly since histograms might be normalized, in which case the number of entries cannot be determined from the sum of bin contents. - used for (de-)selecting histograms with sufficient statistics; if you don't need that type of selection, nentries can be left at default. - default is None, meaning all entries will be set to zero. notes: - no preprocessing is performed, this is assumed to have been done manually (if needed) before adding the histograms - runnbs and lsnbs must correspond to what is already in the current HistStruct, except if this is the first set of histogram to be added - see also add_dataframe for an alternative way of adding histograms","title":"&#10551; add_histograms( self, histname, histograms, runnbs, lsnbs, nentries=None )"},{"location":"src/HistStruct/#add95globalscores-self-globalscores","text":"add an array of global scores (one per lumisection) input arguments: - globalscores: 1D numpy array of scores (must have same length as lumisection and run numbers)","title":"&#10551; add_globalscores( self, globalscores )"},{"location":"src/HistStruct/#add95extglobalscores-self-extname-globalscores","text":"add an array of global scores (one per lumisection) for a specified extra set of histograms in the HistStruct input arguments: - extname: name of extra histogram set - globalscores: 1D numpy array of scores note: this function checks if all histogram types in this set contain the same number of histograms, (and that this number corresponds to the length of globalscores) else adding globalscores is meaningless","title":"&#10551; add_extglobalscores( self, extname, globalscores )"},{"location":"src/HistStruct/#get95globalscores95jsonformat-self-working95pointnone","text":"make a json format listing all lumisections in this histstruct the output list has entries for global score, pass/fail given working point, and masks input arguments: - working_point: if present, an entry will be made for each lumisection whether it passes this working point","title":"&#10551; get_globalscores_jsonformat( self, working_point=None )"},{"location":"src/HistStruct/#add95exthistograms-self-extname-histname-histograms-overwritefalse","text":"add a set of extra histograms to a HistStruct these histograms are not assumed to correspond to physical run/lumisections numbers (e.g. resampled ones), and no consistency checks are done input arguments: - extname: name of the extra histogram set (you can add multiple, e.g. resampled_good, resampled_bad and/or resampled_training) - histname: name of the histogram type - histograms: a numpy array of shape (nhistograms,nbins) - overwrite: boolean whether to overwrite a set of histograms of the same name if present (default: raise exception)","title":"&#10551; add_exthistograms( self, extname, histname, histograms, overwrite=False )"},{"location":"src/HistStruct/#add95mask-self-name-mask","text":"add a mask to a HistStruct input arguments: - name: a name for the mask - mask: a 1D np array of booleans with same length as number of lumisections in HistStruct","title":"&#10551; add_mask( self, name, mask )"},{"location":"src/HistStruct/#remove95mask-self-name","text":"inverse operation of add_mask","title":"&#10551; remove_mask( self, name )"},{"location":"src/HistStruct/#add95json95mask-self-name-jsondict","text":"add a mask corresponding to a json dict input arguments: - name: a name for the mask - jsondict: a dictionary in typical json format (see the golden json file for inspiration) all lumisections present in the jsondict will be masked True, the others False.","title":"&#10551; add_json_mask( self, name, jsondict )"},{"location":"src/HistStruct/#add95goldenjson95mask-self-name","text":"add a mask corresponding to the golden json file input arguments: - name: a name for the mask","title":"&#10551; add_goldenjson_mask( self, name )"},{"location":"src/HistStruct/#add95dcsonjson95mask-self-name","text":"add a mask corresponding to the DCS-bit on json file input arguments: - name: a name for the mask","title":"&#10551; add_dcsonjson_mask( self, name )"},{"location":"src/HistStruct/#add95stat95mask-self-name-histnamesnone-min95entries95to95bins95ratio-1-max95entries95to95bins95ratio-1","text":"add a mask corresponding to lumisections where all histograms have statistics within given bounds input arguments: - histnames: list of histogram names to take into account for making the mask (default: all in the HistStruct) - min_entries_to_bins_ratio: number of entries divided by number of bins, lower boundary for statistics (default: no lower boundary) - max_entries_to_bins_ratio: same but upper boundary instead of lower boundary (default: no upper boundary)","title":"&#10551; add_stat_mask( self, name, histnames=None, min_entries_to_bins_ratio=-1, max_entries_to_bins_ratio=-1 )"},{"location":"src/HistStruct/#add95highstat95mask-self-name-histnamesnone-entries95to95bins95ratio100","text":"shorthand call to add_stat_mask with only lower boundary and no upper boundary for statistics input arguments: - entries_to_bins_ratio: number of entries divided by number of bins, lower boundary for statistics others: see add_stat_mask","title":"&#10551; add_highstat_mask( self, name, histnames=None, entries_to_bins_ratio=100 )"},{"location":"src/HistStruct/#get95combined95mask-self-names","text":"get a combined mask given multiple mask names mostly for internal use; externally you can use get_histograms( histname, <list of mask names>) directly","title":"&#10551; get_combined_mask( self, names )"},{"location":"src/HistStruct/#get95masknames-self","text":"return a simple list of all mask names in the current HistStruct","title":"&#10551; get_masknames( self )"},{"location":"src/HistStruct/#get95runnbs-self-masknamesnone","text":"get the array of run numbers, optionally after masking input arguments: - masknames: list of names of masks (default: no masking, return full array)","title":"&#10551; get_runnbs( self, masknames=None )"},{"location":"src/HistStruct/#get95lsnbs-self-masknamesnone","text":"get the array of lumisection numbers, optionally after masking input arguments: - masknames: list of names of masks (default: no masking, return full array)","title":"&#10551; get_lsnbs( self, masknames=None )"},{"location":"src/HistStruct/#get95index-self-runnb-lsnb","text":"get the index in the current HistStruct of a given run and lumisection number input arguments: - runnb and lsnb: run and lumisection number respectively","title":"&#10551; get_index( self, runnb, lsnb )"},{"location":"src/HistStruct/#get95scores-self-histnamenone-masknamesnone","text":"get the array of scores for a given histogram type, optionally after masking input arguments: - histname: name of the histogram type for which to retrieve the score. if None, return a dict matching histnames to arrays of scores - masknames: list of names of masks (default: no masking, return full array) notes: - this method takes the scores from the HistStruct.scores attribute; make sure to have evaluated the classifiers before calling this method, else an exception will be thrown.","title":"&#10551; get_scores( self, histname=None, masknames=None )"},{"location":"src/HistStruct/#get95scores95array-self-masknamesnone","text":"similar to get_scores, but with different return type: np array of shape (nhistograms, nhistogramtypes)","title":"&#10551; get_scores_array( self, masknames=None )"},{"location":"src/HistStruct/#get95extscores-self-extname-histnamenone","text":"get the array of scores for a given histogram type in a given extra set. input arguments: - extname: name of the extra set (see also add_exthistograms) - histname: name of the histogram type for which to retrieve the score. if None, return a dict matching histnames to arrays of scores notes: - this method takes the scores from the HistStruct.extscores attribute; make sure to have evaluated the classifiers before calling this method, else an exception will be thrown.","title":"&#10551; get_extscores( self, extname, histname=None )"},{"location":"src/HistStruct/#get95extscores95array-self-extname","text":"similar to get_extscores, but with different return type: np array of shape (nhistograms, nhistogramtypes)","title":"&#10551; get_extscores_array( self, extname )"},{"location":"src/HistStruct/#get95scores95ls-self-runnb-lsnb-histnamesnone-suppresswarningsfalse","text":"get the scores for a given run/lumisection number and for given histogram names input arguments: - runnb: run number - lsnb: lumisection number - histnames: names of the histogram types for which to retrieve the score. returns: - a dict matching each name in histnames to a score (or None if no valid score) notes: - this method takes the scores from the HistStruct.scores attribute; make sure to have evaluated the classifiers before calling this method, else the returned scores will be None.","title":"&#10551; get_scores_ls( self, runnb, lsnb, histnames=None, suppresswarnings=False )"},{"location":"src/HistStruct/#get95globalscores-self-masknamesnone","text":"get the array of global scores, optionally after masking input arguments: - masknames: list of names of masks (default: no masking, return full array) notes: - this method takes the scores from the HistStruct.globalscores attribute; make sure to have set this attribute with add_globalscores, else an exception will be thrown.","title":"&#10551; get_globalscores( self, masknames=None )"},{"location":"src/HistStruct/#get95globalscore95ls-self-runnb-lsnb","text":"get the global score for a given run/lumisection number input arguments: - runnb: run number - lsnb: lumisection number - histnames: names of the histogram types for which to retrieve the score. returns: - a dict matching each name in histnames to a score (or None if no valid score) notes: - this method takes the scores from the HistStruct.scores attribute; make sure to have evaluated the classifiers before calling this method, else the returned scores will be None.","title":"&#10551; get_globalscore_ls( self, runnb, lsnb )"},{"location":"src/HistStruct/#get95extglobalscores-self-extname","text":"get the array of global scores for one of the extra histogram sets input arguments: - extname: name of the extra histogram set notes: - this method takes the scores from the HistStruct.extglobalscores attribute; make sure to have set this attribute with add_extglobalscores, else an exception will be thrown.","title":"&#10551; get_extglobalscores( self, extname )"},{"location":"src/HistStruct/#get95histograms-self-histnamenone-masknamesnone","text":"get the array of histograms for a given type, optionally after masking input arguments: - histname: name of the histogram type to retrieve if None, return a dict matching histnames to arrays of histograms - masknames: list of names of masks (default: no masking, return full array)","title":"&#10551; get_histograms( self, histname=None, masknames=None )"},{"location":"src/HistStruct/#get95exthistograms-self-extname-histnamenone","text":"get the array of extra histograms for a given set name and type name input arguments: - extname: name of the set of extra histograms (see also add_exthistograms) - histname: name of the histogram type to retrieve if None, return a dict matching histnames to arrays of histograms","title":"&#10551; get_exthistograms( self, extname, histname=None )"},{"location":"src/HistStruct/#add95classifier-self-histname-classifier-evaluatefalse","text":"add a histogram classifier for a given histogram name to the HistStruct input arguments: - histname: a valid histogram name present in the HistStruct to which this classifier applies - classifier: an object of type HistogramClassifier (i.e. of any class that derives from it) - evaluate: a bool whether to evaluate the classifier (and store the result in the 'scores' attribute) if set to True, the result is both returned and stored in the 'scores' attribute.","title":"&#10551; add_classifier( self, histname, classifier, evaluate=False )"},{"location":"src/HistStruct/#evaluate95classifier-self-histname-extnamenone","text":"evaluate a histogram classifier for a given histogram name in the HistStruct input arguments: - histname: a valid histogram name present in the HistStruct for which to evaluate the classifier - extname: name of a set of extra histograms (see add_exthistograms) if None, will evaluate the classifer for the main set of histograms notes: - the result is both returned and stored in the 'scores' attribute","title":"&#10551; evaluate_classifier( self, histname, extname=None )"},{"location":"src/HistStruct/#plot95histograms-self-histnamesnone-masknamesnone-colorlist-labellist-transparencylist-titledictnone-xaxtitledictnone-physicalxaxfalse-yaxtitledictnone-kwargs","text":"plot the histograms in a HistStruct, optionally after msking note: so far only for 1D hsitograms. case of 2D histograms requires different plotting method since they cannot be clearly overlaid. if a HistStruct contains both 1D and 2D histograms, the 1D histograms must be selected with the histnames argument. input arguments: - histnames: list of names of the histogram types to plot (default: all) - masknames: list of list of mask names note: each element in masknames represents a set of masks to apply; the histograms passing different sets of masks are plotted in different colors - colorlist: list of matplotlib colors, must have same length as masknames - labellist: list of labels for the legend, must have same legnth as masknames - transparencylist: list of transparency values, must have same length as masknames - titledict: dict mapping histogram names to titles for the subplots (default: title = histogram name) - xaxtitledict: dict mapping histogram names to x-axis titles for the subplots (default: no x-axis title) - yaxtitledict: dict mapping histogram names to y-axis titles for the subplots (default: no y-axis title) - physicalxax: bool whether to use physical x-axis range or simply use bin number (default) - kwargs: keyword arguments passed down to plot_utils.plot_sets","title":"&#10551; plot_histograms( self, histnames=None, masknames=None, colorlist=[], labellist=[], transparencylist=[],  titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs )"},{"location":"src/HistStruct/#plot95ls-self-runnb-lsnb-histnamesnone-histlabelnone-recohistnone-recohistlabelreconstruction-refhistsnone-refhistslabelreference-histograms-refhiststransparencynone-titledictnone-xaxtitledictnone-physicalxaxfalse-yaxtitledictnone-kwargs","text":"plot the histograms in a HistStruct for a given run/ls number versus their references and/or their reconstruction note: so far only for 1D histograms. case of 2D histograms requires different plotting method since they cannot be clearly overlaid. if a HistStruct contains both 1D and 2D histograms, the 1D histograms must be selected with the histnames argument. input arguments: - runnb: run number - lsnb: lumisection number - histnames: names of histogram types to plot (default: all) - histlabel: legend entry for the histogram (default: run and lumisection number) - recohist: dict matching histogram names to reconstructed histograms notes: - 'reconstructed histograms' refers to e.g. autoencoder or NMF reconstructions; some models (e.g. simply looking at histogram moments) might not have this kind of reconstruction - in principle one histogram per key is expected, but still the the shape must be 2D (i.e. (1,nbins)) - in case recohist is set to 'auto', the reconstruction is calculated on the fly for the input histograms - recohistlabel: legend entry for the reco histograms - refhists: dict matching histogram names to reference histograms notes: - multiple histograms (i.e. a 2D array) per key are expected; in case there is only one reference histogram, it must be reshaped into (1,nbins) - refhistslabel: legend entry for the reference histograms - titledict: dict mapping histogram names to titles for the subplots (default: title = histogram name) - xaxtitledict: dict mapping histogram names to x-axis titles for the subplots (default: no x-axis title) - yaxtitledict: dict mapping histogram names to y-axis titles for the subplots (default: no y-axis title) - physicalxax: bool whether to use physical x-axis range or simply use bin number (default) - kwargs: keyword arguments passed down to plot_utils.plot_sets","title":"&#10551; plot_ls( self, runnb, lsnb, histnames=None, histlabel=None,  recohist=None, recohistlabel='Reconstruction',  refhists=None, refhistslabel='Reference histograms', refhiststransparency=None, titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs)"},{"location":"src/HistStruct/#plot95run-self-runnb-masknamesnone-recohistnone-recohistlabelreco-refhistsnone-refhistslabelreference-doprintfalse","text":"call plot_ls for all lumisections in a given run","title":"&#10551; plot_run( self, runnb, masknames=None, recohist=None, recohistlabel='reco', refhists=None, refhistslabel='reference', doprint=False)"},{"location":"src/PlotStyleParser/","text":"PlotStyleParser [class] PlotStyleParser (no valid documentation found) \u2937 __init__(self, jsonfile=None) (no valid documentation found) \u2937 get_general_plot_options(self) (no valid documentation found) \u2937 get_general_plot_option(self, attribute, histname=None) (no valid documentation found) \u2937 get_title(self, histname=None) (no valid documentation found) \u2937 get_titlesize(self) (no valid documentation found) \u2937 get_xaxtitle(self, histname=None) (no valid documentation found) \u2937 get_xaxtitlesize(self) (no valid documentation found) \u2937 get_physicalxax(self) (no valid documentation found) \u2937 get_yaxtitle(self, histname=None) (no valid documentation found) \u2937 get_yaxtitlesize(self) (no valid documentation found) \u2937 get_ymaxfactor(self) (no valid documentation found) \u2937 get_extratext(self, histname=None) (no valid documentation found) \u2937 get_extratextsize(self) (no valid documentation found) \u2937 get_legendsize(self) (no valid documentation found) \u2937 get_extracmstext(self) (no valid documentation found) \u2937 get_cmstextsize(self) (no valid documentation found) \u2937 get_condtext(self) (no valid documentation found) \u2937 get_condtextsize(self) (no valid documentation found)","title":"PlotStyleParser"},{"location":"src/PlotStyleParser/#plotstyleparser","text":"","title":"PlotStyleParser"},{"location":"src/PlotStyleParser/#class-plotstyleparser","text":"(no valid documentation found)","title":"[class] PlotStyleParser"},{"location":"src/PlotStyleParser/#9595init9595self-jsonfilenone","text":"(no valid documentation found)","title":"&#10551; __init__(self, jsonfile=None)"},{"location":"src/PlotStyleParser/#get95general95plot95optionsself","text":"(no valid documentation found)","title":"&#10551; get_general_plot_options(self)"},{"location":"src/PlotStyleParser/#get95general95plot95optionself-attribute-histnamenone","text":"(no valid documentation found)","title":"&#10551; get_general_plot_option(self, attribute, histname=None)"},{"location":"src/PlotStyleParser/#get95titleself-histnamenone","text":"(no valid documentation found)","title":"&#10551; get_title(self, histname=None)"},{"location":"src/PlotStyleParser/#get95titlesizeself","text":"(no valid documentation found)","title":"&#10551; get_titlesize(self)"},{"location":"src/PlotStyleParser/#get95xaxtitleself-histnamenone","text":"(no valid documentation found)","title":"&#10551; get_xaxtitle(self, histname=None)"},{"location":"src/PlotStyleParser/#get95xaxtitlesizeself","text":"(no valid documentation found)","title":"&#10551; get_xaxtitlesize(self)"},{"location":"src/PlotStyleParser/#get95physicalxaxself","text":"(no valid documentation found)","title":"&#10551; get_physicalxax(self)"},{"location":"src/PlotStyleParser/#get95yaxtitleself-histnamenone","text":"(no valid documentation found)","title":"&#10551; get_yaxtitle(self, histname=None)"},{"location":"src/PlotStyleParser/#get95yaxtitlesizeself","text":"(no valid documentation found)","title":"&#10551; get_yaxtitlesize(self)"},{"location":"src/PlotStyleParser/#get95ymaxfactorself","text":"(no valid documentation found)","title":"&#10551; get_ymaxfactor(self)"},{"location":"src/PlotStyleParser/#get95extratextself-histnamenone","text":"(no valid documentation found)","title":"&#10551; get_extratext(self, histname=None)"},{"location":"src/PlotStyleParser/#get95extratextsizeself","text":"(no valid documentation found)","title":"&#10551; get_extratextsize(self)"},{"location":"src/PlotStyleParser/#get95legendsizeself","text":"(no valid documentation found)","title":"&#10551; get_legendsize(self)"},{"location":"src/PlotStyleParser/#get95extracmstextself","text":"(no valid documentation found)","title":"&#10551; get_extracmstext(self)"},{"location":"src/PlotStyleParser/#get95cmstextsizeself","text":"(no valid documentation found)","title":"&#10551; get_cmstextsize(self)"},{"location":"src/PlotStyleParser/#get95condtextself","text":"(no valid documentation found)","title":"&#10551; get_condtext(self)"},{"location":"src/PlotStyleParser/#get95condtextsizeself","text":"(no valid documentation found)","title":"&#10551; get_condtextsize(self)"},{"location":"src/classifiers/AutoEncoder/","text":"AutoEncoder Histogram classfier based on the MSE of an autoencoder reconstruction The AutoEncoder derives from the generic HistogramClassifier. For this specific classifier, the output score of a histogram is the mean-square-error (MSE) between the original histogram and its autoencoder reconstruction. In essence, it is just a wrapper for a tensorflow model. [class] AutoEncoder(HistogramClassifier) histogram classfier based on the MSE of an autoencoder reconstruction the AutoEncoder derives from the generic HistogramClassifier. for this specific classifier, the output score of a histogram is the mean-square-error (MSE) between the original histogram and its autoencoder reconstruction. in essence, it is just a wrapper for a tensorflow model. \u2937 __init__( self, model=None, modelpath=None ) intializer from a tensorflow model input arguments: - model: a valid tensorflow model; it does not have to be trained already, the AutoEncoder.train function will take care of this. - modelpath: path to a stored tensorflow model, it does not have to be trained already, the AutoEncoder.train function will take care of this. note: model and modelpath are alternative options, they should not both be used simultaneously. \u2937 train( self, histograms, doplot=True, epochs=10, batch_size=500, shuffle=False, verbose=1, validation_split=0.1, **kwargs ) train the model on a given set of input histograms input arguments: - histograms: set of training histograms, a numpy array of shape (nhistograms,nbins) - doplot: boolean whether to make a plot of the loss value - others: see the keras fit function - kwargs: additional arguments passed down to keras fit function \u2937 evaluate( self, histograms ) classification of a collection of histograms based on their autoencoder reconstruction \u2937 reconstruct( self, histograms ) return the autoencoder reconstruction of a set of histograms \u2937 save( self, path ) save the underlying tensorflow model to a tensorflow SavedModel or H5 format. note: depending on the extension specified in path, the SavedModel or H5 format is chosen, see https://www.tensorflow.org/guide/keras/save_and_serialize \u2937 load( self, path, **kwargs ) get an AutoEncoder instance from a saved tensorflow SavedModel or H5 file [class] classifier = AutoEncoder( model=model ) (no valid documentation found)","title":"AutoEncoder"},{"location":"src/classifiers/AutoEncoder/#autoencoder","text":"Histogram classfier based on the MSE of an autoencoder reconstruction The AutoEncoder derives from the generic HistogramClassifier. For this specific classifier, the output score of a histogram is the mean-square-error (MSE) between the original histogram and its autoencoder reconstruction. In essence, it is just a wrapper for a tensorflow model.","title":"AutoEncoder"},{"location":"src/classifiers/AutoEncoder/#class-autoencoderhistogramclassifier","text":"histogram classfier based on the MSE of an autoencoder reconstruction the AutoEncoder derives from the generic HistogramClassifier. for this specific classifier, the output score of a histogram is the mean-square-error (MSE) between the original histogram and its autoencoder reconstruction. in essence, it is just a wrapper for a tensorflow model.","title":"[class] AutoEncoder(HistogramClassifier)"},{"location":"src/classifiers/AutoEncoder/#9595init9595-self-modelnone-modelpathnone","text":"intializer from a tensorflow model input arguments: - model: a valid tensorflow model; it does not have to be trained already, the AutoEncoder.train function will take care of this. - modelpath: path to a stored tensorflow model, it does not have to be trained already, the AutoEncoder.train function will take care of this. note: model and modelpath are alternative options, they should not both be used simultaneously.","title":"&#10551; __init__( self, model=None, modelpath=None )"},{"location":"src/classifiers/AutoEncoder/#train-self-histograms-doplottrue-epochs10-batch95size500-shufflefalse-verbose1-validation95split01-kwargs","text":"train the model on a given set of input histograms input arguments: - histograms: set of training histograms, a numpy array of shape (nhistograms,nbins) - doplot: boolean whether to make a plot of the loss value - others: see the keras fit function - kwargs: additional arguments passed down to keras fit function","title":"&#10551; train( self, histograms, doplot=True, epochs=10, batch_size=500, shuffle=False, verbose=1, validation_split=0.1, **kwargs )"},{"location":"src/classifiers/AutoEncoder/#evaluate-self-histograms","text":"classification of a collection of histograms based on their autoencoder reconstruction","title":"&#10551; evaluate( self, histograms )"},{"location":"src/classifiers/AutoEncoder/#reconstruct-self-histograms","text":"return the autoencoder reconstruction of a set of histograms","title":"&#10551; reconstruct( self, histograms )"},{"location":"src/classifiers/AutoEncoder/#save-self-path","text":"save the underlying tensorflow model to a tensorflow SavedModel or H5 format. note: depending on the extension specified in path, the SavedModel or H5 format is chosen, see https://www.tensorflow.org/guide/keras/save_and_serialize","title":"&#10551; save( self, path )"},{"location":"src/classifiers/AutoEncoder/#load-self-path-kwargs","text":"get an AutoEncoder instance from a saved tensorflow SavedModel or H5 file","title":"&#10551; load( self, path, **kwargs )"},{"location":"src/classifiers/AutoEncoder/#class-classifier-autoencoder-modelmodel","text":"(no valid documentation found)","title":"[class] classifier = AutoEncoder( model=model )"},{"location":"src/classifiers/HistogramClassifier/","text":"HistogramClassifier Abstract base class for histogram classifying objects Note that all concrete histogram classifiers must inherit from HistogramClassifier! A HistogramClassifier can be any object that classifies a histogram; in more detail: - the input is a collection of histograms (of the same type), represented by a numpy array of shape (nhists,nbins) for 1D histograms or (nhists,nybins,nxbins) for 2D histograms. - the output is an array of numbers of shape (nhists). - the processing between input and output can in principle be anything, but usually some sort of discriminating power is assumed. How to make a concrete HistogramClassifier class: - define a class that inherits from HistogramClassifier - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary See also the existing examples! [class] HistogramClassifier(ABC) abstract base class for histogram classifying objects note that all concrete histogram classifiers must inherit from HistogramClassifier! a HistogramClassifier can be any object that classifies a histogram; in more detail: - the input is a collection of histograms (of the same type), represented by a numpy array of shape (nhists,nbins) for 1D histograms or (nhists,nybins,nxbins) for 2D histograms. - the output is an array of numbers of shape (nhists). - the processing between input and output can in principle be anything, but usually some sort of discriminating power is assumed. how to make a concrete HistogramClassifier class: - define a class that inherits from HistogramClassifier - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary see also the existing examples! \u2937 __init__( self ) empty intializer this is an @abstractmethod and must be overridden in any concrete deriving class! \u2937 train( self, histograms ) train the classifier on a set of input histograms this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins). output: expected to be none. \u2937 evaluate( self, histograms ) main function used to evaluate a set of histograms this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins). output: expected to be a 1D numpy array of shape (nhists), one number per histogram. \u2937 save( self, path ) save a classifier to disk specific implementation in concrete classes, here only path creation \u2937 load( self, path ) load a classifier object from disk specific implementation in concrete classes, here only path checking","title":"HistogramClassifier"},{"location":"src/classifiers/HistogramClassifier/#histogramclassifier","text":"Abstract base class for histogram classifying objects Note that all concrete histogram classifiers must inherit from HistogramClassifier! A HistogramClassifier can be any object that classifies a histogram; in more detail: - the input is a collection of histograms (of the same type), represented by a numpy array of shape (nhists,nbins) for 1D histograms or (nhists,nybins,nxbins) for 2D histograms. - the output is an array of numbers of shape (nhists). - the processing between input and output can in principle be anything, but usually some sort of discriminating power is assumed. How to make a concrete HistogramClassifier class: - define a class that inherits from HistogramClassifier - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary See also the existing examples!","title":"HistogramClassifier"},{"location":"src/classifiers/HistogramClassifier/#class-histogramclassifierabc","text":"abstract base class for histogram classifying objects note that all concrete histogram classifiers must inherit from HistogramClassifier! a HistogramClassifier can be any object that classifies a histogram; in more detail: - the input is a collection of histograms (of the same type), represented by a numpy array of shape (nhists,nbins) for 1D histograms or (nhists,nybins,nxbins) for 2D histograms. - the output is an array of numbers of shape (nhists). - the processing between input and output can in principle be anything, but usually some sort of discriminating power is assumed. how to make a concrete HistogramClassifier class: - define a class that inherits from HistogramClassifier - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary see also the existing examples!","title":"[class] HistogramClassifier(ABC)"},{"location":"src/classifiers/HistogramClassifier/#9595init9595-self","text":"empty intializer this is an @abstractmethod and must be overridden in any concrete deriving class!","title":"&#10551; __init__( self )"},{"location":"src/classifiers/HistogramClassifier/#train-self-histograms","text":"train the classifier on a set of input histograms this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins). output: expected to be none.","title":"&#10551; train( self, histograms )"},{"location":"src/classifiers/HistogramClassifier/#evaluate-self-histograms","text":"main function used to evaluate a set of histograms this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins). output: expected to be a 1D numpy array of shape (nhists), one number per histogram.","title":"&#10551; evaluate( self, histograms )"},{"location":"src/classifiers/HistogramClassifier/#save-self-path","text":"save a classifier to disk specific implementation in concrete classes, here only path creation","title":"&#10551; save( self, path )"},{"location":"src/classifiers/HistogramClassifier/#load-self-path","text":"load a classifier object from disk specific implementation in concrete classes, here only path checking","title":"&#10551; load( self, path )"},{"location":"src/classifiers/MaxPullClassifier/","text":"MaxPullClassifier Histogram classification based on maximum pull between test histogram and reference histogram. Specifically intended for 2D histograms, but should in principle work for 1D as well. Ssee static function 'pull' for definition of bin-per-bin pull and other notes. pull( testhist, refhist ) calculate bin-per-bin pull between two histograms bin-per-bin pull is defined here preliminarily as (testhist(bin)-refhist(bin))/sqrt(refhist(bin)) notes: - bins in the denominator where refhist is < 1 are set to one! This is for histograms with absolute counts, and they should not be normalized! - instead another normalization is applied: the test histogram is multiplied by sum(refhist)/sum(testhist) before computing the pulls input arguments: - testhist, refhist: numpy arrays of the same shape output: numpy array of same shape as testhist and refhist maxabspull( testhist, refhist, n=1 ) calculate maximum of bin-per-bin pulls (in absolute value) between two histograms see definition of bin-per-bin pull in function pull (above) input arguments: - testhist, refhist: numpy arrays of the same shape - n: nubmer of largest pull values to average over (default: 1, just take single maximum) output: a float [class] MaxPullClassifier(HistogramClassifier) histogram classification based on maximum pull between test histogram and reference histogram. specifically intended for 2D histograms, but should in principle work for 1D as well. see static function pull (above) for definition of bin-per-bin pull and other notes. \u2937 __init__( self, nmaxpulls=1 ) initializer input arguments: - nmaxpulls: number of largest pull values to average over (default: 1, just take single maximum) \u2937 set_nmaxpulls( self, nmaxpulls ) set the nmaxpulls parameter (see also initializer) \u2937 train( self, refhist ) 'train' the classifier, i.e. set the reference histogram. input arguments: - refhist: a numpy array of shape (1,nbins) or (1,nybins,nxbins) \u2937 evaluate( self, histograms ) classify the histograms based on their max bin-per-bin pull (in absolute value) with respect to a reference histogram \u2937 getpull( self, histogram ) get the pull histogram for a given test histogram input arguments: histogram: a single histogram, i.e. numpy array of shape (nbins) for 1D or (nybins,nxbins) for 2D. output: numpy array of same shape as histogram containing bin-per-bin pull w.r.t. reference histogram","title":"MaxPullClassifier"},{"location":"src/classifiers/MaxPullClassifier/#maxpullclassifier","text":"Histogram classification based on maximum pull between test histogram and reference histogram. Specifically intended for 2D histograms, but should in principle work for 1D as well. Ssee static function 'pull' for definition of bin-per-bin pull and other notes.","title":"MaxPullClassifier"},{"location":"src/classifiers/MaxPullClassifier/#pull-testhist-refhist","text":"calculate bin-per-bin pull between two histograms bin-per-bin pull is defined here preliminarily as (testhist(bin)-refhist(bin))/sqrt(refhist(bin)) notes: - bins in the denominator where refhist is < 1 are set to one! This is for histograms with absolute counts, and they should not be normalized! - instead another normalization is applied: the test histogram is multiplied by sum(refhist)/sum(testhist) before computing the pulls input arguments: - testhist, refhist: numpy arrays of the same shape output: numpy array of same shape as testhist and refhist","title":"pull( testhist, refhist )"},{"location":"src/classifiers/MaxPullClassifier/#maxabspull-testhist-refhist-n1","text":"calculate maximum of bin-per-bin pulls (in absolute value) between two histograms see definition of bin-per-bin pull in function pull (above) input arguments: - testhist, refhist: numpy arrays of the same shape - n: nubmer of largest pull values to average over (default: 1, just take single maximum) output: a float","title":"maxabspull( testhist, refhist, n=1 )"},{"location":"src/classifiers/MaxPullClassifier/#class-maxpullclassifierhistogramclassifier","text":"histogram classification based on maximum pull between test histogram and reference histogram. specifically intended for 2D histograms, but should in principle work for 1D as well. see static function pull (above) for definition of bin-per-bin pull and other notes.","title":"[class] MaxPullClassifier(HistogramClassifier)"},{"location":"src/classifiers/MaxPullClassifier/#9595init9595-self-nmaxpulls1","text":"initializer input arguments: - nmaxpulls: number of largest pull values to average over (default: 1, just take single maximum)","title":"&#10551; __init__( self, nmaxpulls=1 )"},{"location":"src/classifiers/MaxPullClassifier/#set95nmaxpulls-self-nmaxpulls","text":"set the nmaxpulls parameter (see also initializer)","title":"&#10551; set_nmaxpulls( self, nmaxpulls )"},{"location":"src/classifiers/MaxPullClassifier/#train-self-refhist","text":"'train' the classifier, i.e. set the reference histogram. input arguments: - refhist: a numpy array of shape (1,nbins) or (1,nybins,nxbins)","title":"&#10551; train( self, refhist )"},{"location":"src/classifiers/MaxPullClassifier/#evaluate-self-histograms","text":"classify the histograms based on their max bin-per-bin pull (in absolute value) with respect to a reference histogram","title":"&#10551; evaluate( self, histograms )"},{"location":"src/classifiers/MaxPullClassifier/#getpull-self-histogram","text":"get the pull histogram for a given test histogram input arguments: histogram: a single histogram, i.e. numpy array of shape (nbins) for 1D or (nybins,nxbins) for 2D. output: numpy array of same shape as histogram containing bin-per-bin pull w.r.t. reference histogram","title":"&#10551; getpull( self, histogram )"},{"location":"src/classifiers/NMFClassifier/","text":"NMFClassifier Histogram classification based on nonnegative matrix factorization Specifically intended for 2D histograms, but should in principle work for 1D as well. It is basically a wrapper for a sklearn.decomposition.NMF instance. [class] NMFClassifier(HistogramClassifier) histogram classification based on nonnegative matrix factorization specifically intended for 2D histograms, but should in principle work for 1D as well. it is basically a wrapper for a sklearn.decomposition.NMF instance. \u2937 __init__( self, ncomponents=5, loss_type='mse', nmax=10 ) initializer input arguments: - ncomponents: number of NMF components (aka clusters aka basis vectors) to use in the decomposition - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error) - nmax: number of largest elements to keep in error calculation TODO: add keyword arguments to pass down to sklearn.decomposition.NMF \u2937 train( self, histograms ) train the NMF model on a given set of input histograms input arguments: - histograms: a numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins) that will be used to fit a NMF model \u2937 set_nmax( self, nmax ) set number of largest elements to keep in mean square error calculation useful to quickly re-evaluate the model with different nmax without retraining input arguments: - nmax: number of largest elements to keep in mean square error calculation \u2937 set_loss_type( self, loss_type ) set loss type useful to quickly re-evaluate the model with different loss without retraining input arguments: - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error) \u2937 evaluate( self, histograms ) classify the given histograms based on the MSE with respect to their reconstructed version input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins) \u2937 get_components( self ) return the NMF components (aka cluster centers aka basis vectors) output: a numpy array of shape (ncomponents,nbins) or (ncomponents,nybins,nxbins) \u2937 reconstruct( self, histograms ) return the NMF reconstruction for a given set of histograms input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins)","title":"NMFClassifier"},{"location":"src/classifiers/NMFClassifier/#nmfclassifier","text":"Histogram classification based on nonnegative matrix factorization Specifically intended for 2D histograms, but should in principle work for 1D as well. It is basically a wrapper for a sklearn.decomposition.NMF instance.","title":"NMFClassifier"},{"location":"src/classifiers/NMFClassifier/#class-nmfclassifierhistogramclassifier","text":"histogram classification based on nonnegative matrix factorization specifically intended for 2D histograms, but should in principle work for 1D as well. it is basically a wrapper for a sklearn.decomposition.NMF instance.","title":"[class] NMFClassifier(HistogramClassifier)"},{"location":"src/classifiers/NMFClassifier/#9595init9595-self-ncomponents5-loss95typemse-nmax10","text":"initializer input arguments: - ncomponents: number of NMF components (aka clusters aka basis vectors) to use in the decomposition - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error) - nmax: number of largest elements to keep in error calculation TODO: add keyword arguments to pass down to sklearn.decomposition.NMF","title":"&#10551; __init__( self, ncomponents=5, loss_type='mse', nmax=10 )"},{"location":"src/classifiers/NMFClassifier/#train-self-histograms","text":"train the NMF model on a given set of input histograms input arguments: - histograms: a numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins) that will be used to fit a NMF model","title":"&#10551; train( self, histograms )"},{"location":"src/classifiers/NMFClassifier/#set95nmax-self-nmax","text":"set number of largest elements to keep in mean square error calculation useful to quickly re-evaluate the model with different nmax without retraining input arguments: - nmax: number of largest elements to keep in mean square error calculation","title":"&#10551; set_nmax( self, nmax )"},{"location":"src/classifiers/NMFClassifier/#set95loss95type-self-loss95type","text":"set loss type useful to quickly re-evaluate the model with different loss without retraining input arguments: - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error)","title":"&#10551; set_loss_type( self, loss_type )"},{"location":"src/classifiers/NMFClassifier/#evaluate-self-histograms","text":"classify the given histograms based on the MSE with respect to their reconstructed version input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins)","title":"&#10551; evaluate( self, histograms )"},{"location":"src/classifiers/NMFClassifier/#get95components-self","text":"return the NMF components (aka cluster centers aka basis vectors) output: a numpy array of shape (ncomponents,nbins) or (ncomponents,nybins,nxbins)","title":"&#10551; get_components( self )"},{"location":"src/classifiers/NMFClassifier/#reconstruct-self-histograms","text":"return the NMF reconstruction for a given set of histograms input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins)","title":"&#10551; reconstruct( self, histograms )"},{"location":"src/classifiers/PCAClassifier/","text":"PCAClassifier Histogram classification based on principal component analysis It is basically a wrapper for a sklearn.decomposition.PCA instance. [class] PCAClassifier(HistogramClassifier) histogram classification based on principal component analysis it is basically a wrapper for a sklearn.decomposition.PCA instance. \u2937 __init__( self, ncomponents=None, svd_solver='auto', loss_type='mse', nmax=10 ) initializer input arguments: - ncomponents: number of PCA components (aka clusters aka basis vectors) to use in the decomposition - svd_solver: solver method to extract the PCA components note: both ncomponents and svd_solver are arguments passed down to sklearn.decomposition.PCA, see https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error) - nmax: number of largest elements to keep in error calculation TODO: add keyword arguments to pass down to sklearn.decomposition.PCA \u2937 train( self, histograms ) train the PCA model on a given set of input histograms input arguments: - histograms: a numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins) that will be used to fit a PCA model \u2937 set_nmax( self, nmax ) set number of largest elements to keep in mean square error calculation useful to quickly re-evaluate the model with different nmax without retraining input arguments: - nmax: number of largest elements to keep in mean square error calculation \u2937 set_loss_type( self, loss_type ) set loss type useful to quickly re-evaluate the model with different loss without retraining input arguments: - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error) \u2937 evaluate( self, histograms ) classify the given histograms based on the MSE with respect to their reconstructed version input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins) \u2937 get_components( self ) return the PCA components (aka cluster centers aka basis vectors) output: a numpy array of shape (ncomponents,nbins) or (ncomponents,nybins,nxbins) \u2937 reconstruct( self, histograms ) return the PCA reconstruction for a given set of histograms input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins)","title":"PCAClassifier"},{"location":"src/classifiers/PCAClassifier/#pcaclassifier","text":"Histogram classification based on principal component analysis It is basically a wrapper for a sklearn.decomposition.PCA instance.","title":"PCAClassifier"},{"location":"src/classifiers/PCAClassifier/#class-pcaclassifierhistogramclassifier","text":"histogram classification based on principal component analysis it is basically a wrapper for a sklearn.decomposition.PCA instance.","title":"[class] PCAClassifier(HistogramClassifier)"},{"location":"src/classifiers/PCAClassifier/#9595init9595-self-ncomponentsnone-svd95solverauto-loss95typemse-nmax10","text":"initializer input arguments: - ncomponents: number of PCA components (aka clusters aka basis vectors) to use in the decomposition - svd_solver: solver method to extract the PCA components note: both ncomponents and svd_solver are arguments passed down to sklearn.decomposition.PCA, see https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error) - nmax: number of largest elements to keep in error calculation TODO: add keyword arguments to pass down to sklearn.decomposition.PCA","title":"&#10551; __init__( self, ncomponents=None, svd_solver='auto', loss_type='mse', nmax=10 )"},{"location":"src/classifiers/PCAClassifier/#train-self-histograms","text":"train the PCA model on a given set of input histograms input arguments: - histograms: a numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins) that will be used to fit a PCA model","title":"&#10551; train( self, histograms )"},{"location":"src/classifiers/PCAClassifier/#set95nmax-self-nmax","text":"set number of largest elements to keep in mean square error calculation useful to quickly re-evaluate the model with different nmax without retraining input arguments: - nmax: number of largest elements to keep in mean square error calculation","title":"&#10551; set_nmax( self, nmax )"},{"location":"src/classifiers/PCAClassifier/#set95loss95type-self-loss95type","text":"set loss type useful to quickly re-evaluate the model with different loss without retraining input arguments: - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error)","title":"&#10551; set_loss_type( self, loss_type )"},{"location":"src/classifiers/PCAClassifier/#evaluate-self-histograms","text":"classify the given histograms based on the MSE with respect to their reconstructed version input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins)","title":"&#10551; evaluate( self, histograms )"},{"location":"src/classifiers/PCAClassifier/#get95components-self","text":"return the PCA components (aka cluster centers aka basis vectors) output: a numpy array of shape (ncomponents,nbins) or (ncomponents,nybins,nxbins)","title":"&#10551; get_components( self )"},{"location":"src/classifiers/PCAClassifier/#reconstruct-self-histograms","text":"return the PCA reconstruction for a given set of histograms input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins)","title":"&#10551; reconstruct( self, histograms )"},{"location":"src/classifiers/TemplateBasedClassifier/","text":"TemplateBasedClassifier Histogram classifier based on a direct comparison with templates (i.e. reference histograms) mseTopN_templates( histograms, templates, n=-1 ) calculate the mse between each histogram in histograms and each histogram in templates input arguments: - histograms: 2D numpy array of shape (nhistograms, nbins) - templates: 2D numpy array of shape (ntemplates,nbins) - n: integer representing the number of (sorted) bin squared errors to take into account (default: all) output: 2D numpy array of shape (nhistograms,ntemplates) holding the mseTopN between each mseTopN_min( histograms, templates, n=-1 ) calculate the mse betwee a histogram and each template and return the minimum input arguments: - histograms: 2D numpy array of shape (nhistograms, nbins) - templates: 2D numpy array of shape (ntemplates,nbins) - n: integer representing the number of (sorted) bin squared errors to take into account (default: all) output: 1D numpy array of shape (nhistograms) holding the minimum mseTopN for each histogram mseTop10_min( histograms, templates ) special case of above with n=10 mseTopN_avg( histograms, templates, n=-1 ) calculate the mse betwee a histogram and each template and return the average input arguments: - histograms: 2D numpy array of shape (nhistograms, nbins) - templates: 2D numpy array of shape (ntemplates,nbins) - n: integer representing the number of (sorted) bin squared errors to take into account (default: all) output: 1D numpy array of shape (nhistograms) holding the average mseTopN for each histogram mseTop10_avg( histograms, templates ) special case of above with n=10 [class] TemplateBasedClassifier(HistogramClassifier) histogram classifier based on a direct comparison with templates (i.e. reference histograms) \u2937 __init__( self, comparemethod='minmse' ) initializer input arguments: - comparemethod: string representing the method by which to compare a histogram with a set of templates currently supported methods are: - minmse: minimum mean square error between histogram and all templates - avgmse: average mean square error between histogram and all templates \u2937 train( self, templates ) 'train' the classifier, i.e. set the templates (reference histograms) input arguments: - templates: a 2D numpy array of shape (nhistograms,nbins) \u2937 evaluate( self, histograms ) classification of a collection of histograms based on their deviation from templates","title":"TemplateBasedClassifier"},{"location":"src/classifiers/TemplateBasedClassifier/#templatebasedclassifier","text":"Histogram classifier based on a direct comparison with templates (i.e. reference histograms)","title":"TemplateBasedClassifier"},{"location":"src/classifiers/TemplateBasedClassifier/#msetopn95templates-histograms-templates-n-1","text":"calculate the mse between each histogram in histograms and each histogram in templates input arguments: - histograms: 2D numpy array of shape (nhistograms, nbins) - templates: 2D numpy array of shape (ntemplates,nbins) - n: integer representing the number of (sorted) bin squared errors to take into account (default: all) output: 2D numpy array of shape (nhistograms,ntemplates) holding the mseTopN between each","title":"mseTopN_templates( histograms, templates, n=-1 )"},{"location":"src/classifiers/TemplateBasedClassifier/#msetopn95min-histograms-templates-n-1","text":"calculate the mse betwee a histogram and each template and return the minimum input arguments: - histograms: 2D numpy array of shape (nhistograms, nbins) - templates: 2D numpy array of shape (ntemplates,nbins) - n: integer representing the number of (sorted) bin squared errors to take into account (default: all) output: 1D numpy array of shape (nhistograms) holding the minimum mseTopN for each histogram","title":"mseTopN_min( histograms, templates, n=-1 )"},{"location":"src/classifiers/TemplateBasedClassifier/#msetop1095min-histograms-templates","text":"special case of above with n=10","title":"mseTop10_min( histograms, templates )"},{"location":"src/classifiers/TemplateBasedClassifier/#msetopn95avg-histograms-templates-n-1","text":"calculate the mse betwee a histogram and each template and return the average input arguments: - histograms: 2D numpy array of shape (nhistograms, nbins) - templates: 2D numpy array of shape (ntemplates,nbins) - n: integer representing the number of (sorted) bin squared errors to take into account (default: all) output: 1D numpy array of shape (nhistograms) holding the average mseTopN for each histogram","title":"mseTopN_avg( histograms, templates, n=-1 )"},{"location":"src/classifiers/TemplateBasedClassifier/#msetop1095avg-histograms-templates","text":"special case of above with n=10","title":"mseTop10_avg( histograms, templates )"},{"location":"src/classifiers/TemplateBasedClassifier/#class-templatebasedclassifierhistogramclassifier","text":"histogram classifier based on a direct comparison with templates (i.e. reference histograms)","title":"[class] TemplateBasedClassifier(HistogramClassifier)"},{"location":"src/classifiers/TemplateBasedClassifier/#9595init9595-self-comparemethodminmse","text":"initializer input arguments: - comparemethod: string representing the method by which to compare a histogram with a set of templates currently supported methods are: - minmse: minimum mean square error between histogram and all templates - avgmse: average mean square error between histogram and all templates","title":"&#10551; __init__( self, comparemethod='minmse' )"},{"location":"src/classifiers/TemplateBasedClassifier/#train-self-templates","text":"'train' the classifier, i.e. set the templates (reference histograms) input arguments: - templates: a 2D numpy array of shape (nhistograms,nbins)","title":"&#10551; train( self, templates )"},{"location":"src/classifiers/TemplateBasedClassifier/#evaluate-self-histograms","text":"classification of a collection of histograms based on their deviation from templates","title":"&#10551; evaluate( self, histograms )"},{"location":"src/cloudfitters/CloudFitter/","text":"CloudFitter Abstract base class for all point cloud fitting algorithms Note that all concrete point cloud fitters must inherit from CloudFitter! How to make a concrete CloudFitter class: - define a class that inherits from CloudFitter - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary See also the existing examples! [class] CloudFitter(ABC) abstract base class for all point cloud fitting algorithms note that all concrete point cloud fitters must inherit from CloudFitter! how to make a concrete CloudFitter class: - define a class that inherits from CloudFitter - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary see also the existing examples! \u2937 __init__( self, points ) default intializer this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - points: 2D numpy array of shape (npoints,ndims) \u2937 pdf( self, points ) evaluate the pdf (probability density function) at given points this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - points: a 2D numpy array of shape (npoints,ndims) output: a 1D array of shape (npoints)","title":"CloudFitter"},{"location":"src/cloudfitters/CloudFitter/#cloudfitter","text":"Abstract base class for all point cloud fitting algorithms Note that all concrete point cloud fitters must inherit from CloudFitter! How to make a concrete CloudFitter class: - define a class that inherits from CloudFitter - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary See also the existing examples!","title":"CloudFitter"},{"location":"src/cloudfitters/CloudFitter/#class-cloudfitterabc","text":"abstract base class for all point cloud fitting algorithms note that all concrete point cloud fitters must inherit from CloudFitter! how to make a concrete CloudFitter class: - define a class that inherits from CloudFitter - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary see also the existing examples!","title":"[class] CloudFitter(ABC)"},{"location":"src/cloudfitters/CloudFitter/#9595init9595-self-points","text":"default intializer this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - points: 2D numpy array of shape (npoints,ndims)","title":"&#10551; __init__( self, points )"},{"location":"src/cloudfitters/CloudFitter/#pdf-self-points","text":"evaluate the pdf (probability density function) at given points this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - points: a 2D numpy array of shape (npoints,ndims) output: a 1D array of shape (npoints)","title":"&#10551; pdf( self, points )"},{"location":"src/cloudfitters/ExponentialFitter/","text":"ExponentialFitter Class for fitting an exponential distribution to a point cloud An exponential distribution in N dimensions is fully determined by an N-dimensional vector, representing the N-dimensional decay parameter (or lambda parameter) of the distribution. [class] ExponentialFitter(CloudFitter) class for fitting an exponential distribution to a point cloud parameters - l: multidimensional lambda parameter of exponential \u2937 __init__(self, points) constructor input arguments: - points: a np array of shape (npoints,ndims) \u2937 pdf(self, points) get pdf at points","title":"ExponentialFitter"},{"location":"src/cloudfitters/ExponentialFitter/#exponentialfitter","text":"Class for fitting an exponential distribution to a point cloud An exponential distribution in N dimensions is fully determined by an N-dimensional vector, representing the N-dimensional decay parameter (or lambda parameter) of the distribution.","title":"ExponentialFitter"},{"location":"src/cloudfitters/ExponentialFitter/#class-exponentialfittercloudfitter","text":"class for fitting an exponential distribution to a point cloud parameters - l: multidimensional lambda parameter of exponential","title":"[class] ExponentialFitter(CloudFitter)"},{"location":"src/cloudfitters/ExponentialFitter/#9595init9595self-points","text":"constructor input arguments: - points: a np array of shape (npoints,ndims)","title":"&#10551; __init__(self, points)"},{"location":"src/cloudfitters/ExponentialFitter/#pdfself-points","text":"get pdf at points","title":"&#10551; pdf(self, points)"},{"location":"src/cloudfitters/GaussianKdeFitter/","text":"GaussianKdeFitter Class for fitting a gaussian kernel density to a point cloud Basically a wrapper for scipy.stats.gaussian_kde. A gaussian kernel density can be thought of as a sum of little (potentially multidimensional) gaussians, each one centered at one of the points in the cloud. Hence, the resulting distribution is a sort of smoothed version of the discrete point cloud. [class] GaussianKdeFitter(CloudFitter) class for fitting a gaussian kernel density to a point cloud basically a wrapper for scipy.stats.gaussian_kde. parameters - kernel: scipy.stats.gaussian_kde object - cov: covariance matrix (use np.cov for now, maybe later replace by internal kernel.covariance) \u2937 __init__(self, points, bw_method='scott', bw_scott_factor=None) constructor input arguments: - points: a np array of shape (npoints,ndims) - bw_method: method to calculate the bandwidth of the gaussians, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html - bw_scott_factor: additional multiplication factor applied to bandwidth in case it is set to 'scott' \u2937 pdf(self,points) get pdf at points","title":"GaussianKdeFitter"},{"location":"src/cloudfitters/GaussianKdeFitter/#gaussiankdefitter","text":"Class for fitting a gaussian kernel density to a point cloud Basically a wrapper for scipy.stats.gaussian_kde. A gaussian kernel density can be thought of as a sum of little (potentially multidimensional) gaussians, each one centered at one of the points in the cloud. Hence, the resulting distribution is a sort of smoothed version of the discrete point cloud.","title":"GaussianKdeFitter"},{"location":"src/cloudfitters/GaussianKdeFitter/#class-gaussiankdefittercloudfitter","text":"class for fitting a gaussian kernel density to a point cloud basically a wrapper for scipy.stats.gaussian_kde. parameters - kernel: scipy.stats.gaussian_kde object - cov: covariance matrix (use np.cov for now, maybe later replace by internal kernel.covariance)","title":"[class] GaussianKdeFitter(CloudFitter)"},{"location":"src/cloudfitters/GaussianKdeFitter/#9595init9595self-points-bw95methodscott-bw95scott95factornone","text":"constructor input arguments: - points: a np array of shape (npoints,ndims) - bw_method: method to calculate the bandwidth of the gaussians, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html - bw_scott_factor: additional multiplication factor applied to bandwidth in case it is set to 'scott'","title":"&#10551; __init__(self, points, bw_method='scott', bw_scott_factor=None)"},{"location":"src/cloudfitters/GaussianKdeFitter/#pdfselfpoints","text":"get pdf at points","title":"&#10551; pdf(self,points)"},{"location":"src/cloudfitters/HyperRectangleFitter/","text":"HyperRectangleFitter Simple fitter making a hard cut in each dimension calculate_cut_values( values, quantile, side='both' ) calculate the appropriate cut values to discard a given quantile of values input arguments: - values: a 1D numpy array - quantile: quantile of values to discard, a float between 0 and 1 (or between 0 and 0.5 for side='both') - side: either 'both', 'down' or 'up' for 'up', the cut will discard the quantile highest values, for 'down', cut will discard the quantile lowest values, for 'both', the cut(s) will discard the quantile values both at the high and low end. returns: - a tuple of shape (lower cut, upper cut), with None entries if not applicable [class] HyperRectangleFitter(CloudFitter) Simple fitter making a hard cut in each dimension \u2937 __init__(self, points, quantiles=0, side='both', verbose=False ) constructor input arguments: - points: a np array of shape (npoints,ndims) - quantiles: quantiles of values to discard. can either be a float between 0 and 1 (applied in all dimensions), or a list of such floats with same length as number of dimensions in points. (note: for side='both', quantiles above 0.5 will discard everything) - side: either 'both', 'down' or 'up' for 'up', the cut will discard the quantile highest values, for 'down', cut will discard the quantile lowest values, for 'both', the cut(s) will discard the quantile values both at the high and low end. \u2937 apply_cuts(self, point) apply the cuts to a point and return whether it passes them input arguments: - point: a 1D numpy array of shape (ndims,) returns: - boolean \u2937 pdf(self, points) get pdf at points note that the pdf is either 0 (does not pass cuts) or 1 (passes cuts)","title":"HyperRectangleFitter"},{"location":"src/cloudfitters/HyperRectangleFitter/#hyperrectanglefitter","text":"Simple fitter making a hard cut in each dimension","title":"HyperRectangleFitter"},{"location":"src/cloudfitters/HyperRectangleFitter/#calculate95cut95values-values-quantile-sideboth","text":"calculate the appropriate cut values to discard a given quantile of values input arguments: - values: a 1D numpy array - quantile: quantile of values to discard, a float between 0 and 1 (or between 0 and 0.5 for side='both') - side: either 'both', 'down' or 'up' for 'up', the cut will discard the quantile highest values, for 'down', cut will discard the quantile lowest values, for 'both', the cut(s) will discard the quantile values both at the high and low end. returns: - a tuple of shape (lower cut, upper cut), with None entries if not applicable","title":"calculate_cut_values( values, quantile, side='both' )"},{"location":"src/cloudfitters/HyperRectangleFitter/#class-hyperrectanglefittercloudfitter","text":"Simple fitter making a hard cut in each dimension","title":"[class] HyperRectangleFitter(CloudFitter)"},{"location":"src/cloudfitters/HyperRectangleFitter/#9595init9595self-points-quantiles0-sideboth-verbosefalse","text":"constructor input arguments: - points: a np array of shape (npoints,ndims) - quantiles: quantiles of values to discard. can either be a float between 0 and 1 (applied in all dimensions), or a list of such floats with same length as number of dimensions in points. (note: for side='both', quantiles above 0.5 will discard everything) - side: either 'both', 'down' or 'up' for 'up', the cut will discard the quantile highest values, for 'down', cut will discard the quantile lowest values, for 'both', the cut(s) will discard the quantile values both at the high and low end.","title":"&#10551; __init__(self, points, quantiles=0, side='both', verbose=False )"},{"location":"src/cloudfitters/HyperRectangleFitter/#apply95cutsself-point","text":"apply the cuts to a point and return whether it passes them input arguments: - point: a 1D numpy array of shape (ndims,) returns: - boolean","title":"&#10551; apply_cuts(self, point)"},{"location":"src/cloudfitters/HyperRectangleFitter/#pdfself-points","text":"get pdf at points note that the pdf is either 0 (does not pass cuts) or 1 (passes cuts)","title":"&#10551; pdf(self, points)"},{"location":"src/cloudfitters/LogNormalFitter/","text":"LogNormalFitter Class for fitting a log-normal distribution to a point cloud A log-normal distribution is constructed by fitting a normal distribution to the logarithm of the point coordinates. [class] LogNormalFitter(CloudFitter) class for fitting a log-normal distribution to a point cloud parameters: - mean: multidim mean of underlying normal - cov: multidim covariance matrix of underlying normal - mvn: scipy.stats multivariate_normal object built from the mean and cov \u2937 __init__(self,points) constructor input arguments: - points: a np array of shape (npoints,ndims) \u2937 pdf(self,points) get pdf at points","title":"LogNormalFitter"},{"location":"src/cloudfitters/LogNormalFitter/#lognormalfitter","text":"Class for fitting a log-normal distribution to a point cloud A log-normal distribution is constructed by fitting a normal distribution to the logarithm of the point coordinates.","title":"LogNormalFitter"},{"location":"src/cloudfitters/LogNormalFitter/#class-lognormalfittercloudfitter","text":"class for fitting a log-normal distribution to a point cloud parameters: - mean: multidim mean of underlying normal - cov: multidim covariance matrix of underlying normal - mvn: scipy.stats multivariate_normal object built from the mean and cov","title":"[class] LogNormalFitter(CloudFitter)"},{"location":"src/cloudfitters/LogNormalFitter/#9595init9595selfpoints","text":"constructor input arguments: - points: a np array of shape (npoints,ndims)","title":"&#10551; __init__(self,points)"},{"location":"src/cloudfitters/LogNormalFitter/#pdfselfpoints","text":"get pdf at points","title":"&#10551; pdf(self,points)"},{"location":"src/cloudfitters/SeminormalFitter/","text":"SeminormalFitter Class for fitting a 'seminormal' distribution to a point cloud This is not strictly speaking a probability distribution, only the first quadrant of the result of fitting a normal distribution to the data + its mirror image wrt the origin. [class] SeminormalFitter(CloudFitter) class for fitting a 'seminormal' distribution to a point cloud this is not strictly speaking a probability distribution, only the first quadrant of the result of fitting a normal distribution to the data + its mirror image wrt the origin. parameters - cov: multidim covariance matrix of normal distribution - mvn: scipy.stats multivariate_normal object built from the cov \u2937 __init__(self,points) constructor input arguments: - points: a np array of shape (npoints,ndims) note: points can also be an array or list with length 0, in that case the object is initialized empty. use this followed by the 'load' method to load a previously saved fit! \u2937 pdf(self,points) get pdf at points \u2937 save(self,path) save the covariance matrix as a .npy file specified by path \u2937 load(self,path) load a covariance matrix from a .npy file specified by path and build the fit from it","title":"SeminormalFitter"},{"location":"src/cloudfitters/SeminormalFitter/#seminormalfitter","text":"Class for fitting a 'seminormal' distribution to a point cloud This is not strictly speaking a probability distribution, only the first quadrant of the result of fitting a normal distribution to the data + its mirror image wrt the origin.","title":"SeminormalFitter"},{"location":"src/cloudfitters/SeminormalFitter/#class-seminormalfittercloudfitter","text":"class for fitting a 'seminormal' distribution to a point cloud this is not strictly speaking a probability distribution, only the first quadrant of the result of fitting a normal distribution to the data + its mirror image wrt the origin. parameters - cov: multidim covariance matrix of normal distribution - mvn: scipy.stats multivariate_normal object built from the cov","title":"[class] SeminormalFitter(CloudFitter)"},{"location":"src/cloudfitters/SeminormalFitter/#9595init9595selfpoints","text":"constructor input arguments: - points: a np array of shape (npoints,ndims) note: points can also be an array or list with length 0, in that case the object is initialized empty. use this followed by the 'load' method to load a previously saved fit!","title":"&#10551; __init__(self,points)"},{"location":"src/cloudfitters/SeminormalFitter/#pdfselfpoints","text":"get pdf at points","title":"&#10551; pdf(self,points)"},{"location":"src/cloudfitters/SeminormalFitter/#saveselfpath","text":"save the covariance matrix as a .npy file specified by path","title":"&#10551; save(self,path)"},{"location":"src/cloudfitters/SeminormalFitter/#loadselfpath","text":"load a covariance matrix from a .npy file specified by path and build the fit from it","title":"&#10551; load(self,path)"},{"location":"utils/","text":"Collection of utility functions","title":"README"},{"location":"utils/autoencoder_utils/","text":"autoencoder utils Utilities related to the training and evaluation of autoencoder models with keras The functionality in this script includes: - definition of loss functions (several flavours of MSE or chi-squared) - calculating and plotting ROC curves and confusion matrices - definition of very simple ready-to-use keras model architectures mseTop10(y_true, y_pred) MSE top 10 loss function for autoencoder training input arguments: - y_true and y_pred: two numpy arrays of equal shape, typically a histogram and its autoencoder reconstruction. if two-dimensional, the arrays are assumed to have shape (nhists,nbins)! output: - mean squared error between y_true and y_pred, where only the 10 bins with largest squared error are taken into account. if y_true and y_pred are 2D arrays, this function returns 1D array (mseTop10 for each histogram) mseTop10Raw(y_true, y_pred) same as mseTop10 but without using tf or K the version including tf or K seemed to cause randomly dying kernels, no clear reason could be found, but it was solved using this loss function instead. verified that it gives exactly the same output as the function above on some random arrays. contrary to mseTop10, this function only works for arrays with 2D shapes (so shape (nhists,nbins)), not for (nbins,). mseTopNRaw(y_true, y_pred, n=10) generalization of mseTop10Raw to any number of bins to take into account note: now generalized to also work for 2D histograms, i.e. arrays of shape (nhists,nybins,nxbins)! hence this is the most general method and preferred above mseTop10 and mseTop10Raw, which are only kept for reference input arguments: - y_true, y_pred: numpy arrays between which to calculate the mean square difference, of shape (nhists,nbins) or (nhists,nybins,nxbins) - n: number of largest elements to keep for averaging output: numpy array of shape (nhists) chiSquared(y_true, y_pred) chi2 loss function for autoencoder training input arguments: - y_true and y_pred: two numpy arrays of equal shape, typically a histogram and its autoencoder reconstruction. if two-dimensional, the arrays are assumed to have shape (nhists,nbins)! output: - relative mean squared error between y_true and y_pred, if y_true and y_pred are 2D arrays, this function returns 1D array (chiSquared for each histogram) chiSquaredTopNRaw(y_true, y_pred, n=10) generalization of chiSquared to any number of bins to take into account note: should work for 2D histograms as well (i.e. arrays of shape (nhistograms,nybins,nxbins)), but not yet tested! input arguments: - y_true, y_pred: numpy arrays between which to calculate the mean square difference, of shape (nhists,nbins) or (nhists,nybins,nxbins) - n: number of largest elements to keep for summing output: numpy array of shape (nhists) calculate_roc(scores, labels, scoreax) calculate a roc curve input arguments: - scores is a 1D numpy array containing output scores of any algorithm - labels is a 1D numpy array (equally long as scores) containing labels note that 1 for signal and 0 for background is assumed! this convention is only used to define what scores belong to signal or background; the scores itself can be anything (not limited to (0,1)), as long as the target for signal is higher than the target for background - scoreax is an array of score thresholds for which to compute the signal and background efficiency, assumed to be sorted in increasing order (i.e. from loose to tight) output: - tuple of two np arrays (signal efficiency and background efficiency) get_roc(scores, labels, mode='lin', npoints=100, doprint=False, doplot=True, plotmode='classic', doshow=True) make a ROC curve input arguments: - scores is a 1D numpy array containing output scores of any algorithm - labels is a 1D numpy array (equally long as scores) containing labels note that 1 for signal and 0 for background is assumed! this convention is only used to define what scores belong to signal or background; the scores itself can be anything (not limited to (0,1)), as long as the target for signal is higher than the target for background - mode: how to determine the points where to calculate signal and background efficiencies; options are: - 'lin': np.linspace between min and max score - 'geom': np. geomspace between min and max score - 'full': one point per score instance - npoints: number of points where to calculate the signal and background efficiencies (ignored if mode is 'full') - doprint: boolean whether to print score thresholds and corresponding signal and background efficiencies - doplot: boolean whether to make a plot or simply return the auc. - plotmode: how to plot the roc curve; options are: - 'classic' = signal efficiency afo background efficiency get_roc_from_hists(hists, labels, predicted_hists, mode='lin', npoints=100, doprint=False, doplot=True, plotmode='classic') make a ROC curve without manually calculating the scores the output score is the mseTop10Raw between the histograms and their reconstruction - input arguments: - hists and predicted_hists are 2D numpy arrays of shape (nhistograms,nbins) - other arguments: see get_roc get_confusion_matrix(scores, labels, wp='maxauc', plotwp=True) plot a confusion matrix input arguments: - scores and labels: defined in the same way as for get_roc - wp: the chosen working point (i.e. any score above wp is flagged as signal, any below is flagged as background) note: wp can be a integer or float, in which case that value will be used directly, or it can be a string in which case it will be used as the 'method' argument in get_wp! - plotwp: only relevant if wp is a string (see above), in which case plotwp will be used as the 'doplot' argument in get_wp get_confusion_matrix_from_hists(hists, labels, predicted_hists, msewp=None) plot a confusion matrix without manually calculating the scores the output score is the mse between the histograms and their reconstruction get_wp(scores, labels, method='maxauc', doplot=False) automatically calculate a suitable working point input arguments: - scores, labels: equally long 1d numpy arrays of predictions and true labels respectively note: in all methods, the labels are assumed to be 0 (for background) or 1 (for signal)! - method: method to calculate the working point currently supported: 'maxauc' - doplot: make a plot (if a plotting method exists for the chosen method) get_wp_maxauc(scores, labels, doplot=False) calculate the working point corresponding to maximum pseudo-AUC (i.e. maximize the rectangular area enclosed by the working point) getautoencoder(input_size,arch,act=[],opt='adam',loss=mseTop10) get a trainable autoencoder model input args: - input_size: size of vector that autoencoder will operate on - arch: list of number of nodes per hidden layer (excluding input and output layer) - act: list of activations per layer (default: tanh) - opt: optimizer to use (default: adam) - loss: loss function to use (defualt: mseTop10) train_simple_autoencoder(hists, nepochs=-1, modelname='', batch_size=500, shuffle=False, verbose=1, validation_split=0.1) create and train a very simple keras model the model consists of one hidden layer (with half as many units as there are input bins), tanh activation, adam optimizer and mseTop10 loss. input args: - hists is a 2D numpy array of shape (nhistograms, nbins) - nepochs is the number of epochs to use (has a default value if left unspecified) - modelname is a file name to save the model in (default: model is not saved to a file) clip_scores( scores ) clip +-inf values in scores +inf values in scores will be replaced by the maximum value (exclucing +inf) plus one -inf values in scores will be replaced by the minimim value (exclucing -inf) minus one input arguments: - scores: 1D numpy array returns - array with same length as scores with elements replaced as explained above","title":"autoencoder_utils"},{"location":"utils/autoencoder_utils/#autoencoder-utils","text":"Utilities related to the training and evaluation of autoencoder models with keras The functionality in this script includes: - definition of loss functions (several flavours of MSE or chi-squared) - calculating and plotting ROC curves and confusion matrices - definition of very simple ready-to-use keras model architectures","title":"autoencoder utils"},{"location":"utils/autoencoder_utils/#msetop10y95true-y95pred","text":"MSE top 10 loss function for autoencoder training input arguments: - y_true and y_pred: two numpy arrays of equal shape, typically a histogram and its autoencoder reconstruction. if two-dimensional, the arrays are assumed to have shape (nhists,nbins)! output: - mean squared error between y_true and y_pred, where only the 10 bins with largest squared error are taken into account. if y_true and y_pred are 2D arrays, this function returns 1D array (mseTop10 for each histogram)","title":"mseTop10(y_true, y_pred)"},{"location":"utils/autoencoder_utils/#msetop10rawy95true-y95pred","text":"same as mseTop10 but without using tf or K the version including tf or K seemed to cause randomly dying kernels, no clear reason could be found, but it was solved using this loss function instead. verified that it gives exactly the same output as the function above on some random arrays. contrary to mseTop10, this function only works for arrays with 2D shapes (so shape (nhists,nbins)), not for (nbins,).","title":"mseTop10Raw(y_true, y_pred)"},{"location":"utils/autoencoder_utils/#msetopnrawy95true-y95pred-n10","text":"generalization of mseTop10Raw to any number of bins to take into account note: now generalized to also work for 2D histograms, i.e. arrays of shape (nhists,nybins,nxbins)! hence this is the most general method and preferred above mseTop10 and mseTop10Raw, which are only kept for reference input arguments: - y_true, y_pred: numpy arrays between which to calculate the mean square difference, of shape (nhists,nbins) or (nhists,nybins,nxbins) - n: number of largest elements to keep for averaging output: numpy array of shape (nhists)","title":"mseTopNRaw(y_true, y_pred, n=10)"},{"location":"utils/autoencoder_utils/#chisquaredy95true-y95pred","text":"chi2 loss function for autoencoder training input arguments: - y_true and y_pred: two numpy arrays of equal shape, typically a histogram and its autoencoder reconstruction. if two-dimensional, the arrays are assumed to have shape (nhists,nbins)! output: - relative mean squared error between y_true and y_pred, if y_true and y_pred are 2D arrays, this function returns 1D array (chiSquared for each histogram)","title":"chiSquared(y_true, y_pred)"},{"location":"utils/autoencoder_utils/#chisquaredtopnrawy95true-y95pred-n10","text":"generalization of chiSquared to any number of bins to take into account note: should work for 2D histograms as well (i.e. arrays of shape (nhistograms,nybins,nxbins)), but not yet tested! input arguments: - y_true, y_pred: numpy arrays between which to calculate the mean square difference, of shape (nhists,nbins) or (nhists,nybins,nxbins) - n: number of largest elements to keep for summing output: numpy array of shape (nhists)","title":"chiSquaredTopNRaw(y_true, y_pred, n=10)"},{"location":"utils/autoencoder_utils/#calculate95rocscores-labels-scoreax","text":"calculate a roc curve input arguments: - scores is a 1D numpy array containing output scores of any algorithm - labels is a 1D numpy array (equally long as scores) containing labels note that 1 for signal and 0 for background is assumed! this convention is only used to define what scores belong to signal or background; the scores itself can be anything (not limited to (0,1)), as long as the target for signal is higher than the target for background - scoreax is an array of score thresholds for which to compute the signal and background efficiency, assumed to be sorted in increasing order (i.e. from loose to tight) output: - tuple of two np arrays (signal efficiency and background efficiency)","title":"calculate_roc(scores, labels, scoreax)"},{"location":"utils/autoencoder_utils/#get95rocscores-labels-modelin-npoints100-doprintfalse-doplottrue-plotmodeclassic-doshowtrue","text":"make a ROC curve input arguments: - scores is a 1D numpy array containing output scores of any algorithm - labels is a 1D numpy array (equally long as scores) containing labels note that 1 for signal and 0 for background is assumed! this convention is only used to define what scores belong to signal or background; the scores itself can be anything (not limited to (0,1)), as long as the target for signal is higher than the target for background - mode: how to determine the points where to calculate signal and background efficiencies; options are: - 'lin': np.linspace between min and max score - 'geom': np. geomspace between min and max score - 'full': one point per score instance - npoints: number of points where to calculate the signal and background efficiencies (ignored if mode is 'full') - doprint: boolean whether to print score thresholds and corresponding signal and background efficiencies - doplot: boolean whether to make a plot or simply return the auc. - plotmode: how to plot the roc curve; options are: - 'classic' = signal efficiency afo background efficiency","title":"get_roc(scores, labels, mode='lin', npoints=100, doprint=False, doplot=True, plotmode='classic', doshow=True)"},{"location":"utils/autoencoder_utils/#get95roc95from95histshists-labels-predicted95hists-modelin-npoints100-doprintfalse-doplottrue-plotmodeclassic","text":"make a ROC curve without manually calculating the scores the output score is the mseTop10Raw between the histograms and their reconstruction - input arguments: - hists and predicted_hists are 2D numpy arrays of shape (nhistograms,nbins) - other arguments: see get_roc","title":"get_roc_from_hists(hists, labels, predicted_hists, mode='lin', npoints=100, doprint=False, doplot=True, plotmode='classic')"},{"location":"utils/autoencoder_utils/#get95confusion95matrixscores-labels-wpmaxauc-plotwptrue","text":"plot a confusion matrix input arguments: - scores and labels: defined in the same way as for get_roc - wp: the chosen working point (i.e. any score above wp is flagged as signal, any below is flagged as background) note: wp can be a integer or float, in which case that value will be used directly, or it can be a string in which case it will be used as the 'method' argument in get_wp! - plotwp: only relevant if wp is a string (see above), in which case plotwp will be used as the 'doplot' argument in get_wp","title":"get_confusion_matrix(scores, labels, wp='maxauc', plotwp=True)"},{"location":"utils/autoencoder_utils/#get95confusion95matrix95from95histshists-labels-predicted95hists-msewpnone","text":"plot a confusion matrix without manually calculating the scores the output score is the mse between the histograms and their reconstruction","title":"get_confusion_matrix_from_hists(hists, labels, predicted_hists, msewp=None)"},{"location":"utils/autoencoder_utils/#get95wpscores-labels-methodmaxauc-doplotfalse","text":"automatically calculate a suitable working point input arguments: - scores, labels: equally long 1d numpy arrays of predictions and true labels respectively note: in all methods, the labels are assumed to be 0 (for background) or 1 (for signal)! - method: method to calculate the working point currently supported: 'maxauc' - doplot: make a plot (if a plotting method exists for the chosen method)","title":"get_wp(scores, labels, method='maxauc', doplot=False)"},{"location":"utils/autoencoder_utils/#get95wp95maxaucscores-labels-doplotfalse","text":"calculate the working point corresponding to maximum pseudo-AUC (i.e. maximize the rectangular area enclosed by the working point)","title":"get_wp_maxauc(scores, labels, doplot=False)"},{"location":"utils/autoencoder_utils/#getautoencoderinput95sizearchactoptadamlossmsetop10","text":"get a trainable autoencoder model input args: - input_size: size of vector that autoencoder will operate on - arch: list of number of nodes per hidden layer (excluding input and output layer) - act: list of activations per layer (default: tanh) - opt: optimizer to use (default: adam) - loss: loss function to use (defualt: mseTop10)","title":"getautoencoder(input_size,arch,act=[],opt='adam',loss=mseTop10)"},{"location":"utils/autoencoder_utils/#train95simple95autoencoderhists-nepochs-1-modelname-batch95size500-shufflefalse-verbose1-validation95split01","text":"create and train a very simple keras model the model consists of one hidden layer (with half as many units as there are input bins), tanh activation, adam optimizer and mseTop10 loss. input args: - hists is a 2D numpy array of shape (nhistograms, nbins) - nepochs is the number of epochs to use (has a default value if left unspecified) - modelname is a file name to save the model in (default: model is not saved to a file)","title":"train_simple_autoencoder(hists, nepochs=-1, modelname='',  batch_size=500, shuffle=False,  verbose=1, validation_split=0.1)"},{"location":"utils/autoencoder_utils/#clip95scores-scores","text":"clip +-inf values in scores +inf values in scores will be replaced by the maximum value (exclucing +inf) plus one -inf values in scores will be replaced by the minimim value (exclucing -inf) minus one input arguments: - scores: 1D numpy array returns - array with same length as scores with elements replaced as explained above","title":"clip_scores( scores )"},{"location":"utils/clustering_utils/","text":"clustering utils A collection of functions used for performing clustering tasks This collection of tools is a little deprecated at this moment but kept for reference; it contains functionality for pre-filtering the histograms in the training set based on their moments (e.g. mean, rms). Note that the functions here have not been used in a long time and might need some maintenance before they work properly again. vecdist(moments, index) calculate the vectorial distance between a set of moments input arguments: - moments: 2D numpy array of shape (ninstances,nmoments) - index: index for which instance to calculate the distance relative to the other instances returns: - a distance measure for the given index w.r.t. the other instances in 'moments' notes: - for this distance measure, the points are considered as vectors and the point at index is the origin. with respect to this origin, the average vector before index and the average vector after index are calculated. the distance is then defined as the norm of the difference of these vectors, normalized by the norms of the individual vectors. costhetadist(moments, index) calculate the costheta distance between a set of moments input arguments: - moments: 2D numpy array of shape (ninstances,nmoments) - index: index for which instance to calculate the distance relative to the other instances returns: - a distance measure for the given index w.r.t. the other instances in 'moments' notes: - this distance measure takes the cosine of the angle between the point at index and the one at index-1 (interpreted as vectors from the origin). avgnndist(moments, index, nn) calculate average euclidean distance to neighbouring points input arguments: - moments: 2D numpy array of shape (ninstances,nmoments) - index: index for which instance to calculate the distance relative to the other instances - nn: (half-) window size returns: - a distance measure for the given index w.r.t. the other instances in 'moments' notes: - for this distance measure, the average euclidean distance is calculated between the point at 'index' and the points at index-nn and index+nn (e.g. the nn previous and next lumisections). getavgnndist(hists, nmoments, xmin, xmax, nbins, nneighbours) apply avgnndist to a set of histograms filteranomalous(df, nmoments=3, rmouterflow=True, rmlargest=0., doplot=True) do a pre-filtering, removing the histograms with anomalous moments","title":"clustering_utils"},{"location":"utils/clustering_utils/#clustering-utils","text":"A collection of functions used for performing clustering tasks This collection of tools is a little deprecated at this moment but kept for reference; it contains functionality for pre-filtering the histograms in the training set based on their moments (e.g. mean, rms). Note that the functions here have not been used in a long time and might need some maintenance before they work properly again.","title":"clustering utils"},{"location":"utils/clustering_utils/#vecdistmoments-index","text":"calculate the vectorial distance between a set of moments input arguments: - moments: 2D numpy array of shape (ninstances,nmoments) - index: index for which instance to calculate the distance relative to the other instances returns: - a distance measure for the given index w.r.t. the other instances in 'moments' notes: - for this distance measure, the points are considered as vectors and the point at index is the origin. with respect to this origin, the average vector before index and the average vector after index are calculated. the distance is then defined as the norm of the difference of these vectors, normalized by the norms of the individual vectors.","title":"vecdist(moments, index)"},{"location":"utils/clustering_utils/#costhetadistmoments-index","text":"calculate the costheta distance between a set of moments input arguments: - moments: 2D numpy array of shape (ninstances,nmoments) - index: index for which instance to calculate the distance relative to the other instances returns: - a distance measure for the given index w.r.t. the other instances in 'moments' notes: - this distance measure takes the cosine of the angle between the point at index and the one at index-1 (interpreted as vectors from the origin).","title":"costhetadist(moments, index)"},{"location":"utils/clustering_utils/#avgnndistmoments-index-nn","text":"calculate average euclidean distance to neighbouring points input arguments: - moments: 2D numpy array of shape (ninstances,nmoments) - index: index for which instance to calculate the distance relative to the other instances - nn: (half-) window size returns: - a distance measure for the given index w.r.t. the other instances in 'moments' notes: - for this distance measure, the average euclidean distance is calculated between the point at 'index' and the points at index-nn and index+nn (e.g. the nn previous and next lumisections).","title":"avgnndist(moments, index, nn)"},{"location":"utils/clustering_utils/#getavgnndisthists-nmoments-xmin-xmax-nbins-nneighbours","text":"apply avgnndist to a set of histograms","title":"getavgnndist(hists, nmoments, xmin, xmax, nbins, nneighbours)"},{"location":"utils/clustering_utils/#filteranomalousdf-nmoments3-rmouterflowtrue-rmlargest0-doplottrue","text":"do a pre-filtering, removing the histograms with anomalous moments","title":"filteranomalous(df, nmoments=3, rmouterflow=True, rmlargest=0., doplot=True)"},{"location":"utils/csv_utils/","text":"csv utils A collection of useful basic functions for reading and processing the input csv files. Functionality includes: - reading the raw input csv files and producing more manageable csv files (grouped per histogram type). - reading csv files into pandas dataframes and writing pandas dataframes back to csv files. Note: the functionality of these utils has been absorbed into the DataLoader class, which is now the recommended way to read the data! get_data_dirs(year='2017', eras=[], dim=1) yield all data directories note that the location of the data is hard-coded; this function might break for newer or later reprocessings of the data. - year is a string, either '2017' or '2018' - era is a list containing a selection of era names (default empty list = all eras) - dim is either 1 or 2 (for 1D or 2D plots) get_csv_files(inputdir) yields paths to all csv files in input directory note that the output paths consist of input_dir/filename this function is only meant for 1-level down searching, i.e. the .csv files listed directly under input_dir. sort_filenames(filelist) sort filenames in numerical order (e.g. 2 before 10) note that the number is supposed to be in ..._<number>.<extension> format read_csv(csv_file) read csv file into pandas dataframe csv_file is the path to the csv file to be read write_csv(dataframe,csvfilename) write a dataframe to a csv file note: just a wrapper for builtin dataframe.to_csv read_and_merge_csv(csv_files, histnames=[], runnbs=[]) read and merge list of csv files into a single df csv_files is a list of paths to files to merge into a df histnames is a list of the types of histograms to keep (default: all) runnbs is a list of run numbers to keep (default: all) write_skimmed_csv(histnames, year, eras=['all'], dim=1) read all available data for a given year/era and make a file per histogram type input arguments: - histnames: list of histogram names for which to make a separate file - year: data-taking year (in string format) - eras: data-taking eras for which to make a separate file (in string format) use 'all' to make a file with all eras merged, i.e. a full data taking year - dim: dimension of histograms (1 or 2), needed to retrieve the correct folder containing input files output: - one csv file per year/era and per histogram type note: this function can take quite a while to run!","title":"csv_utils"},{"location":"utils/csv_utils/#csv-utils","text":"A collection of useful basic functions for reading and processing the input csv files. Functionality includes: - reading the raw input csv files and producing more manageable csv files (grouped per histogram type). - reading csv files into pandas dataframes and writing pandas dataframes back to csv files. Note: the functionality of these utils has been absorbed into the DataLoader class, which is now the recommended way to read the data!","title":"csv utils"},{"location":"utils/csv_utils/#get95data95dirsyear2017-eras-dim1","text":"yield all data directories note that the location of the data is hard-coded; this function might break for newer or later reprocessings of the data. - year is a string, either '2017' or '2018' - era is a list containing a selection of era names (default empty list = all eras) - dim is either 1 or 2 (for 1D or 2D plots)","title":"get_data_dirs(year='2017', eras=[], dim=1)"},{"location":"utils/csv_utils/#get95csv95filesinputdir","text":"yields paths to all csv files in input directory note that the output paths consist of input_dir/filename this function is only meant for 1-level down searching, i.e. the .csv files listed directly under input_dir.","title":"get_csv_files(inputdir)"},{"location":"utils/csv_utils/#sort95filenamesfilelist","text":"sort filenames in numerical order (e.g. 2 before 10) note that the number is supposed to be in ..._<number>.<extension> format","title":"sort_filenames(filelist)"},{"location":"utils/csv_utils/#read95csvcsv95file","text":"read csv file into pandas dataframe csv_file is the path to the csv file to be read","title":"read_csv(csv_file)"},{"location":"utils/csv_utils/#write95csvdataframecsvfilename","text":"write a dataframe to a csv file note: just a wrapper for builtin dataframe.to_csv","title":"write_csv(dataframe,csvfilename)"},{"location":"utils/csv_utils/#read95and95merge95csvcsv95files-histnames-runnbs","text":"read and merge list of csv files into a single df csv_files is a list of paths to files to merge into a df histnames is a list of the types of histograms to keep (default: all) runnbs is a list of run numbers to keep (default: all)","title":"read_and_merge_csv(csv_files, histnames=[], runnbs=[])"},{"location":"utils/csv_utils/#write95skimmed95csvhistnames-year-erasall-dim1","text":"read all available data for a given year/era and make a file per histogram type input arguments: - histnames: list of histogram names for which to make a separate file - year: data-taking year (in string format) - eras: data-taking eras for which to make a separate file (in string format) use 'all' to make a file with all eras merged, i.e. a full data taking year - dim: dimension of histograms (1 or 2), needed to retrieve the correct folder containing input files output: - one csv file per year/era and per histogram type note: this function can take quite a while to run!","title":"write_skimmed_csv(histnames, year, eras=['all'], dim=1)"},{"location":"utils/dataframe_utils/","text":"dataframe utils A collection of useful basic functions for manipulating pandas dataframes. Functionality includes (among others): - selecting DCS-bit on data or golden json data. - selecting specific runs, lumisections, or types of histograms get_histnames(df) get a list of (unique) histogram names present in a df df is a dataframe read from an input csv file. select_histnames(df, histnames) keep only a subset of histograms in a df histnames is a list of histogram names to keep in the df. get_runs(df) return a list of (unique) run numbers present in a df df is a dataframe read from an input csv file. select_runs(df, runnbs) keep only a subset of runs in a df runnbs is a list of run numbers to keep in the df. get_ls(df) return a list of ls numbers present in a df note that the numbers are not required to be unique! note: no check is done on the run number! select_ls(df, lsnbs) keep only a subset of lumisection numbers in a df lsnbs is a list of lumisection numbers to keep in the df. note: no check is done on the run number! get_runsls(df) return a dictionary with runs and lumisections in a dataframe (same format as e.g. golden json) select_json(df, jsonfile) keep only lumisections that are in the given json file select_runsls(df, jsondict) equivalent to select_json but using a pre-loaded json dict instead of a json file on disk select_golden(df) keep only golden lumisections in df select_notgolden(df) keep all but golden lumisections in df select_dcson(df) keep only lumisections in df that have DCS-bit on select_dcsoff(df) keep only lumisections in df that have DCS-bit off select_pixelgood(df) keep only lumisections in df that are in good pixel json select_pixelbad(df) keep only lumisections in df that are in bad pixel json get_highstat(df, entries_to_bins_ratio=100) return a select object of runs and ls of histograms with high statistics select_highstat(df, entries_to_bins_ratio=100) keep only lumisection in df with high statistics get_hist_values(df) same as builtin \"df['histo'].values\" but convert strings to np arrays input arguments: - df: a dataframe containing histograms (assumed to be of a single type!) note: this function works for both 1D and 2D histograms, the distinction is made based on whether or not 'Ybins' is present as a column in the dataframe update: 'Ybins' is also present for 1D histograms, but has value 1! output: a tuple containing the following elements: - np array of shape (nhists,nbins) (for 1D) or (nhists,nybins,nxbins) (for 2D) - np array of run numbers of length nhists - np array of lumisection numbers of length nhists warning: no check is done to assure that all histograms are of the same type!","title":"dataframe_utils"},{"location":"utils/dataframe_utils/#dataframe-utils","text":"A collection of useful basic functions for manipulating pandas dataframes. Functionality includes (among others): - selecting DCS-bit on data or golden json data. - selecting specific runs, lumisections, or types of histograms","title":"dataframe utils"},{"location":"utils/dataframe_utils/#get95histnamesdf","text":"get a list of (unique) histogram names present in a df df is a dataframe read from an input csv file.","title":"get_histnames(df)"},{"location":"utils/dataframe_utils/#select95histnamesdf-histnames","text":"keep only a subset of histograms in a df histnames is a list of histogram names to keep in the df.","title":"select_histnames(df, histnames)"},{"location":"utils/dataframe_utils/#get95runsdf","text":"return a list of (unique) run numbers present in a df df is a dataframe read from an input csv file.","title":"get_runs(df)"},{"location":"utils/dataframe_utils/#select95runsdf-runnbs","text":"keep only a subset of runs in a df runnbs is a list of run numbers to keep in the df.","title":"select_runs(df, runnbs)"},{"location":"utils/dataframe_utils/#get95lsdf","text":"return a list of ls numbers present in a df note that the numbers are not required to be unique! note: no check is done on the run number!","title":"get_ls(df)"},{"location":"utils/dataframe_utils/#select95lsdf-lsnbs","text":"keep only a subset of lumisection numbers in a df lsnbs is a list of lumisection numbers to keep in the df. note: no check is done on the run number!","title":"select_ls(df, lsnbs)"},{"location":"utils/dataframe_utils/#get95runslsdf","text":"return a dictionary with runs and lumisections in a dataframe (same format as e.g. golden json)","title":"get_runsls(df)"},{"location":"utils/dataframe_utils/#select95jsondf-jsonfile","text":"keep only lumisections that are in the given json file","title":"select_json(df, jsonfile)"},{"location":"utils/dataframe_utils/#select95runslsdf-jsondict","text":"equivalent to select_json but using a pre-loaded json dict instead of a json file on disk","title":"select_runsls(df, jsondict)"},{"location":"utils/dataframe_utils/#select95goldendf","text":"keep only golden lumisections in df","title":"select_golden(df)"},{"location":"utils/dataframe_utils/#select95notgoldendf","text":"keep all but golden lumisections in df","title":"select_notgolden(df)"},{"location":"utils/dataframe_utils/#select95dcsondf","text":"keep only lumisections in df that have DCS-bit on","title":"select_dcson(df)"},{"location":"utils/dataframe_utils/#select95dcsoffdf","text":"keep only lumisections in df that have DCS-bit off","title":"select_dcsoff(df)"},{"location":"utils/dataframe_utils/#select95pixelgooddf","text":"keep only lumisections in df that are in good pixel json","title":"select_pixelgood(df)"},{"location":"utils/dataframe_utils/#select95pixelbaddf","text":"keep only lumisections in df that are in bad pixel json","title":"select_pixelbad(df)"},{"location":"utils/dataframe_utils/#get95highstatdf-entries95to95bins95ratio100","text":"return a select object of runs and ls of histograms with high statistics","title":"get_highstat(df, entries_to_bins_ratio=100)"},{"location":"utils/dataframe_utils/#select95highstatdf-entries95to95bins95ratio100","text":"keep only lumisection in df with high statistics","title":"select_highstat(df, entries_to_bins_ratio=100)"},{"location":"utils/dataframe_utils/#get95hist95valuesdf","text":"same as builtin \"df['histo'].values\" but convert strings to np arrays input arguments: - df: a dataframe containing histograms (assumed to be of a single type!) note: this function works for both 1D and 2D histograms, the distinction is made based on whether or not 'Ybins' is present as a column in the dataframe update: 'Ybins' is also present for 1D histograms, but has value 1! output: a tuple containing the following elements: - np array of shape (nhists,nbins) (for 1D) or (nhists,nybins,nxbins) (for 2D) - np array of run numbers of length nhists - np array of lumisection numbers of length nhists warning: no check is done to assure that all histograms are of the same type!","title":"get_hist_values(df)"},{"location":"utils/generate_data_2d_utils/","text":"generate data 2d utils Extension of generate_data_utils.py towards 2D histograms goodnoise_nd(shape, fstd=None, kmaxscale=0.25, ncomponents=3) generate one sample of 'good' noise consisting of fourier components generalization of goodnoise (see generate_data_utils) to arbitrary number of dimensions input args: - shape: a tuple, shape of the noise array to be sampled note: in case of 1D, a comma is needed e.g. shape = (30,) else it will be automatically parsed to int and raise an error - fstd: an array of shape given by shape argument, used for scaling of the amplitude of the noise bin-by-bin (default: no scaling). - kmaxscale: scale factor to limit maximum frequency (lower kmaxscale means smoother noise) note: can be a tuple with same length as shape, to scale differently in different dimensions. - ncomponents: number of random sines to add per dimension note: can be a tuple with same length as shape, to use a different number of components in different dimensions. output: - numpy array of shape detailed by shape argument containing the noise whitenoise_nd(shape, fstd=None) generate one sample of white noise (standard normally distributed, uncorrelated between bins) generalization of whitenoise (see generate_data_utils) to arbitrary number of dimensions input args: - shape: a tuple, shape of the noise array to be sampled note: in case of 1D, a comma is needed e.g. shape = (30,) else it will be automatically parsed to int and raise an error - fstd: an array of shape given by shape argument, used for scaling of the amplitude of the noise bin-by-bin (default: no scaling). output: - numpy array of shape detailed by shape argument containing the noise random_lico_nd(hists) generate one linear combination of histograms with random coefficients in (0,1) summing to 1. generalization of random_lico (see generate_data_utils) to arbitrary number of dimensions. input args: - numpy array of shape (nhists,<arbitrary number of additional dimensions>) output: - numpy array of shape (<same dimensions as input>), containing the new histogram fourier_noise_nd(hists, outfilename='', figname='', nresamples=1, nonnegative=True, stdfactor=15., kmaxscale=0.25, ncomponents=3) apply fourier noise on random histograms with simple flat amplitude scaling. generalization of fourier_noise (see generate_data_utils) to arbitrary number of dimensions. input args: - hists: numpy array of shape (nhists,<arbitrary number of dimensions>) used for seeding - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw per input histogram - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: factor to scale magnitude of noise (larger factor = smaller noise) - kmaxscale and ncomponents: see goodnoise_nd white_noise_nd(hists, figname='', nresamples=1, nonnegative=True, stdfactor=15.) apply white noise to the histograms in hists. generalization of white_noise (see generate_data_utils) to arbitrary number of dimensions. input args: - hists: np array (nhists,<arbitrary number of dimensions>) containing input histograms - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw per input histogram - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: scaling factor of white noise amplitude (higher factor = smaller noise) resample_lico_nd(hists, nresamples=1, nonnegative=True) take random linear combinations of input histograms generalization of fourier_noise (see generate_data_utils) to arbitrary number of dimensions. input args: - hists: numpy array of shape (nhists,<arbitrary number of dimensions>) used for seeding - nresamples: number of samples to draw - nonnegative: boolean whether to set all bins to minimum zero after applying noise note: coefficients in linear combination are always nonnegative, so this setting is superfluous is input histograms are all nonnegative","title":"generate_data_2d_utils"},{"location":"utils/generate_data_2d_utils/#generate-data-2d-utils","text":"Extension of generate_data_utils.py towards 2D histograms","title":"generate data 2d utils"},{"location":"utils/generate_data_2d_utils/#goodnoise95ndshape-fstdnone-kmaxscale025-ncomponents3","text":"generate one sample of 'good' noise consisting of fourier components generalization of goodnoise (see generate_data_utils) to arbitrary number of dimensions input args: - shape: a tuple, shape of the noise array to be sampled note: in case of 1D, a comma is needed e.g. shape = (30,) else it will be automatically parsed to int and raise an error - fstd: an array of shape given by shape argument, used for scaling of the amplitude of the noise bin-by-bin (default: no scaling). - kmaxscale: scale factor to limit maximum frequency (lower kmaxscale means smoother noise) note: can be a tuple with same length as shape, to scale differently in different dimensions. - ncomponents: number of random sines to add per dimension note: can be a tuple with same length as shape, to use a different number of components in different dimensions. output: - numpy array of shape detailed by shape argument containing the noise","title":"goodnoise_nd(shape, fstd=None, kmaxscale=0.25, ncomponents=3)"},{"location":"utils/generate_data_2d_utils/#whitenoise95ndshape-fstdnone","text":"generate one sample of white noise (standard normally distributed, uncorrelated between bins) generalization of whitenoise (see generate_data_utils) to arbitrary number of dimensions input args: - shape: a tuple, shape of the noise array to be sampled note: in case of 1D, a comma is needed e.g. shape = (30,) else it will be automatically parsed to int and raise an error - fstd: an array of shape given by shape argument, used for scaling of the amplitude of the noise bin-by-bin (default: no scaling). output: - numpy array of shape detailed by shape argument containing the noise","title":"whitenoise_nd(shape, fstd=None)"},{"location":"utils/generate_data_2d_utils/#random95lico95ndhists","text":"generate one linear combination of histograms with random coefficients in (0,1) summing to 1. generalization of random_lico (see generate_data_utils) to arbitrary number of dimensions. input args: - numpy array of shape (nhists,<arbitrary number of additional dimensions>) output: - numpy array of shape (<same dimensions as input>), containing the new histogram","title":"random_lico_nd(hists)"},{"location":"utils/generate_data_2d_utils/#fourier95noise95ndhists-outfilename-figname-nresamples1-nonnegativetrue-stdfactor15-kmaxscale025-ncomponents3","text":"apply fourier noise on random histograms with simple flat amplitude scaling. generalization of fourier_noise (see generate_data_utils) to arbitrary number of dimensions. input args: - hists: numpy array of shape (nhists,<arbitrary number of dimensions>) used for seeding - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw per input histogram - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: factor to scale magnitude of noise (larger factor = smaller noise) - kmaxscale and ncomponents: see goodnoise_nd","title":"fourier_noise_nd(hists, outfilename='', figname='', nresamples=1, nonnegative=True,  stdfactor=15., kmaxscale=0.25, ncomponents=3)"},{"location":"utils/generate_data_2d_utils/#white95noise95ndhists-figname-nresamples1-nonnegativetrue-stdfactor15","text":"apply white noise to the histograms in hists. generalization of white_noise (see generate_data_utils) to arbitrary number of dimensions. input args: - hists: np array (nhists,<arbitrary number of dimensions>) containing input histograms - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw per input histogram - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: scaling factor of white noise amplitude (higher factor = smaller noise)","title":"white_noise_nd(hists, figname='', nresamples=1, nonnegative=True, stdfactor=15.)"},{"location":"utils/generate_data_2d_utils/#resample95lico95ndhists-nresamples1-nonnegativetrue","text":"take random linear combinations of input histograms generalization of fourier_noise (see generate_data_utils) to arbitrary number of dimensions. input args: - hists: numpy array of shape (nhists,<arbitrary number of dimensions>) used for seeding - nresamples: number of samples to draw - nonnegative: boolean whether to set all bins to minimum zero after applying noise note: coefficients in linear combination are always nonnegative, so this setting is superfluous is input histograms are all nonnegative","title":"resample_lico_nd(hists, nresamples=1, nonnegative=True)"},{"location":"utils/generate_data_utils/","text":"generate data utils A collection of functions for artificially creating a labeled dataset. See the function documentation below for more details on the implemented methods. Also check the tutorial generate_data.ipynb for examples! goodnoise(nbins, fstd=None) generate one sample of 'good' noise consisting of fourier components input args: - nbins: number of bins, length of noise array to be sampled - fstd: an array of length nbins used for scaling of the amplitude of the noise bin-by-bin. output: - numpy array of length nbins containing the noise badnoise(nbins, fstd=None) generate one sample of 'bad' noise consisting of fourier components (higher frequency and amplitude than 'good' noise) input args and output: simlar to goodnoise WARNING: NOT NECESSARILY REPRESENTATIVE OF ANOMALIES TO BE EXPECTED, DO NOT USE whitenoise(nbins, fstd=None) generate one sample of white noise (uncorrelated between bins) input args and output: similar to goodnoise random_lico(hists) generate one linear combination of histograms with random coefficients in (0,1) summing to 1 input args: - numpy array of shape (nhists,nbins), the rows of which will be linearly combined output: - numpy array of shape (nbins), containing the new histogram smoother(inarray, halfwidth=1) smooth the rows of a 2D array using the 2*halfwidth+1 surrounding values. mse_correlation_vector(hists, index) calculate mse of a histogram at given index wrt all other histograms input args: - hists: numpy array of shape (nhists,nbins) containing the histograms - index: the index (must be in (0,len(hists)-1)) of the histogram in question output: - numpy array of length nhists containing mse of the indexed histogram with respect to all other histograms WARNING: can be slow if called many times on a large collection of histograms with many bins. moments_correlation_vector(moments, index) calculate moment distance of hist at index wrt all other hists very similar to mse_correlation_vector but using histogram moments instead of full histograms for speed-up plot_data_and_gen(datahists, genhists, nplot=10, figname='fig.png') plot a couple of random examples from data and generated histograms input arguments: - datahists, genhists: numpy arrays of shape (nhists,nbins) - nplot: integer, maximum number of examples to plot - figname: name of figure to plot plot_seed_and_gen(seedhists, genhists, figname='fig.png') plot seed and generated histograms input arguments: - seedhists, genhists: numpy arrays of shape (nhists,nbins) - figname: name of figure to plot plot_noise(noise, histstd=None, figname='fig.png') plot histograms in noise (numpy array of shape (nhists,nbins)) optional argument histstd plots +- histstd as boundaries fourier_noise_on_mean(hists, outfilename='', figname='', nresamples=0, nonnegative=True) apply fourier noise on the bin-per-bin mean histogram, with amplitude scaling based on bin-per-bin std histogram. input args: - hists: numpy array of shape (nhists,nbins) used for determining mean and std - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw (default: number of input histograms / 10) - nonnegative: boolean whether to set all bins to minimum zero after applying noise MOSTLY SUITABLE AS HELP FUNCTION FOR RESAMPLE_SIMILAR_FOURIER_NOISE, NOT AS GENERATOR IN ITSELF advantages: mean histogram is almost certainly 'good' because of averaging, eliminate bad histograms disadvantages: deviations from mean are small, does not model systematic shifts by lumi. fourier_noise(hists, outfilename='', figname='', nresamples=1, nonnegative=True, stdfactor=15.) apply fourier noise on random histograms with simple flat amplitude scaling. input args: - hists: numpy array of shape (nhists,nbins) used for seeding - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw per input histogram - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: factor to scale magnitude of noise (larger factor = smaller noise) advantages: resampled histograms will have statistically same features as original input set disadvantages: also 'bad' histograms will be resampled if included in hists upsample_hist_set(hists, ntarget=-1, fourierstdfactor=15., figname='f') wrapper for fourier_noise allowing for a fixed target number of histograms instead of a fixed resampling factor useful function for quickly generating a fixed number of resampled histograms, without bothering too much about what exact resampling technique or detailed settings would be most appropriate. input arguments: hists: input histogram set ntarget: targetted number of resampled histograms (default: equally many as in hists) fourierstdfactor: see fourier_noise white_noise(hists, figname='', stdfactor=15.) apply white noise to the histograms in hists. input args: - hists: np array (nhists,nbins) containing input histograms - figname: path to figure plotting examples (default: no plotting) - stdfactor: scaling factor of white noise amplitude (higher factor = smaller noise) resample_bin_per_bin(hists, outfilename='', figname='', nresamples=0, nonnegative=True, smoothinghalfwidth=2) do resampling from bin-per-bin probability distributions input args: - hists: np array (nhists,nbins) containing the histograms to draw new samples from - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw (default: 1/10 of number of input histograms) - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - smoothinghalfwidth: halfwidth of smoothing procedure to apply on the result (default: no smoothing) advantages: no arbitrary noise modeling disadvantages: bins are considered independent, shape of historams not taken into account, does not work well on small number of input histograms, does not work well on histograms with systematic shifts resample_similar_bin_per_bin( allhists, selhists, outfilename='', figname='', nresamples=1, nonnegative=True, keeppercentage=1.) resample from bin-per-bin probability distributions, but only from similar looking histograms. input args: - allhists: np array (nhists,nbins) containing all available histograms (to determine mean) - selhists: np array (nhists,nbins) conataining selected histograms used as seeds (e.g. 'good' histograms) - outfilename: path of csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples per input histogram in selhists - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - keeppercentage: percentage (between 1 and 100) of histograms in allhists to use per input histogram advantages: no assumptions on shape of noise, can handle systematic shifts in histograms disadvantages: bins are treated independently from each other resample_similar_fourier_noise( allhists, selhists, outfilename='', figname='', nresamples=1, nonnegative=True, keeppercentage=1.) apply fourier noise on mean histogram, where the mean is determined from a set of similar-looking histograms input args: - allhists: np array (nhists,nbins) containing all available histograms (to determine mean) - selhists: np array (nhists,nbins) conataining selected histograms used as seeds (e.g. 'good' histograms) - outfilename: path of csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples per input histogram in selhists - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - keeppercentage: percentage (between 1 and 100) of histograms in allhists to use per input histogram advantages: most of fourier_noise_on_mean but can additionally handle shifting histograms, apart from fourier noise, also white noise can be applied. disadvantages: does not filter out odd histograms as long as enough other odd histograms look more or less similar resample_similar_lico( allhists, selhists, outfilename='', figname='', nresamples=1, nonnegative=True, keeppercentage=1.) take linear combinations of similar histograms input arguments: - allhists: 2D np array (nhists,nbins) with all available histograms, used to take linear combinations - selhists: 2D np array (nhists,nbins) with selected hists used for seeding (e.g. 'good' histograms) - outfilename: path to csv file to write result to (default: no writing) - figname: path to figure plotting examples (defautl: no plotting) - nresamples: number of combinations to make per input histogram - nonnegative: boolean whether to make all final histograms nonnegative - keeppercentage: percentage (between 0. and 100.) of histograms in allhists to use per input histogram advantages: no assumptions on noise disadvantages: sensitive to outlying histograms (more than with averaging) mc_sampling(hists, nMC=10000 , nresamples=10) resampling of a histogram using MC methods Drawing random points from a space defined by the range of the histogram in all axes. Points are \"accepted\" if the fall under the sampled histogram: f(x) - sampled distribution x_r, y_r -> randomly sampled point if y_r<=f(x_r), fill the new distribution at bin corresponding to x_r with weight: weight = (sum of input hist)/(#mc points accepted) this is equal to weight = (MC space volume)/(all MC points)","title":"generate_data_utils"},{"location":"utils/generate_data_utils/#generate-data-utils","text":"A collection of functions for artificially creating a labeled dataset. See the function documentation below for more details on the implemented methods. Also check the tutorial generate_data.ipynb for examples!","title":"generate data utils"},{"location":"utils/generate_data_utils/#goodnoisenbins-fstdnone","text":"generate one sample of 'good' noise consisting of fourier components input args: - nbins: number of bins, length of noise array to be sampled - fstd: an array of length nbins used for scaling of the amplitude of the noise bin-by-bin. output: - numpy array of length nbins containing the noise","title":"goodnoise(nbins, fstd=None)"},{"location":"utils/generate_data_utils/#badnoisenbins-fstdnone","text":"generate one sample of 'bad' noise consisting of fourier components (higher frequency and amplitude than 'good' noise) input args and output: simlar to goodnoise WARNING: NOT NECESSARILY REPRESENTATIVE OF ANOMALIES TO BE EXPECTED, DO NOT USE","title":"badnoise(nbins, fstd=None)"},{"location":"utils/generate_data_utils/#whitenoisenbins-fstdnone","text":"generate one sample of white noise (uncorrelated between bins) input args and output: similar to goodnoise","title":"whitenoise(nbins, fstd=None)"},{"location":"utils/generate_data_utils/#random95licohists","text":"generate one linear combination of histograms with random coefficients in (0,1) summing to 1 input args: - numpy array of shape (nhists,nbins), the rows of which will be linearly combined output: - numpy array of shape (nbins), containing the new histogram","title":"random_lico(hists)"},{"location":"utils/generate_data_utils/#smootherinarray-halfwidth1","text":"smooth the rows of a 2D array using the 2*halfwidth+1 surrounding values.","title":"smoother(inarray, halfwidth=1)"},{"location":"utils/generate_data_utils/#mse95correlation95vectorhists-index","text":"calculate mse of a histogram at given index wrt all other histograms input args: - hists: numpy array of shape (nhists,nbins) containing the histograms - index: the index (must be in (0,len(hists)-1)) of the histogram in question output: - numpy array of length nhists containing mse of the indexed histogram with respect to all other histograms WARNING: can be slow if called many times on a large collection of histograms with many bins.","title":"mse_correlation_vector(hists, index)"},{"location":"utils/generate_data_utils/#moments95correlation95vectormoments-index","text":"calculate moment distance of hist at index wrt all other hists very similar to mse_correlation_vector but using histogram moments instead of full histograms for speed-up","title":"moments_correlation_vector(moments, index)"},{"location":"utils/generate_data_utils/#plot95data95and95gendatahists-genhists-nplot10-fignamefigpng","text":"plot a couple of random examples from data and generated histograms input arguments: - datahists, genhists: numpy arrays of shape (nhists,nbins) - nplot: integer, maximum number of examples to plot - figname: name of figure to plot","title":"plot_data_and_gen(datahists, genhists, nplot=10, figname='fig.png')"},{"location":"utils/generate_data_utils/#plot95seed95and95genseedhists-genhists-fignamefigpng","text":"plot seed and generated histograms input arguments: - seedhists, genhists: numpy arrays of shape (nhists,nbins) - figname: name of figure to plot","title":"plot_seed_and_gen(seedhists, genhists, figname='fig.png')"},{"location":"utils/generate_data_utils/#plot95noisenoise-histstdnone-fignamefigpng","text":"plot histograms in noise (numpy array of shape (nhists,nbins)) optional argument histstd plots +- histstd as boundaries","title":"plot_noise(noise, histstd=None, figname='fig.png')"},{"location":"utils/generate_data_utils/#fourier95noise95on95meanhists-outfilename-figname-nresamples0-nonnegativetrue","text":"apply fourier noise on the bin-per-bin mean histogram, with amplitude scaling based on bin-per-bin std histogram. input args: - hists: numpy array of shape (nhists,nbins) used for determining mean and std - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw (default: number of input histograms / 10) - nonnegative: boolean whether to set all bins to minimum zero after applying noise MOSTLY SUITABLE AS HELP FUNCTION FOR RESAMPLE_SIMILAR_FOURIER_NOISE, NOT AS GENERATOR IN ITSELF advantages: mean histogram is almost certainly 'good' because of averaging, eliminate bad histograms disadvantages: deviations from mean are small, does not model systematic shifts by lumi.","title":"fourier_noise_on_mean(hists, outfilename='', figname='', nresamples=0, nonnegative=True)"},{"location":"utils/generate_data_utils/#fourier95noisehists-outfilename-figname-nresamples1-nonnegativetrue-stdfactor15","text":"apply fourier noise on random histograms with simple flat amplitude scaling. input args: - hists: numpy array of shape (nhists,nbins) used for seeding - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw per input histogram - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: factor to scale magnitude of noise (larger factor = smaller noise) advantages: resampled histograms will have statistically same features as original input set disadvantages: also 'bad' histograms will be resampled if included in hists","title":"fourier_noise(hists, outfilename='', figname='', nresamples=1, nonnegative=True, stdfactor=15.)"},{"location":"utils/generate_data_utils/#upsample95hist95sethists-ntarget-1-fourierstdfactor15-fignamef","text":"wrapper for fourier_noise allowing for a fixed target number of histograms instead of a fixed resampling factor useful function for quickly generating a fixed number of resampled histograms, without bothering too much about what exact resampling technique or detailed settings would be most appropriate. input arguments: hists: input histogram set ntarget: targetted number of resampled histograms (default: equally many as in hists) fourierstdfactor: see fourier_noise","title":"upsample_hist_set(hists, ntarget=-1, fourierstdfactor=15., figname='f')"},{"location":"utils/generate_data_utils/#white95noisehists-figname-stdfactor15","text":"apply white noise to the histograms in hists. input args: - hists: np array (nhists,nbins) containing input histograms - figname: path to figure plotting examples (default: no plotting) - stdfactor: scaling factor of white noise amplitude (higher factor = smaller noise)","title":"white_noise(hists, figname='', stdfactor=15.)"},{"location":"utils/generate_data_utils/#resample95bin95per95binhists-outfilename-figname-nresamples0-nonnegativetrue-smoothinghalfwidth2","text":"do resampling from bin-per-bin probability distributions input args: - hists: np array (nhists,nbins) containing the histograms to draw new samples from - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw (default: 1/10 of number of input histograms) - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - smoothinghalfwidth: halfwidth of smoothing procedure to apply on the result (default: no smoothing) advantages: no arbitrary noise modeling disadvantages: bins are considered independent, shape of historams not taken into account, does not work well on small number of input histograms, does not work well on histograms with systematic shifts","title":"resample_bin_per_bin(hists, outfilename='', figname='', nresamples=0, nonnegative=True, smoothinghalfwidth=2)"},{"location":"utils/generate_data_utils/#resample95similar95bin95per95bin-allhists-selhists-outfilename-figname-nresamples1-nonnegativetrue-keeppercentage1","text":"resample from bin-per-bin probability distributions, but only from similar looking histograms. input args: - allhists: np array (nhists,nbins) containing all available histograms (to determine mean) - selhists: np array (nhists,nbins) conataining selected histograms used as seeds (e.g. 'good' histograms) - outfilename: path of csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples per input histogram in selhists - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - keeppercentage: percentage (between 1 and 100) of histograms in allhists to use per input histogram advantages: no assumptions on shape of noise, can handle systematic shifts in histograms disadvantages: bins are treated independently from each other","title":"resample_similar_bin_per_bin( allhists, selhists, outfilename='', figname='', nresamples=1, nonnegative=True, keeppercentage=1.)"},{"location":"utils/generate_data_utils/#resample95similar95fourier95noise-allhists-selhists-outfilename-figname-nresamples1-nonnegativetrue-keeppercentage1","text":"apply fourier noise on mean histogram, where the mean is determined from a set of similar-looking histograms input args: - allhists: np array (nhists,nbins) containing all available histograms (to determine mean) - selhists: np array (nhists,nbins) conataining selected histograms used as seeds (e.g. 'good' histograms) - outfilename: path of csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples per input histogram in selhists - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - keeppercentage: percentage (between 1 and 100) of histograms in allhists to use per input histogram advantages: most of fourier_noise_on_mean but can additionally handle shifting histograms, apart from fourier noise, also white noise can be applied. disadvantages: does not filter out odd histograms as long as enough other odd histograms look more or less similar","title":"resample_similar_fourier_noise( allhists, selhists, outfilename='', figname='', nresamples=1, nonnegative=True, keeppercentage=1.)"},{"location":"utils/generate_data_utils/#resample95similar95lico-allhists-selhists-outfilename-figname-nresamples1-nonnegativetrue-keeppercentage1","text":"take linear combinations of similar histograms input arguments: - allhists: 2D np array (nhists,nbins) with all available histograms, used to take linear combinations - selhists: 2D np array (nhists,nbins) with selected hists used for seeding (e.g. 'good' histograms) - outfilename: path to csv file to write result to (default: no writing) - figname: path to figure plotting examples (defautl: no plotting) - nresamples: number of combinations to make per input histogram - nonnegative: boolean whether to make all final histograms nonnegative - keeppercentage: percentage (between 0. and 100.) of histograms in allhists to use per input histogram advantages: no assumptions on noise disadvantages: sensitive to outlying histograms (more than with averaging)","title":"resample_similar_lico( allhists, selhists, outfilename='', figname='', nresamples=1, nonnegative=True, keeppercentage=1.)"},{"location":"utils/generate_data_utils/#mc95samplinghists-nmc10000-nresamples10","text":"resampling of a histogram using MC methods Drawing random points from a space defined by the range of the histogram in all axes. Points are \"accepted\" if the fall under the sampled histogram: f(x) - sampled distribution x_r, y_r -> randomly sampled point if y_r<=f(x_r), fill the new distribution at bin corresponding to x_r with weight: weight = (sum of input hist)/(#mc points accepted) this is equal to weight = (MC space volume)/(all MC points)","title":"mc_sampling(hists, nMC=10000 , nresamples=10)"},{"location":"utils/hist_utils/","text":"hist utils A collection of useful basic functions for processing histograms. Functionality includes: - rebinning and normalization - moment calculation - averaging - higher-level functions preparing data for ML training, starting from a dataframe or input csv file. crophists(hists, slices=None) perform cropping on a set of histograms input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - slices: a slice object (builtin python type) or a list of two slices (for 2D) notes: - a slice can be created using the builtin python syntax 'slice(start,stop,step)', and the syntax 'list[slice]' is equivalent to 'list[start:stop:step]'. use 'None' to ignore one of the arguments for slice creation (equivalent to ':' in direct slicing) - for 1D histograms, slices can be either a slice object or a list of length 1 containing a single slice. example usage: - see tutorials/plot_histograms_2d.ipynb returns: - a numpy array containing the same histograms as input but cropped according to the slices argument rebinhists(hists, factor=None) perform rebinning on a set of histograms input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - factor: the rebinning factor (for 1D), or a tuple of (y axis rebinning factor, x axis rebinning factor) (for 2D) note: the rebinning applied here is simple summing of bin contents, and the rebinning factors must be divisors of the respective number of bins! example usage: - see tutorials/plot_histograms_2d.ipynb returns: - a numpy array containing the same histograms as input but rebinned according to the factor argument normalizehists(hists) perform normalization on a set of histograms note: - for 1D histograms, the sum of bin contents is set equal one for each histogram - for 2D histograms, the bin contents are scaled so the maximum is 1 for each histogram - maybe later make more flexible by adding normalization stragy as argument... input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D returns: - a numpy array containing the same histograms as input but normalized averagehists(hists, nout=None) partition a set of histograms into equal parts and take the average histogram of each part input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - nout: number of partitions, i.e. number of output histograms note: nout=1 corresponds to simply taking the average of all histograms in hists. note: if nout is negative or if nout is larger than number of input histograms, the original set of histograms is returned. returns: - a numpy array of shape (nout,nbins) running_average_hists(hists, window=None, weights=None) replace each histogram in a collection of histograms by its running average input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - window: number of histograms to consider for the averaging if window is an integer, it is the number of previous histograms in hists used for averaging (so window=0 would correspond to no averaging) if window is a tuple, it corresponds to (nprevious,nnext), and the nprevious previous and nnext next histograms in hists are used for averaging (so window=(0,0) would correspond to no averaging) - weights: a list or numpy array containing the relative weights of the histograms in the averaging procedure. note: the weights can be any number, but they will be normalized to have unit sum. note: weights must have length nwindow+1 or nprevious+1+nnext. note: the default behaviour is a uniform array with values 1./(window+1) (or 1./(nprevious+1+nnext)) returns: - a numpy array with same shape as input but where each histogram is replaced by its running average notes: - at the edges, the weights are cropped to match the input array and renormalized - this function will crash when the length of the set of histograms is smaller than the total window length, maybe extend later (although this is not normally needed) moment(bins, counts, order) get n-th central moment of a histogram input arguments: - bins: a 1D or 2D np array holding the bin centers (shape (nbins) or (nhistograms,nbins)) - counts: a 2D np array containing the bin counts (shape (nhistograms,nbins)) - order: the order of the moment to calculate (0 = maximum value, 1 = mean value) returns: - an array of shape (nhistograms) holding the requested moment per histogram notes: - for now only 1D histograms are supported! histmean(bins, counts) special case of moment calculation (with order=1) histrms(bins, counts) special case of moment calculation histmoments(bins, counts, orders) apply moment calculation for a list of orders input arguments: - see function moment(bins, counts, order), the only difference being that orders is a list instead of a single number returns: - a numpy array of shape (nhistograms,nmoments) preparedatafromnpy(dataname, cropslices=None, rebinningfactor=None, donormalize=True, doplot=False) read a .npy file and output the histograms input arguments: - see e.g. preparedatafromdf notes: - not yet tested for 2D histograms, but is expected to work... preparedatafromdf(df, returnrunls=False, cropslices=None, rebinningfactor=None, donormalize=False, doplot=False) prepare the data contained in a dataframe in the form of a numpy array input arguments: - returnrunls: boolean whether to return a tuple of (histograms, run numbers, lumisection numbers). (default: return only histograms) - cropslices: list of slices (one per dimension) by which to crop the historams (default: no cropping) - rebinningfactor: an integer (or tuple of integers for 2D histograms) to downsample/rebin the histograms (default: no rebinning) - donormalize: boolean whether to normalize the data - doplot: if True, some example plots are made showing the histograms preparedatafromcsv(dataname, returnrunls=False, cropslices=None, rebinningfactor=None, donormalize=True, doplot=False) prepare the data contained in a dataframe csv file in the form of a numpy array input arguments: - returnrunls: boolean whether to return a tuple of (histograms, run numbers, lumisection numbers). (default: return only histograms) - cropslices: list of slices (one per dimension) by which to crop the historams (default: no cropping) - rebinningfactor: an integer (or tuple of integers for 2D histograms) to downsample/rebin the histograms (default: no rebinning) - donormalize: boolean whether to normalize the data - doplot: if True, some example plots are made showing the histograms","title":"hist_utils"},{"location":"utils/hist_utils/#hist-utils","text":"A collection of useful basic functions for processing histograms. Functionality includes: - rebinning and normalization - moment calculation - averaging - higher-level functions preparing data for ML training, starting from a dataframe or input csv file.","title":"hist utils"},{"location":"utils/hist_utils/#crophistshists-slicesnone","text":"perform cropping on a set of histograms input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - slices: a slice object (builtin python type) or a list of two slices (for 2D) notes: - a slice can be created using the builtin python syntax 'slice(start,stop,step)', and the syntax 'list[slice]' is equivalent to 'list[start:stop:step]'. use 'None' to ignore one of the arguments for slice creation (equivalent to ':' in direct slicing) - for 1D histograms, slices can be either a slice object or a list of length 1 containing a single slice. example usage: - see tutorials/plot_histograms_2d.ipynb returns: - a numpy array containing the same histograms as input but cropped according to the slices argument","title":"crophists(hists, slices=None)"},{"location":"utils/hist_utils/#rebinhistshists-factornone","text":"perform rebinning on a set of histograms input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - factor: the rebinning factor (for 1D), or a tuple of (y axis rebinning factor, x axis rebinning factor) (for 2D) note: the rebinning applied here is simple summing of bin contents, and the rebinning factors must be divisors of the respective number of bins! example usage: - see tutorials/plot_histograms_2d.ipynb returns: - a numpy array containing the same histograms as input but rebinned according to the factor argument","title":"rebinhists(hists, factor=None)"},{"location":"utils/hist_utils/#normalizehistshists","text":"perform normalization on a set of histograms note: - for 1D histograms, the sum of bin contents is set equal one for each histogram - for 2D histograms, the bin contents are scaled so the maximum is 1 for each histogram - maybe later make more flexible by adding normalization stragy as argument... input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D returns: - a numpy array containing the same histograms as input but normalized","title":"normalizehists(hists)"},{"location":"utils/hist_utils/#averagehistshists-noutnone","text":"partition a set of histograms into equal parts and take the average histogram of each part input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - nout: number of partitions, i.e. number of output histograms note: nout=1 corresponds to simply taking the average of all histograms in hists. note: if nout is negative or if nout is larger than number of input histograms, the original set of histograms is returned. returns: - a numpy array of shape (nout,nbins)","title":"averagehists(hists, nout=None)"},{"location":"utils/hist_utils/#running95average95histshists-windownone-weightsnone","text":"replace each histogram in a collection of histograms by its running average input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - window: number of histograms to consider for the averaging if window is an integer, it is the number of previous histograms in hists used for averaging (so window=0 would correspond to no averaging) if window is a tuple, it corresponds to (nprevious,nnext), and the nprevious previous and nnext next histograms in hists are used for averaging (so window=(0,0) would correspond to no averaging) - weights: a list or numpy array containing the relative weights of the histograms in the averaging procedure. note: the weights can be any number, but they will be normalized to have unit sum. note: weights must have length nwindow+1 or nprevious+1+nnext. note: the default behaviour is a uniform array with values 1./(window+1) (or 1./(nprevious+1+nnext)) returns: - a numpy array with same shape as input but where each histogram is replaced by its running average notes: - at the edges, the weights are cropped to match the input array and renormalized - this function will crash when the length of the set of histograms is smaller than the total window length, maybe extend later (although this is not normally needed)","title":"running_average_hists(hists, window=None, weights=None)"},{"location":"utils/hist_utils/#momentbins-counts-order","text":"get n-th central moment of a histogram input arguments: - bins: a 1D or 2D np array holding the bin centers (shape (nbins) or (nhistograms,nbins)) - counts: a 2D np array containing the bin counts (shape (nhistograms,nbins)) - order: the order of the moment to calculate (0 = maximum value, 1 = mean value) returns: - an array of shape (nhistograms) holding the requested moment per histogram notes: - for now only 1D histograms are supported!","title":"moment(bins, counts, order)"},{"location":"utils/hist_utils/#histmeanbins-counts","text":"special case of moment calculation (with order=1)","title":"histmean(bins, counts)"},{"location":"utils/hist_utils/#histrmsbins-counts","text":"special case of moment calculation","title":"histrms(bins, counts)"},{"location":"utils/hist_utils/#histmomentsbins-counts-orders","text":"apply moment calculation for a list of orders input arguments: - see function moment(bins, counts, order), the only difference being that orders is a list instead of a single number returns: - a numpy array of shape (nhistograms,nmoments)","title":"histmoments(bins, counts, orders)"},{"location":"utils/hist_utils/#preparedatafromnpydataname-cropslicesnone-rebinningfactornone-donormalizetrue-doplotfalse","text":"read a .npy file and output the histograms input arguments: - see e.g. preparedatafromdf notes: - not yet tested for 2D histograms, but is expected to work...","title":"preparedatafromnpy(dataname, cropslices=None, rebinningfactor=None, donormalize=True, doplot=False)"},{"location":"utils/hist_utils/#preparedatafromdfdf-returnrunlsfalse-cropslicesnone-rebinningfactornone-donormalizefalse-doplotfalse","text":"prepare the data contained in a dataframe in the form of a numpy array input arguments: - returnrunls: boolean whether to return a tuple of (histograms, run numbers, lumisection numbers). (default: return only histograms) - cropslices: list of slices (one per dimension) by which to crop the historams (default: no cropping) - rebinningfactor: an integer (or tuple of integers for 2D histograms) to downsample/rebin the histograms (default: no rebinning) - donormalize: boolean whether to normalize the data - doplot: if True, some example plots are made showing the histograms","title":"preparedatafromdf(df, returnrunls=False, cropslices=None, rebinningfactor=None, donormalize=False, doplot=False)"},{"location":"utils/hist_utils/#preparedatafromcsvdataname-returnrunlsfalse-cropslicesnone-rebinningfactornone-donormalizetrue-doplotfalse","text":"prepare the data contained in a dataframe csv file in the form of a numpy array input arguments: - returnrunls: boolean whether to return a tuple of (histograms, run numbers, lumisection numbers). (default: return only histograms) - cropslices: list of slices (one per dimension) by which to crop the historams (default: no cropping) - rebinningfactor: an integer (or tuple of integers for 2D histograms) to downsample/rebin the histograms (default: no rebinning) - donormalize: boolean whether to normalize the data - doplot: if True, some example plots are made showing the histograms","title":"preparedatafromcsv(dataname, returnrunls=False, cropslices=None, rebinningfactor=None, donormalize=True, doplot=False)"},{"location":"utils/json_utils/","text":"json utils A collection of useful basic functions for manipulating json files. Functionality includes: - reading and writing json files for given sets of run numbers and lumisection numbers - checking if a given run number, lumisection number or combination is present in a given json file Note that the json files are always assumed to contain the following structure: - dict - run number (in string format) - list - list of two elements - starting lumisection number, ending lumisection number Example: { \"294927\": [ [ 55,85 ], [ 95,105] ] } There is one exception to this rule: instead of [ start, stop ], the lumisection list can also be [ -1 ], which is short for all lumisections within that run. loadjson( jsonfile ) load the content of a json file into a python object input arguments: - jsonfile: the name (or full path if needed) to the json file to be read output: - an dict object as specified in the note below note: the json file is supposed to contain an object like this example: { \"294927\": [ [ 55,85 ], [ 95,105] ], \"294928\": [ [1,33 ] ] } although no explicit checking is done in this function, objects that don't have this structure will probably lead to errors further in the code writejson( jsondict, outputfile, overwrite=False ) inverse function of loadjson input arguments - jsondict: dict object to be written to a json file - outputfile: output file to be written, extension '.json' will be appended automatically - overwrite: boolean whether to overwrite outputfile if it exists (default: throw exception) injson_single( run, lumi, jsondict ) helper function for injson, only for internal use input arguments: - run and lumi are integers - jsondict is an object loaded from a json file output: - boolean whether the run/lumi combination is in the json dict injson( run, lumi, jsonfile=None, jsondict=None ) find if a run and lumi combination is in a given json file input arguments: - run and lumi: integers or (equally long) arrays of integers - jsonfile: a path to a json file - jsondict: a dict loaded from a json file note: either jsonfile or jsondict must not be None! output: boolean or array of booleans (depending on run and lumi) getjsondir() internal helper function returning the path to where json files are stored isgolden(run, lumi) find if a run and lumi combination is in the golden json file input arguments: - run and lumi: either integers or (equally long) arrays of integers isdcson(run, lumi) find if a run and lumi combination is in DCS-only json file input arguments: - run and lumi: either integers or (equally long) arrays of integers plainlist_to_rangelist( plainlist ) helper function for tuplelist_to_jsondict, only for internal use input arguments: - plainlist: a list of integers in increasing order, must have length >= 2 output: - a list lists representing ranges example: [1,2,3,5,6] -> [ [1,3], [5,6] ] rangelist_to_plainlist( rangelist ) inverse function of plainlist_to_rangelist, for internal use only tuplelist_to_jsondict( tuplelist ) convert a list of tuples of format (run number, [lumisection numbers]) to json dict jsondict_to_tuplelist( jsondict ) inverse function of tuplelist_to_jsondict get_lcs( jsonlist ) return a jsondict object that is the largest common subset (LCS) between the jsondict objects in jsonlist input arguments: - jsonlist: a list of dicts in the conventional json format, so each element in jsonlist must be e.g. { \"294927\": [ [ 55,85 ], [ 95,105] ], \"294928\": [ [1,33 ] ] } remark: this is probably not the most efficient implementation, open for improvement...","title":"json_utils"},{"location":"utils/json_utils/#json-utils","text":"A collection of useful basic functions for manipulating json files. Functionality includes: - reading and writing json files for given sets of run numbers and lumisection numbers - checking if a given run number, lumisection number or combination is present in a given json file Note that the json files are always assumed to contain the following structure: - dict - run number (in string format) - list - list of two elements - starting lumisection number, ending lumisection number Example: { \"294927\": [ [ 55,85 ], [ 95,105] ] } There is one exception to this rule: instead of [ start, stop ], the lumisection list can also be [ -1 ], which is short for all lumisections within that run.","title":"json utils"},{"location":"utils/json_utils/#loadjson-jsonfile","text":"load the content of a json file into a python object input arguments: - jsonfile: the name (or full path if needed) to the json file to be read output: - an dict object as specified in the note below note: the json file is supposed to contain an object like this example: { \"294927\": [ [ 55,85 ], [ 95,105] ], \"294928\": [ [1,33 ] ] } although no explicit checking is done in this function, objects that don't have this structure will probably lead to errors further in the code","title":"loadjson( jsonfile )"},{"location":"utils/json_utils/#writejson-jsondict-outputfile-overwritefalse","text":"inverse function of loadjson input arguments - jsondict: dict object to be written to a json file - outputfile: output file to be written, extension '.json' will be appended automatically - overwrite: boolean whether to overwrite outputfile if it exists (default: throw exception)","title":"writejson( jsondict, outputfile, overwrite=False )"},{"location":"utils/json_utils/#injson95single-run-lumi-jsondict","text":"helper function for injson, only for internal use input arguments: - run and lumi are integers - jsondict is an object loaded from a json file output: - boolean whether the run/lumi combination is in the json dict","title":"injson_single( run, lumi, jsondict )"},{"location":"utils/json_utils/#injson-run-lumi-jsonfilenone-jsondictnone","text":"find if a run and lumi combination is in a given json file input arguments: - run and lumi: integers or (equally long) arrays of integers - jsonfile: a path to a json file - jsondict: a dict loaded from a json file note: either jsonfile or jsondict must not be None! output: boolean or array of booleans (depending on run and lumi)","title":"injson( run, lumi, jsonfile=None, jsondict=None )"},{"location":"utils/json_utils/#getjsondir","text":"internal helper function returning the path to where json files are stored","title":"getjsondir()"},{"location":"utils/json_utils/#isgoldenrun-lumi","text":"find if a run and lumi combination is in the golden json file input arguments: - run and lumi: either integers or (equally long) arrays of integers","title":"isgolden(run, lumi)"},{"location":"utils/json_utils/#isdcsonrun-lumi","text":"find if a run and lumi combination is in DCS-only json file input arguments: - run and lumi: either integers or (equally long) arrays of integers","title":"isdcson(run, lumi)"},{"location":"utils/json_utils/#plainlist95to95rangelist-plainlist","text":"helper function for tuplelist_to_jsondict, only for internal use input arguments: - plainlist: a list of integers in increasing order, must have length >= 2 output: - a list lists representing ranges example: [1,2,3,5,6] -> [ [1,3], [5,6] ]","title":"plainlist_to_rangelist( plainlist )"},{"location":"utils/json_utils/#rangelist95to95plainlist-rangelist","text":"inverse function of plainlist_to_rangelist, for internal use only","title":"rangelist_to_plainlist( rangelist )"},{"location":"utils/json_utils/#tuplelist95to95jsondict-tuplelist","text":"convert a list of tuples of format (run number, [lumisection numbers]) to json dict","title":"tuplelist_to_jsondict( tuplelist )"},{"location":"utils/json_utils/#jsondict95to95tuplelist-jsondict","text":"inverse function of tuplelist_to_jsondict","title":"jsondict_to_tuplelist( jsondict )"},{"location":"utils/json_utils/#get95lcs-jsonlist","text":"return a jsondict object that is the largest common subset (LCS) between the jsondict objects in jsonlist input arguments: - jsonlist: a list of dicts in the conventional json format, so each element in jsonlist must be e.g. { \"294927\": [ [ 55,85 ], [ 95,105] ], \"294928\": [ [1,33 ] ] } remark: this is probably not the most efficient implementation, open for improvement...","title":"get_lcs( jsonlist )"},{"location":"utils/mask_utils/","text":"mask utils Utilities for working with HistStruct masks Mostly meant for internal use. get_combined_name( masklist ) concatenate all the masknames in masklist to a combined name input arguments: - masklist: list of strings output: string with contatenated name","title":"mask_utils"},{"location":"utils/mask_utils/#mask-utils","text":"Utilities for working with HistStruct masks Mostly meant for internal use.","title":"mask utils"},{"location":"utils/mask_utils/#get95combined95name-masklist","text":"concatenate all the masknames in masklist to a combined name input arguments: - masklist: list of strings output: string with contatenated name","title":"get_combined_name( masklist )"},{"location":"utils/plot_utils/","text":"plot utils A collection of useful basic functions for plotting. make_legend_opaque( leg ) set the transparency of all entries in a legend to zero add_text( ax, text, pos, fontsize=10, background_facecolor=None, background_alpha=None, background_edgecolor=None ) add text to an axis at a specified position (in relative figure coordinates) input arguments: - ax: matplotlib axis object - text: string, can contain latex syntax such as /textbf{} and /textit{} - pos: tuple with relative x- and y-axis coordinates of bottom left corner add_cms_label( ax, pos=(0.1,0.9), extratext=None, **kwargs ) add the CMS label and extra text (e.g. 'Preliminary') to a plot special case of add_text, for convenience make_text_latex_safe( text ) make a string safe to process with matplotlib's latex parser in case no tex parsing is wanted (e.g. escape underscores) to be extended when the need arises! plot_hists(histlist, fig=None, ax=None, colorlist=[], labellist=[], transparency=1, xlims=(-0.5,-1), title=None, xaxtitle=None, yaxtitle=None, bkgcolor=None, bkgcmap='spring', bkgrange=None, bkgtitle=None) plot some histograms (in histlist) in one figure using specified colors and/or labels - histlist is a list of 1D arrays containing the histograms (or a 2D array of shape (nhistograms,nbins)) - colorlist is a list or array containing colors (in string format), of length nhistograms note: it can also be a single string representing a color (in pyplot), then all histograms will take this color - labellist is a list or array containing labels for in legend, of length nhistograms - xlims is a tuple of min and max for the x-axis labels, defaults to (-0.5,nbins-0.5) - title, xaxtitle, yaxtitle: strings for histogram title, x-axis title and y-axis title respectively - bkgcolor: 1D array representing background color for the plot (color axis will be scaled between min and max in bkgcolor) note: if bkgcolor does not have the same length as the x-axis, it will be compressed or stretched to fit the axis, but this might be meaningless, depending on what you are trying to visualize! - bkgmap: name of valid pyplot color map for plotting the background color output: tuple of figure and axis objects, that can be used to further tune the look of the figure or save it plot_hists_multi(histlist, fig=None, ax=None, colorlist=[], labellist=[], transparency=1, xlims=(-0.5,-1), title=None, xaxtitle=None, yaxtitle=None) plot many histograms (in histlist) in one figure using specified colors and/or labels - histlist is a list of 1D arrays containing the histograms (or a 2D array of shape (nhistograms,nbins)) - colorlist is a list or array containing numbers to be mapped to colors - labellist is a list or array containing labels for in legend output: tuple of figure and axis objects, that can be used to further tune the look of the figure or save it plot_hist_2d(hist, fig=None, ax=None, title=None, xaxtitle=None, yaxtitle=None, caxrange=None) plot a 2D histogram - hist is a 2D numpy array of shape (nxbins, nybins) notes: - if the histogram contains only nonnegative values, values below 1e-12 will not be plotted (i.e. they will be shown as white spots in the plot) to discriminate zero from small but nonzero - if the histogram contains negative values, the color axis will be symmetrized around zero plot_hists_2d(hists, ncols=4, title=None, subtitles=None, xaxtitle=None, yaxtitle=None, caxrange=None) plot multiple 2D histograms next to each other - hists: list of 2D numpy arrays of shape (nxbins,nybins), or an equivalent 3D numpy array - ncols: number of columns to use plot_hists_2d_gif(hists, titles=None, xaxtitle=None, yaxtitle=None, duration=0.3, figname='temp_gif.gif') (no valid documentation found) plot_hists_from_df(df, histtype, nhists) plot a number of histograms in a dataframe - df is the dataframe from which to plot - histtype is the name of the histogram type (e.g. 'chargeInner_PXLayer_1') - nhists is the number of histograms to plot plot_sets(setlist, fig=None, ax=None, colorlist=[], labellist=[], transparencylist=[], title=None, titlesize=None, xaxtitle=None, xaxtitlesize=None, xlims=(-0.5,-1), yaxtitle=None, yaxtitlesize=None, ymaxfactor=None, legendsize=None, opaque_legend=False) plot multiple sets of histograms to compare the shapes - setlist is a list of 2D numpy arrays containing histograms - fig and ax: a pyplot figure and axis object (if one of both is none a new figure is created) - title is a string that will be used as the title for the ax object other parameters are lists of which each element applies to one list of histograms plot_anomalous(histlist, ls, highlight=-1, hrange=-1) plot a range of histograms and highlight one of them input arguments: - histlist and ls: a list of histograms and corresponding lumisection numbers - highlight: the lumisection number of the histogram to highlight - hrange: the number of histograms before and after lsnumber to plot (default: whole run) plot_moments(moments, ls, dims=(0,1), fig=None, ax=None, markersize=10) plot the moments of a set of histograms input arguments: - moments: a numpy array of shape (nhists,nmoments) - dims: a tuple of two or three values between 0 and nmoments-1 plot_distance(dists, ls=None, rmlargest=0., doplot=True, title=None, xaxtitle='lumisection number', yaxtitle='distance metric') (no valid documentation found) plot_loss(data, xlims=None, title=None, xaxtitle='epoch', yaxtitle='loss', doshow=True) plot the training and validation loss data is the object returned by the .fit method when called upon a keras model e.g. history = <your autoencoder>.fit(<training params>) plot_loss(history,'a title') plot_mse(mse, rmlargest=0., doplot=True, title=None, xaxtitle='lumisection number', yaxtitle='mse') plot the mse's and return the mean and std input args: - mse is a 1D numpy array of mse scores - doplot: boolean whether to make a plot or simply return mean and std - rmlargest: fraction of largest mse's to remove (to avoid being too sensitive to outliers) plot_score_dist( scores, labels, fig=None, ax=None, nbins=20, normalize=False, siglabel='signal', sigcolor='g', bcklabel='background', bckcolor='r', title=None, xaxtitle=None, yaxtitle=None, doshow=True) make a plot showing the distributions of the output scores for signal and background plot_metric( wprange, metric, label=None, color=None, sig_eff=None, sig_label=None, sig_color=None, bck_eff=None, bck_label=None, bck_color=None, title=None, xaxtitle='working point', yaxlog=False, ymaxfactor=1.3, yaxtitle='metric' ) plot a metric based on signal and background efficiencies. along with the metric, the actual signal and background efficiencies can be plotted as well. input arguments: - wprange, metric: equally long 1D-numpy arrays, x- and y-data respectively - label: label for the metric to put in the legend - color: color for the metric (default: blue) - sig_eff: 1D-numpy array of signal efficiencies corresponding to wprange - sig_label: label for sig_eff in the legend - color: color for sig_eff (default: green) - bck_eff, bck_label, bck_color: same as for signal - title, xaxtitle and yaxtitle: titles for the plot and axes - yaxlog: boolean whether to put y axis in log scale - ymaxfactor: factor to add extra space on the top of the plot (for the legend) plot_fit_2d( points, fitfunc=None, logprob=False, clipprob=False, onlycontour=False, xlims=5, ylims=5, onlypositive=False, xaxtitle=None, xaxtitlesize=None, yaxtitle=None, yaxtitlesize=None, title=None, titlesize=None, caxtitle=None, caxtitlesize=None, transparency=1 ) make a scatter plot of a 2D point cloud with fitted contour input arguments: - points: a numpy array of shape (npoints,ndims) - fitfunc: an object of type CloudFitter (see src/cloudfitters) or any other object that implements a pdf(points) method - logprob: boolean whether to plot log probability or normal probability - clipprob: boolean whether to replace +- inf values by (non-inf) max and min - onlycontour: a boolean whether to draw only the fit or include the data points - xlims and ylims: tuples of (low,high) note: can be an integer, in which case the range will be determined automatically from the formula low = mean-xlims*std, high = mean+xlims*std, where mean and std are determined from the points array. - onlypositive: overrides previous argument to set lower bound of plotting range at 0 in both dimensions. - xaxtitle and yaxtitle: titles for axes. plot_fit_2d_clusters( points, clusters, labels, colors, **kwargs ) make a scatter plot of a fitted contour with point clouds superimposed input arguments: - points: numpy arrays of shape (npoints,ndims), usually the points to which the fit was done note: only used to determine plotting range, these points are not plotted! - clusters: list of numpy arrays of shape (npoints,ndims), clouds of points to plot - labels: list with legend entries (must be same length as clusters) - colors: list with colors (must be same length as clusters) - kwargs: passed down to plot_fit_2d note: onlycontour is set automatically and should not be in kwargs","title":"plot_utils"},{"location":"utils/plot_utils/#plot-utils","text":"A collection of useful basic functions for plotting.","title":"plot utils"},{"location":"utils/plot_utils/#make95legend95opaque-leg","text":"set the transparency of all entries in a legend to zero","title":"make_legend_opaque( leg )"},{"location":"utils/plot_utils/#add95text-ax-text-pos-fontsize10-background95facecolornone-background95alphanone-background95edgecolornone","text":"add text to an axis at a specified position (in relative figure coordinates) input arguments: - ax: matplotlib axis object - text: string, can contain latex syntax such as /textbf{} and /textit{} - pos: tuple with relative x- and y-axis coordinates of bottom left corner","title":"add_text( ax, text, pos,  fontsize=10, background_facecolor=None,  background_alpha=None, background_edgecolor=None )"},{"location":"utils/plot_utils/#add95cms95label-ax-pos0109-extratextnone-kwargs","text":"add the CMS label and extra text (e.g. 'Preliminary') to a plot special case of add_text, for convenience","title":"add_cms_label( ax, pos=(0.1,0.9), extratext=None, **kwargs )"},{"location":"utils/plot_utils/#make95text95latex95safe-text","text":"make a string safe to process with matplotlib's latex parser in case no tex parsing is wanted (e.g. escape underscores) to be extended when the need arises!","title":"make_text_latex_safe( text )"},{"location":"utils/plot_utils/#plot95histshistlist-fignone-axnone-colorlist-labellist-transparency1-xlims-05-1-titlenone-xaxtitlenone-yaxtitlenone-bkgcolornone-bkgcmapspring-bkgrangenone-bkgtitlenone","text":"plot some histograms (in histlist) in one figure using specified colors and/or labels - histlist is a list of 1D arrays containing the histograms (or a 2D array of shape (nhistograms,nbins)) - colorlist is a list or array containing colors (in string format), of length nhistograms note: it can also be a single string representing a color (in pyplot), then all histograms will take this color - labellist is a list or array containing labels for in legend, of length nhistograms - xlims is a tuple of min and max for the x-axis labels, defaults to (-0.5,nbins-0.5) - title, xaxtitle, yaxtitle: strings for histogram title, x-axis title and y-axis title respectively - bkgcolor: 1D array representing background color for the plot (color axis will be scaled between min and max in bkgcolor) note: if bkgcolor does not have the same length as the x-axis, it will be compressed or stretched to fit the axis, but this might be meaningless, depending on what you are trying to visualize! - bkgmap: name of valid pyplot color map for plotting the background color output: tuple of figure and axis objects, that can be used to further tune the look of the figure or save it","title":"plot_hists(histlist, fig=None, ax=None, colorlist=[], labellist=[], transparency=1, xlims=(-0.5,-1), title=None, xaxtitle=None, yaxtitle=None,  bkgcolor=None, bkgcmap='spring', bkgrange=None, bkgtitle=None)"},{"location":"utils/plot_utils/#plot95hists95multihistlist-fignone-axnone-colorlist-labellist-transparency1-xlims-05-1-titlenone-xaxtitlenone-yaxtitlenone","text":"plot many histograms (in histlist) in one figure using specified colors and/or labels - histlist is a list of 1D arrays containing the histograms (or a 2D array of shape (nhistograms,nbins)) - colorlist is a list or array containing numbers to be mapped to colors - labellist is a list or array containing labels for in legend output: tuple of figure and axis objects, that can be used to further tune the look of the figure or save it","title":"plot_hists_multi(histlist, fig=None, ax=None, colorlist=[], labellist=[], transparency=1, xlims=(-0.5,-1), title=None, xaxtitle=None, yaxtitle=None)"},{"location":"utils/plot_utils/#plot95hist952dhist-fignone-axnone-titlenone-xaxtitlenone-yaxtitlenone-caxrangenone","text":"plot a 2D histogram - hist is a 2D numpy array of shape (nxbins, nybins) notes: - if the histogram contains only nonnegative values, values below 1e-12 will not be plotted (i.e. they will be shown as white spots in the plot) to discriminate zero from small but nonzero - if the histogram contains negative values, the color axis will be symmetrized around zero","title":"plot_hist_2d(hist, fig=None, ax=None, title=None, xaxtitle=None, yaxtitle=None, caxrange=None)"},{"location":"utils/plot_utils/#plot95hists952dhists-ncols4-titlenone-subtitlesnone-xaxtitlenone-yaxtitlenone-caxrangenone","text":"plot multiple 2D histograms next to each other - hists: list of 2D numpy arrays of shape (nxbins,nybins), or an equivalent 3D numpy array - ncols: number of columns to use","title":"plot_hists_2d(hists, ncols=4, title=None, subtitles=None, xaxtitle=None, yaxtitle=None, caxrange=None)"},{"location":"utils/plot_utils/#plot95hists952d95gifhists-titlesnone-xaxtitlenone-yaxtitlenone-duration03-fignametemp95gifgif","text":"(no valid documentation found)","title":"plot_hists_2d_gif(hists, titles=None, xaxtitle=None, yaxtitle=None, duration=0.3, figname='temp_gif.gif')"},{"location":"utils/plot_utils/#plot95hists95from95dfdf-histtype-nhists","text":"plot a number of histograms in a dataframe - df is the dataframe from which to plot - histtype is the name of the histogram type (e.g. 'chargeInner_PXLayer_1') - nhists is the number of histograms to plot","title":"plot_hists_from_df(df, histtype, nhists)"},{"location":"utils/plot_utils/#plot95setssetlist-fignone-axnone-colorlist-labellist-transparencylist-titlenone-titlesizenone-xaxtitlenone-xaxtitlesizenone-xlims-05-1-yaxtitlenone-yaxtitlesizenone-ymaxfactornone-legendsizenone-opaque95legendfalse","text":"plot multiple sets of histograms to compare the shapes - setlist is a list of 2D numpy arrays containing histograms - fig and ax: a pyplot figure and axis object (if one of both is none a new figure is created) - title is a string that will be used as the title for the ax object other parameters are lists of which each element applies to one list of histograms","title":"plot_sets(setlist, fig=None, ax=None, colorlist=[], labellist=[], transparencylist=[], title=None, titlesize=None,  xaxtitle=None, xaxtitlesize=None, xlims=(-0.5,-1),  yaxtitle=None, yaxtitlesize=None, ymaxfactor=None,  legendsize=None, opaque_legend=False)"},{"location":"utils/plot_utils/#plot95anomaloushistlist-ls-highlight-1-hrange-1","text":"plot a range of histograms and highlight one of them input arguments: - histlist and ls: a list of histograms and corresponding lumisection numbers - highlight: the lumisection number of the histogram to highlight - hrange: the number of histograms before and after lsnumber to plot (default: whole run)","title":"plot_anomalous(histlist, ls, highlight=-1, hrange=-1)"},{"location":"utils/plot_utils/#plot95momentsmoments-ls-dims01-fignone-axnone-markersize10","text":"plot the moments of a set of histograms input arguments: - moments: a numpy array of shape (nhists,nmoments) - dims: a tuple of two or three values between 0 and nmoments-1","title":"plot_moments(moments, ls, dims=(0,1), fig=None, ax=None, markersize=10)"},{"location":"utils/plot_utils/#plot95distancedists-lsnone-rmlargest0-doplottrue-titlenone-xaxtitlelumisection-number-yaxtitledistance-metric","text":"(no valid documentation found)","title":"plot_distance(dists, ls=None, rmlargest=0., doplot=True, title=None, xaxtitle='lumisection number', yaxtitle='distance metric')"},{"location":"utils/plot_utils/#plot95lossdata-xlimsnone-titlenone-xaxtitleepoch-yaxtitleloss-doshowtrue","text":"plot the training and validation loss data is the object returned by the .fit method when called upon a keras model e.g. history = <your autoencoder>.fit(<training params>) plot_loss(history,'a title')","title":"plot_loss(data, xlims=None, title=None, xaxtitle='epoch', yaxtitle='loss', doshow=True)"},{"location":"utils/plot_utils/#plot95msemse-rmlargest0-doplottrue-titlenone-xaxtitlelumisection-number-yaxtitlemse","text":"plot the mse's and return the mean and std input args: - mse is a 1D numpy array of mse scores - doplot: boolean whether to make a plot or simply return mean and std - rmlargest: fraction of largest mse's to remove (to avoid being too sensitive to outliers)","title":"plot_mse(mse, rmlargest=0., doplot=True, title=None, xaxtitle='lumisection number', yaxtitle='mse')"},{"location":"utils/plot_utils/#plot95score95dist-scores-labels-fignone-axnone-nbins20-normalizefalse-siglabelsignal-sigcolorg-bcklabelbackground-bckcolorr-titlenone-xaxtitlenone-yaxtitlenone-doshowtrue","text":"make a plot showing the distributions of the output scores for signal and background","title":"plot_score_dist( scores, labels, fig=None, ax=None, nbins=20, normalize=False, siglabel='signal', sigcolor='g', bcklabel='background', bckcolor='r', title=None, xaxtitle=None, yaxtitle=None, doshow=True)"},{"location":"utils/plot_utils/#plot95metric-wprange-metric-labelnone-colornone-sig95effnone-sig95labelnone-sig95colornone-bck95effnone-bck95labelnone-bck95colornone-titlenone-xaxtitleworking-point-yaxlogfalse-ymaxfactor13-yaxtitlemetric","text":"plot a metric based on signal and background efficiencies. along with the metric, the actual signal and background efficiencies can be plotted as well. input arguments: - wprange, metric: equally long 1D-numpy arrays, x- and y-data respectively - label: label for the metric to put in the legend - color: color for the metric (default: blue) - sig_eff: 1D-numpy array of signal efficiencies corresponding to wprange - sig_label: label for sig_eff in the legend - color: color for sig_eff (default: green) - bck_eff, bck_label, bck_color: same as for signal - title, xaxtitle and yaxtitle: titles for the plot and axes - yaxlog: boolean whether to put y axis in log scale - ymaxfactor: factor to add extra space on the top of the plot (for the legend)","title":"plot_metric( wprange, metric, label=None, color=None, sig_eff=None, sig_label=None, sig_color=None, bck_eff=None, bck_label=None, bck_color=None, title=None, xaxtitle='working point', yaxlog=False, ymaxfactor=1.3, yaxtitle='metric' )"},{"location":"utils/plot_utils/#plot95fit952d-points-fitfuncnone-logprobfalse-clipprobfalse-onlycontourfalse-xlims5-ylims5-onlypositivefalse-xaxtitlenone-xaxtitlesizenone-yaxtitlenone-yaxtitlesizenone-titlenone-titlesizenone-caxtitlenone-caxtitlesizenone-transparency1","text":"make a scatter plot of a 2D point cloud with fitted contour input arguments: - points: a numpy array of shape (npoints,ndims) - fitfunc: an object of type CloudFitter (see src/cloudfitters) or any other object that implements a pdf(points) method - logprob: boolean whether to plot log probability or normal probability - clipprob: boolean whether to replace +- inf values by (non-inf) max and min - onlycontour: a boolean whether to draw only the fit or include the data points - xlims and ylims: tuples of (low,high) note: can be an integer, in which case the range will be determined automatically from the formula low = mean-xlims*std, high = mean+xlims*std, where mean and std are determined from the points array. - onlypositive: overrides previous argument to set lower bound of plotting range at 0 in both dimensions. - xaxtitle and yaxtitle: titles for axes.","title":"plot_fit_2d( points, fitfunc=None, logprob=False, clipprob=False,  onlycontour=False, xlims=5, ylims=5, onlypositive=False, xaxtitle=None, xaxtitlesize=None, yaxtitle=None, yaxtitlesize=None,  title=None, titlesize=None, caxtitle=None, caxtitlesize=None, transparency=1 )"},{"location":"utils/plot_utils/#plot95fit952d95clusters-points-clusters-labels-colors-kwargs","text":"make a scatter plot of a fitted contour with point clouds superimposed input arguments: - points: numpy arrays of shape (npoints,ndims), usually the points to which the fit was done note: only used to determine plotting range, these points are not plotted! - clusters: list of numpy arrays of shape (npoints,ndims), clouds of points to plot - labels: list with legend entries (must be same length as clusters) - colors: list with colors (must be same length as clusters) - kwargs: passed down to plot_fit_2d note: onlycontour is set automatically and should not be in kwargs","title":"plot_fit_2d_clusters( points, clusters, labels, colors, **kwargs )"},{"location":"utils/refruns_utils/","text":"refruns utils Tools for retrieving a reference run for a given run Preliminary implementation, based on a json file generated by the Tracker DQM group. Retrieved from here . Perhaps modify code later to fetch the up-to-date version at runtime instead of having to download a new version. Maybe also using an API to retrieve only the requested run instead of loading the entire file into memory. get_reference_run( runnb, jsonlist=None, jsonfile='json_allRunsRefRuns.json' ) get the reference run for a given run number input arguments: - runnb: integer representing a run number. - jsonlist: list matching run numbers to reference run numbers. note: the list is supposed to contain dicts with keys 'run_number' and 'reference_run_number', this convention is based on the json file provided by the tracker group. note: if jsonlist is None, jsonfile (see below) will be opened and a jsonlist read from it. - jsonfile: path to json file matching run numbers to reference run numbers. note: the json file must contain a list of dicts with keys 'run_number' and 'reference_run_number', as explained above. note: ignored if jsonlist is not None. output: integer representing the reference run number for the given run. if the given run is not in the json, -1 is returned.","title":"refruns_utils"},{"location":"utils/refruns_utils/#refruns-utils","text":"Tools for retrieving a reference run for a given run Preliminary implementation, based on a json file generated by the Tracker DQM group. Retrieved from here . Perhaps modify code later to fetch the up-to-date version at runtime instead of having to download a new version. Maybe also using an API to retrieve only the requested run instead of loading the entire file into memory.","title":"refruns utils"},{"location":"utils/refruns_utils/#get95reference95run-runnb-jsonlistnone-jsonfilejson95allrunsrefrunsjson","text":"get the reference run for a given run number input arguments: - runnb: integer representing a run number. - jsonlist: list matching run numbers to reference run numbers. note: the list is supposed to contain dicts with keys 'run_number' and 'reference_run_number', this convention is based on the json file provided by the tracker group. note: if jsonlist is None, jsonfile (see below) will be opened and a jsonlist read from it. - jsonfile: path to json file matching run numbers to reference run numbers. note: the json file must contain a list of dicts with keys 'run_number' and 'reference_run_number', as explained above. note: ignored if jsonlist is not None. output: integer representing the reference run number for the given run. if the given run is not in the json, -1 is returned.","title":"get_reference_run( runnb, jsonlist=None, jsonfile='json_allRunsRefRuns.json' )"}]}