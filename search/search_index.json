{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is some test text","title":"Home"},{"location":"utils/ae_combine_utils/","text":"ae combine utils ## histstructure (no valid documentation found) ### __init__(self) (no valid documentation found) ### create(self,year,histnames,jsonselector=None,highstatonly=False,dcsononly=False) **create the histstructure given the arguments provided** most arguments are self-explanatory remarks: - if jsonselector is None, no selection will be done on run/ls number, i.e. all runs and lumisections are kept - if jsonselector contains a single negative run number as key, templates will be used (e.g. averaging the dataset) instead of actual ls from the data for example, if jsonselector = {\"-15\":[[-1]]}, the dataset will be split in 15 parts and each part will be averaged to yield a single histogram (per type) ### get_golden_mask(self) return a boolean mask on the lumisections whether or not they belong to the golden json ### get_golden_indices(self) return an array of indices of lumisections that belong to the golden json ### get_perrun_indices(self) return a list of arrays of indices of lumisections, one element in the list represents one run ## get_mse_array(histstruct,valkey,dims=[]) (no valid documentation found) ## fitseminormal(histstruct,valkey,dims=[],fitnew=True,savefit=False) (no valid documentation found) ## fitgaussiankde(histstruct,valkey,dims=[],maxnpoints=-1) (no valid documentation found) ## plotfit2d(histstruct,valkey,dims,fitfunc,doinitialplot=True,onlycontour=False,rangestd=30) (no valid documentation found) ## msenormalizer (no valid documentation found) ### __init__(self) (no valid documentation found) ### fit(self,array) (no valid documentation found) ### apply(self,array) (no valid documentation found)","title":"ae combine utils"},{"location":"utils/ae_combine_utils/#ae-combine-utils","text":"## histstructure (no valid documentation found) ### __init__(self) (no valid documentation found) ### create(self,year,histnames,jsonselector=None,highstatonly=False,dcsononly=False) **create the histstructure given the arguments provided** most arguments are self-explanatory remarks: - if jsonselector is None, no selection will be done on run/ls number, i.e. all runs and lumisections are kept - if jsonselector contains a single negative run number as key, templates will be used (e.g. averaging the dataset) instead of actual ls from the data for example, if jsonselector = {\"-15\":[[-1]]}, the dataset will be split in 15 parts and each part will be averaged to yield a single histogram (per type) ### get_golden_mask(self) return a boolean mask on the lumisections whether or not they belong to the golden json ### get_golden_indices(self) return an array of indices of lumisections that belong to the golden json ### get_perrun_indices(self) return a list of arrays of indices of lumisections, one element in the list represents one run ## get_mse_array(histstruct,valkey,dims=[]) (no valid documentation found) ## fitseminormal(histstruct,valkey,dims=[],fitnew=True,savefit=False) (no valid documentation found) ## fitgaussiankde(histstruct,valkey,dims=[],maxnpoints=-1) (no valid documentation found) ## plotfit2d(histstruct,valkey,dims,fitfunc,doinitialplot=True,onlycontour=False,rangestd=30) (no valid documentation found) ## msenormalizer (no valid documentation found) ### __init__(self) (no valid documentation found) ### fit(self,array) (no valid documentation found) ### apply(self,array) (no valid documentation found)","title":"ae combine utils"},{"location":"utils/autoencoder_utils/","text":"autoencoder utils ## mseTop10(y_true, y_pred) (no valid documentation found) ## mseTop10Raw(y_true, y_pred) same as above but without using tf or K the version including tf or K seemed to cause randomly dying kernels, no clear reason could be found, but it was solved using this loss function instead. verified that it gives exactly the same output as the function above on some random arrays does only work for arrays with 2D shapes, not for (nbins,) ## mseTopNRaw(y_true, y_pred, n=10) generalization of the above ## chiSquared(y_true, y_pred) (no valid documentation found) ## chiSquaredTop10(y_true, y_pred) (no valid documentation found) ## get_roc(scores, labels, mode='classic', doplot=True) make a ROC curve input arguments: - scores is a 1D numpy array containing output scores of any algorithm - labels is a 1D numpy array (equally long as scores) containing labels note that 1 for signal and 0 for background is assumed! this convention is only used to define what scores belong to signal or background; the scores itself can be anything (not limited to (0,1)), as long as the target for signal is higher than the target for background - mode: how to plot the roc curve; options are: - 'classic' = signal efficiency afo background efficiency - doplot: boolean whether to make a plot or simply return the auc. ## get_roc_from_hists(hists, labels, predicted_hists, mode='classic', doplot=True) make a ROC curve without manually calculating the scores the output score is the mse between the histograms and their reconstruction hists and predicted_hists are 2D numpy arrays of shape (nhistograms,nbins) other arguments: see get_roc ## get_confusion_matrix(scores, labels, wp) plot a confusion matrix scores and labels are defined in the same way as for get_roc wp is the chosen working point (i.e. any score above wp is flagged as signal, any below is flagged as background) ## get_confusion_matrix_from_hists(hists, labels, predicted_hists, msewp) plot a confusion matrix without manually calculating the scores the output score is the mse between the histograms and their reconstruction ## getautoencoder(input_size,arch,act=[],opt='adam',loss=mseTop10) get a trainable autoencoder model input args: - input_size: size of vector that autoencoder will operate on - arch: list of number of nodes per hidden layer (excluding input and output layer) - act: list of activations per layer (default: tanh) - opt: optimizer to use (default: adam) - loss: loss function to use (defualt: mseTop10) ## train_simple_autoencoder(hists,nepochs=-1,modelname='') create and train a very simple keras model the model consists of one hidden layer (with half as many units as there are input bins), tanh activation, adam optimizer and mseTop10 loss. input args: - hists is a 2D numpy array of shape (nhistograms, nbins) - nepochs is the number of epochs to use (has a default value if left unspecified) - modelname is a file name to save the model in (default: model is not saved to a file)","title":"autoencoder utils"},{"location":"utils/autoencoder_utils/#autoencoder-utils","text":"## mseTop10(y_true, y_pred) (no valid documentation found) ## mseTop10Raw(y_true, y_pred) same as above but without using tf or K the version including tf or K seemed to cause randomly dying kernels, no clear reason could be found, but it was solved using this loss function instead. verified that it gives exactly the same output as the function above on some random arrays does only work for arrays with 2D shapes, not for (nbins,) ## mseTopNRaw(y_true, y_pred, n=10) generalization of the above ## chiSquared(y_true, y_pred) (no valid documentation found) ## chiSquaredTop10(y_true, y_pred) (no valid documentation found) ## get_roc(scores, labels, mode='classic', doplot=True) make a ROC curve input arguments: - scores is a 1D numpy array containing output scores of any algorithm - labels is a 1D numpy array (equally long as scores) containing labels note that 1 for signal and 0 for background is assumed! this convention is only used to define what scores belong to signal or background; the scores itself can be anything (not limited to (0,1)), as long as the target for signal is higher than the target for background - mode: how to plot the roc curve; options are: - 'classic' = signal efficiency afo background efficiency - doplot: boolean whether to make a plot or simply return the auc. ## get_roc_from_hists(hists, labels, predicted_hists, mode='classic', doplot=True) make a ROC curve without manually calculating the scores the output score is the mse between the histograms and their reconstruction hists and predicted_hists are 2D numpy arrays of shape (nhistograms,nbins) other arguments: see get_roc ## get_confusion_matrix(scores, labels, wp) plot a confusion matrix scores and labels are defined in the same way as for get_roc wp is the chosen working point (i.e. any score above wp is flagged as signal, any below is flagged as background) ## get_confusion_matrix_from_hists(hists, labels, predicted_hists, msewp) plot a confusion matrix without manually calculating the scores the output score is the mse between the histograms and their reconstruction ## getautoencoder(input_size,arch,act=[],opt='adam',loss=mseTop10) get a trainable autoencoder model input args: - input_size: size of vector that autoencoder will operate on - arch: list of number of nodes per hidden layer (excluding input and output layer) - act: list of activations per layer (default: tanh) - opt: optimizer to use (default: adam) - loss: loss function to use (defualt: mseTop10) ## train_simple_autoencoder(hists,nepochs=-1,modelname='') create and train a very simple keras model the model consists of one hidden layer (with half as many units as there are input bins), tanh activation, adam optimizer and mseTop10 loss. input args: - hists is a 2D numpy array of shape (nhistograms, nbins) - nepochs is the number of epochs to use (has a default value if left unspecified) - modelname is a file name to save the model in (default: model is not saved to a file)","title":"autoencoder utils"},{"location":"utils/clustering_utils/","text":"clustering utils ## fitfunction abstract base class for all fit functions all other fit functions inherit from fitfunction and overload its functions no concrete fitting procedure is implemented, but some basic checks on dimensionality are performed ### __init__(self,points) (no valid documentation found) ### pdf(self,points) **get the pdf (probability density function) value at given points** points is a 2D numpy array of shape (npoints,ndims) the output is a 1D array of shape (npoints) ### pdfgrid(self,grid) **get the pdf (probability density function) value at a given grid** (only applicable to 2D case!) grid is a np array of shape (nx,ny,2) containing the x- and y-values in its first and second depth-wise dimension respectively. the grid is typically (but not necessarily) created via: x,y = np.mgrid[<xrange>,<yrange>] grid = np.dstack(x,y) ## lognormal(fitfunction) (no valid documentation found) ### __init__(self,points) points is a np array of shape (npoints,ndims) ### pdf(self,points) (no valid documentation found) ### pdfgrid(self,grid) (no valid documentation found) ## exponential(fitfunction) (no valid documentation found) ### __init__(self,points) points is a np array of shape (npoints,ndims) ### pdf(self,points) (no valid documentation found) ### pdfgrid(self,grid) (no valid documentation found) ## seminormal(fitfunction) this is not strictly speaking a probability distribution, only the first quadrant of the result of fitting a normal distribution to the data + its mirror image wrt the origin ### __init__(self,points=[]) (no valid documentation found) ### pdf(self,points) (no valid documentation found) ### pdfgrid(self,grid) (no valid documentation found) ### save(self,path) (no valid documentation found) ### load(self,path) (no valid documentation found) ## gaussiankde(fitfunction) wrapper for scipy.stats.gaussian_kde (gaussian kernel density estimation) ### __init__(self,points=[],bw='default') (no valid documentation found) ### pdf(self,points) (no valid documentation found) ### pdfgrid(self,grid) (no valid documentation found) ## vecdist(moments,index) does not work well if there are outliers which dominate the distance ## costhetadist(moments,index) works more or less but not all bad points have small values, allows to identify problematic regions but not individual LS ## avgnndist(moments,index,nn) seems to work well for the runs tested! ## getavgnndist(hists,nmoments,xmin,xmax,nbins,nneighbours) (no valid documentation found) ## filteranomalous(df,nmoments=3,rmouterflow=True,rmlargest=0.,doplot=True,) (no valid documentation found)","title":"clustering utils"},{"location":"utils/clustering_utils/#clustering-utils","text":"## fitfunction abstract base class for all fit functions all other fit functions inherit from fitfunction and overload its functions no concrete fitting procedure is implemented, but some basic checks on dimensionality are performed ### __init__(self,points) (no valid documentation found) ### pdf(self,points) **get the pdf (probability density function) value at given points** points is a 2D numpy array of shape (npoints,ndims) the output is a 1D array of shape (npoints) ### pdfgrid(self,grid) **get the pdf (probability density function) value at a given grid** (only applicable to 2D case!) grid is a np array of shape (nx,ny,2) containing the x- and y-values in its first and second depth-wise dimension respectively. the grid is typically (but not necessarily) created via: x,y = np.mgrid[<xrange>,<yrange>] grid = np.dstack(x,y) ## lognormal(fitfunction) (no valid documentation found) ### __init__(self,points) points is a np array of shape (npoints,ndims) ### pdf(self,points) (no valid documentation found) ### pdfgrid(self,grid) (no valid documentation found) ## exponential(fitfunction) (no valid documentation found) ### __init__(self,points) points is a np array of shape (npoints,ndims) ### pdf(self,points) (no valid documentation found) ### pdfgrid(self,grid) (no valid documentation found) ## seminormal(fitfunction) this is not strictly speaking a probability distribution, only the first quadrant of the result of fitting a normal distribution to the data + its mirror image wrt the origin ### __init__(self,points=[]) (no valid documentation found) ### pdf(self,points) (no valid documentation found) ### pdfgrid(self,grid) (no valid documentation found) ### save(self,path) (no valid documentation found) ### load(self,path) (no valid documentation found) ## gaussiankde(fitfunction) wrapper for scipy.stats.gaussian_kde (gaussian kernel density estimation) ### __init__(self,points=[],bw='default') (no valid documentation found) ### pdf(self,points) (no valid documentation found) ### pdfgrid(self,grid) (no valid documentation found) ## vecdist(moments,index) does not work well if there are outliers which dominate the distance ## costhetadist(moments,index) works more or less but not all bad points have small values, allows to identify problematic regions but not individual LS ## avgnndist(moments,index,nn) seems to work well for the runs tested! ## getavgnndist(hists,nmoments,xmin,xmax,nbins,nneighbours) (no valid documentation found) ## filteranomalous(df,nmoments=3,rmouterflow=True,rmlargest=0.,doplot=True,) (no valid documentation found)","title":"clustering utils"},{"location":"utils/csv_utils/","text":"csv utils ## get_data_dirs(year='2017',eras=[],dim=1) yield all data directories note that the location of the data is hard-coded; this function might break for newer or later reprocessings of the data. - year is a string, either '2017' or '2018' - era is a list containing a selection of era names (default empty list = all eras) - dim is either 1 or 2 (for 1D or 2D plots) ## get_csv_files(inputdir) yields paths to all csv files in input directory note that the output paths consist of input_dir/filename this function is only meant for 1-level down searching, i.e. the .csv files listed directly under input_dir. ## sort_filenames(filelist) sort filenames in numerical order (e.g. 2 before 10) note that the number is supposed to be in ..._ . format ## read_csv(csv_file) read csv file into pandas dataframe csv_file is the path to the csv file to be read ## read_and_merge_csv(csv_files,histnames=[],runnbs=[]) read and merge list of csv files into a single df csv_files is a list of paths to files to merge into a df histnames is a list of the types of histograms to keep (default: all) runnbs is a list of run numbers to keep (default: all) ## write_skimmed_csv(histnames,year,eras=['all']) read all available data for a given year/era and make a file per histogram type input arguments: - histnames: list of histogram names for which to make a separate file - year: data-taking year (in string format) - eras: data-taking eras for which to make a separate file (in string format) use 'all' to make a file with all eras merged, i.e. a full data taking year output: - one csv file per year/era and per histogram type note: this function can take quite a while to run!","title":"csv utils"},{"location":"utils/csv_utils/#csv-utils","text":"## get_data_dirs(year='2017',eras=[],dim=1) yield all data directories note that the location of the data is hard-coded; this function might break for newer or later reprocessings of the data. - year is a string, either '2017' or '2018' - era is a list containing a selection of era names (default empty list = all eras) - dim is either 1 or 2 (for 1D or 2D plots) ## get_csv_files(inputdir) yields paths to all csv files in input directory note that the output paths consist of input_dir/filename this function is only meant for 1-level down searching, i.e. the .csv files listed directly under input_dir. ## sort_filenames(filelist) sort filenames in numerical order (e.g. 2 before 10) note that the number is supposed to be in ..._ . format ## read_csv(csv_file) read csv file into pandas dataframe csv_file is the path to the csv file to be read ## read_and_merge_csv(csv_files,histnames=[],runnbs=[]) read and merge list of csv files into a single df csv_files is a list of paths to files to merge into a df histnames is a list of the types of histograms to keep (default: all) runnbs is a list of run numbers to keep (default: all) ## write_skimmed_csv(histnames,year,eras=['all']) read all available data for a given year/era and make a file per histogram type input arguments: - histnames: list of histogram names for which to make a separate file - year: data-taking year (in string format) - eras: data-taking eras for which to make a separate file (in string format) use 'all' to make a file with all eras merged, i.e. a full data taking year output: - one csv file per year/era and per histogram type note: this function can take quite a while to run!","title":"csv utils"},{"location":"utils/dataframe_utils/","text":"dataframe utils ## get_histnames(df) get a list of (unique) histogram names present in a df df is a dataframe read from an input csv file. ## select_histnames(df,histnames) keep only a subset of histograms in a df histnames is a list of histogram names to keep in the df. ## get_runs(df) return a list of (unique) run numbers present in a df df is a dataframe read from an input csv file. ## select_runs(df,runnbs) keep only a subset of runs in a df runnbs is a list of run numbers to keep in the df. ## get_ls(df) return a list of ls numbers present in a df note that the numbers are not required to be unique! note: no check is done on the run number! ## select_ls(df,lsnbs) keep only a subset of lumisection numbers in a df lsnbs is a list of lumisection numbers to keep in the df. note: no check is done on the run number! ## get_runsls(df) return a dictionary with runs and lumisections in a dataframe (same format as e.g. golden json) ## select_json(df,jsonfile) keep only lumisections that are in the given json file ## select_runsls(df,jsondict) equivalent to select_json but using a pre-loaded json dict instead of a json file on disk ## select_golden(df) keep only golden lumisections in df ## select_notgolden(df) keep all but golden lumisections in df ## select_dcson(df) keep only lumisections in df that have DCS-bit on ## select_dcsoff(df) keep only lumisections in df that have DCS-bit off ## select_pixelgood(df) keep only lumisections in df that are in good pixel json ## select_pixelbad(df) keep only lumisections in df that are in bad pixel json ## get_highstat(df,entries_to_bins_ratio=100) return a select object of runs and ls of histograms with high statistics ## select_highstat(df,entries_to_bins_ratio=100) (no valid documentation found) ## get_hist_values(df) same as builtin \"df['histo'].values\" but convert strings to np arrays also an array of run and LS numbers is returned warning: no check is done to assure that all histograms are of the same type!","title":"dataframe utils"},{"location":"utils/dataframe_utils/#dataframe-utils","text":"## get_histnames(df) get a list of (unique) histogram names present in a df df is a dataframe read from an input csv file. ## select_histnames(df,histnames) keep only a subset of histograms in a df histnames is a list of histogram names to keep in the df. ## get_runs(df) return a list of (unique) run numbers present in a df df is a dataframe read from an input csv file. ## select_runs(df,runnbs) keep only a subset of runs in a df runnbs is a list of run numbers to keep in the df. ## get_ls(df) return a list of ls numbers present in a df note that the numbers are not required to be unique! note: no check is done on the run number! ## select_ls(df,lsnbs) keep only a subset of lumisection numbers in a df lsnbs is a list of lumisection numbers to keep in the df. note: no check is done on the run number! ## get_runsls(df) return a dictionary with runs and lumisections in a dataframe (same format as e.g. golden json) ## select_json(df,jsonfile) keep only lumisections that are in the given json file ## select_runsls(df,jsondict) equivalent to select_json but using a pre-loaded json dict instead of a json file on disk ## select_golden(df) keep only golden lumisections in df ## select_notgolden(df) keep all but golden lumisections in df ## select_dcson(df) keep only lumisections in df that have DCS-bit on ## select_dcsoff(df) keep only lumisections in df that have DCS-bit off ## select_pixelgood(df) keep only lumisections in df that are in good pixel json ## select_pixelbad(df) keep only lumisections in df that are in bad pixel json ## get_highstat(df,entries_to_bins_ratio=100) return a select object of runs and ls of histograms with high statistics ## select_highstat(df,entries_to_bins_ratio=100) (no valid documentation found) ## get_hist_values(df) same as builtin \"df['histo'].values\" but convert strings to np arrays also an array of run and LS numbers is returned warning: no check is done to assure that all histograms are of the same type!","title":"dataframe utils"},{"location":"utils/generate_data_utils/","text":"generate data utils ## goodnoise(nbins, fstd=None) generate one sample of 'good' noise consisting of fourier components input args: - nbins: number of bins, length of noise array to be sampled - fstd: an array of length nbins used for scaling of the amplitude of the noise bin-by-bin. output: - numpy array of length nbins containing the noise ## badnoise(nbins,fstd=None) generate one sample of 'bad' noise consisting of fourier components (higher frequency and amplitude than 'good' noise) input args and output: simlar to goodnoise WARNING: NOT NECESSARILY REPRESENTATIVE OF ANOMALIES TO BE EXPECTED, DO NOT USE ## whitenoise(nbins,fstd=None) generate one sample of white noise (uncorrelated between bins) input args and output: similar to goodnoise ## random_lico(hists) generate one linear combination of histograms with random coefficients in (0,1) summing to 1 input args: - numpy array of shape (nhists,nbins), the rows of which will be linearly combined output: - numpy array of shape (nbins), containing the new histogram ## smoother(inarray,halfwidth) smooth the rows of a 2D array using the 2*halfwidth+1 surrounding values. ## mse_correlation_vector(hists,index) calculate mse of a histogram at given index wrt all other histograms input args: - hists: numpy array of shape (nhists,nbins) containing the histograms - index: the index (must be in (0,len(hists)-1)) of the histogram in question output: - numpy array of length nhists containing mse of the indexed histogram with respect to all other histograms WARNING: can be slow if called many times on a large collection of histograms with many bins. ## moments_correlation_vector(moments,index) calculate moment distance of hist at index wrt all other hists very similar to mse_correlation_vector but using histogram moments instead of full histograms for speed-up ## plot_data_and_gen(nplot,datahist,genhist,figname='fig.png') plot a couple of random examples from rhist (data), ghist (resampled 'good') and bhist (resampled 'bad') input arguments: - nplot: integer, maximum number of examples to plot - datahist, genhist: numpy arrays of shape (nhists,nbins) - figname: name of figure to plot ## plot_seed_and_gen(seedhist,genhist,figname='fig.png') plot a couple of random examples from rhist (data), ghist (resampled 'good') and bhist (resampled 'bad') input arguments: - datahist, genhist: numpy arrays of shape (nhists,nbins) - figname: name of figure to plot ## plot_noise(noise,histstd=None,figname='fig.png') plot histograms in noise (numpy array of shape (nhists,nbins)) optional argument histstd plots +- histstd as boundaries ## fourier_noise_on_mean(hists,outfilename='',figname='',nresamples=0,nonnegative=True) apply fourier noise on the bin-per-bin mean histogram, with amplitude scaling based on bin-per-bin std histogram. input args: - hists: numpy array of shape (nhists,nbins) used for determining mean and std - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw (default: number of input histograms / 10) - nonnegative: boolean whether to set all bins to minimum zero after applying noise MOSTLY SUITABLE AS HELP FUNCTION FOR RESAMPLE_SIMILAR_FOURIER_NOISE, NOT AS GENERATOR IN ITSELF ## fourier_noise(hists,outfilename='',figname='',nresamples=1,nonnegative=True,stdfactor=15.) apply fourier noise on random histograms with simple flat amplitude scaling. input args: - hists: numpy array of shape (nhists,nbins) used for seeding - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw per input histogram - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: factor to scale magnitude of noise (larger factor = smaller noise) ## upsample_hist_set(hists,ntarget,fourierstdfactor=15.,figname='f') (no valid documentation found) ## white_noise(hists,figname='',stdfactor=15.) apply white noise to the histograms in hists. input args: - hists: np array (nhists,nbins) containing input histograms - figname: path to figure plotting examples (default: no plotting) - stdfactor: scaling factor of white noise amplitude (higher factor = smaller noise) ## resample_bin_per_bin(hists,outfilename='',figname='',nresamples=0,nonnegative=True,smoothinghalfwidth=2) do resampling from bin-per-bin probability distributions input args: - hists: np array (nhists,nbins) containing the histograms to draw new samples from - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw (default: 1/10 of number of input histograms) - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - smoothinghalfwidth: halfwidth of smoothing procedure to apply on the result (default: no smoothing) ## resample_similar_bin_per_bin( allhists, selhists, outfilename='', figname='', nresamples=1, nonnegative=True, (no valid documentation found) ## resample_similar_fourier_noise( allhists, selhists, outfilename='', figname='', nresamples=1, nonnegative=True, (no valid documentation found) ## resample_similar_lico( allhists, selhists, outfilename='', figname='', nresamples=1, nonnegative=True, (no valid documentation found) ## mc_sampling(hists, nMC=10000 , nresamples=10) resampling of a histogram using MC methods (Marek's method) Drawing random points from a space defined by the range of the histogram in all axes. Points are \"accepted\" if the fall under the sampled histogram: f(x) - sampled distribution x_r, y_r -> randomly sampled point if y_r<=f(x_r), fill the new distribution at bin corresponding to x_r with weight: weight = (sum of input hist)/(#mc points accepted) this is equal to weight = (MC space volume)/(# all MC points)","title":"generate data utils"},{"location":"utils/generate_data_utils/#generate-data-utils","text":"## goodnoise(nbins, fstd=None) generate one sample of 'good' noise consisting of fourier components input args: - nbins: number of bins, length of noise array to be sampled - fstd: an array of length nbins used for scaling of the amplitude of the noise bin-by-bin. output: - numpy array of length nbins containing the noise ## badnoise(nbins,fstd=None) generate one sample of 'bad' noise consisting of fourier components (higher frequency and amplitude than 'good' noise) input args and output: simlar to goodnoise WARNING: NOT NECESSARILY REPRESENTATIVE OF ANOMALIES TO BE EXPECTED, DO NOT USE ## whitenoise(nbins,fstd=None) generate one sample of white noise (uncorrelated between bins) input args and output: similar to goodnoise ## random_lico(hists) generate one linear combination of histograms with random coefficients in (0,1) summing to 1 input args: - numpy array of shape (nhists,nbins), the rows of which will be linearly combined output: - numpy array of shape (nbins), containing the new histogram ## smoother(inarray,halfwidth) smooth the rows of a 2D array using the 2*halfwidth+1 surrounding values. ## mse_correlation_vector(hists,index) calculate mse of a histogram at given index wrt all other histograms input args: - hists: numpy array of shape (nhists,nbins) containing the histograms - index: the index (must be in (0,len(hists)-1)) of the histogram in question output: - numpy array of length nhists containing mse of the indexed histogram with respect to all other histograms WARNING: can be slow if called many times on a large collection of histograms with many bins. ## moments_correlation_vector(moments,index) calculate moment distance of hist at index wrt all other hists very similar to mse_correlation_vector but using histogram moments instead of full histograms for speed-up ## plot_data_and_gen(nplot,datahist,genhist,figname='fig.png') plot a couple of random examples from rhist (data), ghist (resampled 'good') and bhist (resampled 'bad') input arguments: - nplot: integer, maximum number of examples to plot - datahist, genhist: numpy arrays of shape (nhists,nbins) - figname: name of figure to plot ## plot_seed_and_gen(seedhist,genhist,figname='fig.png') plot a couple of random examples from rhist (data), ghist (resampled 'good') and bhist (resampled 'bad') input arguments: - datahist, genhist: numpy arrays of shape (nhists,nbins) - figname: name of figure to plot ## plot_noise(noise,histstd=None,figname='fig.png') plot histograms in noise (numpy array of shape (nhists,nbins)) optional argument histstd plots +- histstd as boundaries ## fourier_noise_on_mean(hists,outfilename='',figname='',nresamples=0,nonnegative=True) apply fourier noise on the bin-per-bin mean histogram, with amplitude scaling based on bin-per-bin std histogram. input args: - hists: numpy array of shape (nhists,nbins) used for determining mean and std - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw (default: number of input histograms / 10) - nonnegative: boolean whether to set all bins to minimum zero after applying noise MOSTLY SUITABLE AS HELP FUNCTION FOR RESAMPLE_SIMILAR_FOURIER_NOISE, NOT AS GENERATOR IN ITSELF ## fourier_noise(hists,outfilename='',figname='',nresamples=1,nonnegative=True,stdfactor=15.) apply fourier noise on random histograms with simple flat amplitude scaling. input args: - hists: numpy array of shape (nhists,nbins) used for seeding - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw per input histogram - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: factor to scale magnitude of noise (larger factor = smaller noise) ## upsample_hist_set(hists,ntarget,fourierstdfactor=15.,figname='f') (no valid documentation found) ## white_noise(hists,figname='',stdfactor=15.) apply white noise to the histograms in hists. input args: - hists: np array (nhists,nbins) containing input histograms - figname: path to figure plotting examples (default: no plotting) - stdfactor: scaling factor of white noise amplitude (higher factor = smaller noise) ## resample_bin_per_bin(hists,outfilename='',figname='',nresamples=0,nonnegative=True,smoothinghalfwidth=2) do resampling from bin-per-bin probability distributions input args: - hists: np array (nhists,nbins) containing the histograms to draw new samples from - outfilename: path to csv file to write results to (default: no writing) - figname: path to figure plotting examples (default: no plotting) - nresamples: number of samples to draw (default: 1/10 of number of input histograms) - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - smoothinghalfwidth: halfwidth of smoothing procedure to apply on the result (default: no smoothing) ## resample_similar_bin_per_bin( allhists, selhists, outfilename='', figname='', nresamples=1, nonnegative=True, (no valid documentation found) ## resample_similar_fourier_noise( allhists, selhists, outfilename='', figname='', nresamples=1, nonnegative=True, (no valid documentation found) ## resample_similar_lico( allhists, selhists, outfilename='', figname='', nresamples=1, nonnegative=True, (no valid documentation found) ## mc_sampling(hists, nMC=10000 , nresamples=10) resampling of a histogram using MC methods (Marek's method) Drawing random points from a space defined by the range of the histogram in all axes. Points are \"accepted\" if the fall under the sampled histogram: f(x) - sampled distribution x_r, y_r -> randomly sampled point if y_r<=f(x_r), fill the new distribution at bin corresponding to x_r with weight: weight = (sum of input hist)/(#mc points accepted) this is equal to weight = (MC space volume)/(# all MC points)","title":"generate data utils"},{"location":"utils/hist_utils/","text":"hist utils ## rebinhists(hists,factor) perform rebinning on a set of histograms hists is a numpy array of shape (nhistograms,nbins) factor is the rebinning factor, which must be a divisor of nbins. ## normalizehists(hists) perform normalization (i.e. sum of bin contents equals one for each histogram) ## averagehists(hists,nout) partition hists (of shape (nhistograms,nbins)) into nout parts and take the average histogram of each part ## moment(bins,counts,order) get n-th central moment of a histogram - bins is a 1D or 2D np array holding the bin centers (shape (nbins) or (nhistograms,nbins)) - array is a 2D np array containing the bin counts (shape (nhistograms,nbins)) - order is the order of the moment to calculate (0 = maximum, 1 = mean value) ## histmean(bins,counts) special case of moment calculation ## histrms(bins,counts) special case of moment calculation ## histmoments(bins,counts,orders) apply moment calculation for a list of orders the return type is a numpy array of shape (nhistograms,nmoments) ## preparedatafromnpy(dataname, rebinningfactor=1, donormalize=True, doplot=False) read a .npy file and output the histograms ## preparedatafromdf(df, returnrunls=False, onlygolden=False, rebinningfactor=1, donormalize=True, doplot=False) prepare the data contained in a dataframe in the form of a numpy array args: - returnrunls: wether to return only a histogram array or 1D arrays of run and lumisection numbers as well - onlygolden: if True, only lumisections in the golden json file are kept - rebinningfactor: an integer number to downsample the histograms in the dataframe - donormalize: if True, data are normalized - doplot: if True, some example plots are made showing the histograms ## preparedatafromcsv(dataname, returnrunls=False, onlygolden=False, rebinningfactor=1, donormalize=True, doplot=False) prepare the data contained in a dataframe csv file in the form of a numpy array args: - returnrunls: wether to return only a histogram array or 1D arrays of run and lumisection numbers as well - onlygolden: if True, only lumisections in the golden json file are kept - rebinningfactor: an integer number to downsample the histograms in the dataframe - doplot: if True, some example plots are made showing the histograms","title":"hist utils"},{"location":"utils/hist_utils/#hist-utils","text":"## rebinhists(hists,factor) perform rebinning on a set of histograms hists is a numpy array of shape (nhistograms,nbins) factor is the rebinning factor, which must be a divisor of nbins. ## normalizehists(hists) perform normalization (i.e. sum of bin contents equals one for each histogram) ## averagehists(hists,nout) partition hists (of shape (nhistograms,nbins)) into nout parts and take the average histogram of each part ## moment(bins,counts,order) get n-th central moment of a histogram - bins is a 1D or 2D np array holding the bin centers (shape (nbins) or (nhistograms,nbins)) - array is a 2D np array containing the bin counts (shape (nhistograms,nbins)) - order is the order of the moment to calculate (0 = maximum, 1 = mean value) ## histmean(bins,counts) special case of moment calculation ## histrms(bins,counts) special case of moment calculation ## histmoments(bins,counts,orders) apply moment calculation for a list of orders the return type is a numpy array of shape (nhistograms,nmoments) ## preparedatafromnpy(dataname, rebinningfactor=1, donormalize=True, doplot=False) read a .npy file and output the histograms ## preparedatafromdf(df, returnrunls=False, onlygolden=False, rebinningfactor=1, donormalize=True, doplot=False) prepare the data contained in a dataframe in the form of a numpy array args: - returnrunls: wether to return only a histogram array or 1D arrays of run and lumisection numbers as well - onlygolden: if True, only lumisections in the golden json file are kept - rebinningfactor: an integer number to downsample the histograms in the dataframe - donormalize: if True, data are normalized - doplot: if True, some example plots are made showing the histograms ## preparedatafromcsv(dataname, returnrunls=False, onlygolden=False, rebinningfactor=1, donormalize=True, doplot=False) prepare the data contained in a dataframe csv file in the form of a numpy array args: - returnrunls: wether to return only a histogram array or 1D arrays of run and lumisection numbers as well - onlygolden: if True, only lumisections in the golden json file are kept - rebinningfactor: an integer number to downsample the histograms in the dataframe - doplot: if True, some example plots are made showing the histograms","title":"hist utils"},{"location":"utils/json_utils/","text":"json utils ## loadjson( jsonfile ) load the content of a json file into a python object input arguments: - jsonfile: the name (or full path if needed) to the json file to be read output: - an dict object as specified in the note below note: the json file is supposed to contain an object like this example: { \"294927\": [ [ 55,85 ], [ 95,105] ], \"294928\": [ [1,33 ] ] } although no explicit checking is done in this function, objects that don't have this structure will probably lead to errors further in the code ## writejson( jsondict, outputfile, overwrite=False ) inverse function of loadjson input arguments - jsondict: dict object to be written to a json file - outputfile: output file to be written, extension '.json' will be appended automatically - overwrite: boolean whether to overwrite outputfile if it exists (default: throw exception) ## injson_single( run, lumi, jsondict ) helper function for injson, only for internal use input arguments: - run and lumi are integers - jsondict is an object loaded from a json file output: - boolean whether the run/lumi combination is in the json dict ## injson( run, lumi, jsonfile=None, jsondict=None ) find if a run and lumi combination is in a given json file input arguments: - run and lumi: integers or (equally long) arrays of integers - jsonfile: a path to a json file - jsondict: a dict loaded from a json file note: either jsonfile or jsondict must not be None! output: boolean or array of booleans (depending on run and lumi) ## isgolden(run,lumi) find if a run and lumi combination is in the golden json file input arguments: - run and lumi: either integers or (equally long) arrays of integers ## isdcson(run,lumi) find if a run and lumi combination is in DCS-only json file input arguments: - run and lumi: either integers or (equally long) arrays of integers ## ispixelgood(run,lumi) find if a run and lumi combination is in the json with good pixel flag note: this json was custom generated in run regisitry and not official! ## ispixelbad(run,lumi) find if a run and lumi combination is in the json with bad pixel flag note: this json was custom generated in run registry and not official! note: not simply the negation of ispixelgood! json has more relaxed conditions on DCS-like criteria. ## plainlist_to_rangelist( plainlist ) helper function for tuplelist_to_jsondict, only for internal use input arguments: - plainlist: a list of integers in increasing order, must have length >= 2 output: - a list lists representing ranges example: [1,2,3,5,6] -> [ [1,3], [5,6] ] ## rangelist_to_plainlist( rangelist ) inverse function of plainlist_to_rangelist, for internal use only ## tuplelist_to_jsondict( tuplelist ) convert a list of tuples of format (run number, [lumisection numbers]) to json dict ## jsondict_to_tuplelist( jsondict ) inverse function of tuplelist_to_jsondict ## get_lcs( jsonlist ) return a jsondict object that is the largest common subset (LCS) between the jsondict objects in jsonlist input arguments: - jsonlist: a list of dicts in the conventional json format, so each element in jsonlist must be e.g. { \"294927\": [ [ 55,85 ], [ 95,105] ], \"294928\": [ [1,33 ] ] } remark: this is probably not the most efficient implementation, open for improvement...","title":"json utils"},{"location":"utils/json_utils/#json-utils","text":"## loadjson( jsonfile ) load the content of a json file into a python object input arguments: - jsonfile: the name (or full path if needed) to the json file to be read output: - an dict object as specified in the note below note: the json file is supposed to contain an object like this example: { \"294927\": [ [ 55,85 ], [ 95,105] ], \"294928\": [ [1,33 ] ] } although no explicit checking is done in this function, objects that don't have this structure will probably lead to errors further in the code ## writejson( jsondict, outputfile, overwrite=False ) inverse function of loadjson input arguments - jsondict: dict object to be written to a json file - outputfile: output file to be written, extension '.json' will be appended automatically - overwrite: boolean whether to overwrite outputfile if it exists (default: throw exception) ## injson_single( run, lumi, jsondict ) helper function for injson, only for internal use input arguments: - run and lumi are integers - jsondict is an object loaded from a json file output: - boolean whether the run/lumi combination is in the json dict ## injson( run, lumi, jsonfile=None, jsondict=None ) find if a run and lumi combination is in a given json file input arguments: - run and lumi: integers or (equally long) arrays of integers - jsonfile: a path to a json file - jsondict: a dict loaded from a json file note: either jsonfile or jsondict must not be None! output: boolean or array of booleans (depending on run and lumi) ## isgolden(run,lumi) find if a run and lumi combination is in the golden json file input arguments: - run and lumi: either integers or (equally long) arrays of integers ## isdcson(run,lumi) find if a run and lumi combination is in DCS-only json file input arguments: - run and lumi: either integers or (equally long) arrays of integers ## ispixelgood(run,lumi) find if a run and lumi combination is in the json with good pixel flag note: this json was custom generated in run regisitry and not official! ## ispixelbad(run,lumi) find if a run and lumi combination is in the json with bad pixel flag note: this json was custom generated in run registry and not official! note: not simply the negation of ispixelgood! json has more relaxed conditions on DCS-like criteria. ## plainlist_to_rangelist( plainlist ) helper function for tuplelist_to_jsondict, only for internal use input arguments: - plainlist: a list of integers in increasing order, must have length >= 2 output: - a list lists representing ranges example: [1,2,3,5,6] -> [ [1,3], [5,6] ] ## rangelist_to_plainlist( rangelist ) inverse function of plainlist_to_rangelist, for internal use only ## tuplelist_to_jsondict( tuplelist ) convert a list of tuples of format (run number, [lumisection numbers]) to json dict ## jsondict_to_tuplelist( jsondict ) inverse function of tuplelist_to_jsondict ## get_lcs( jsonlist ) return a jsondict object that is the largest common subset (LCS) between the jsondict objects in jsonlist input arguments: - jsonlist: a list of dicts in the conventional json format, so each element in jsonlist must be e.g. { \"294927\": [ [ 55,85 ], [ 95,105] ], \"294928\": [ [1,33 ] ] } remark: this is probably not the most efficient implementation, open for improvement...","title":"json utils"},{"location":"utils/plot_utils/","text":"plot utils ## plot_hists(histlist,colorlist=[],labellist=[],transparency=1,xlims=(0,-1), (no valid documentation found) ## plot_hists_multi(histlist,colorlist=[],labellist=[],transparency=1,xlims=(0,-1), (no valid documentation found) ## plot_hists_from_df(df,histtype,nhists) plot a number of histograms in a dataframe - df is the dataframe from which to plot - histtype is the name of the histogram type (e.g. 'chargeInner_PXLayer_1') - nhists is the number of histograms to plot ## plot_sets(setlist,fig=None,ax=None,colorlist=[],labellist=[],transparencylist=[],xlims=(0,-1), (no valid documentation found) ## plot_anomalous(histlist,ls,highlight=-1,hrange=-1) histlist and ls are a list of histograms and corresponding lumisection numbers lsnumber is the lumisection number of the histogram to highlight hrange is the number of histograms before and after lsnumber to plot (default: whole run) ## plot_moments(moments,ls,dims,fig=None,ax=None,markersize=10) moments is an (nhists,nmoments) array dims is a tuple of two or three values between 0 and nmoments-1 ## plot_distance(dists,ls=None,rmlargest=0.,doplot=True, (no valid documentation found) ## plot_loss(data, xlims=None, (no valid documentation found) ## plot_mse(mse,rmlargest=0.,doplot=True, (no valid documentation found) ## plot_score_dist( scores, labels, nbins=20, normalize=False, (no valid documentation found)","title":"plot utils"},{"location":"utils/plot_utils/#plot-utils","text":"## plot_hists(histlist,colorlist=[],labellist=[],transparency=1,xlims=(0,-1), (no valid documentation found) ## plot_hists_multi(histlist,colorlist=[],labellist=[],transparency=1,xlims=(0,-1), (no valid documentation found) ## plot_hists_from_df(df,histtype,nhists) plot a number of histograms in a dataframe - df is the dataframe from which to plot - histtype is the name of the histogram type (e.g. 'chargeInner_PXLayer_1') - nhists is the number of histograms to plot ## plot_sets(setlist,fig=None,ax=None,colorlist=[],labellist=[],transparencylist=[],xlims=(0,-1), (no valid documentation found) ## plot_anomalous(histlist,ls,highlight=-1,hrange=-1) histlist and ls are a list of histograms and corresponding lumisection numbers lsnumber is the lumisection number of the histogram to highlight hrange is the number of histograms before and after lsnumber to plot (default: whole run) ## plot_moments(moments,ls,dims,fig=None,ax=None,markersize=10) moments is an (nhists,nmoments) array dims is a tuple of two or three values between 0 and nmoments-1 ## plot_distance(dists,ls=None,rmlargest=0.,doplot=True, (no valid documentation found) ## plot_loss(data, xlims=None, (no valid documentation found) ## plot_mse(mse,rmlargest=0.,doplot=True, (no valid documentation found) ## plot_score_dist( scores, labels, nbins=20, normalize=False, (no valid documentation found)","title":"plot utils"}]}